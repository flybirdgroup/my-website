{
  "docusaurus-plugin-content-docs": {
    "default": {
      "loadedVersions": [
        {
          "versionName": "current",
          "label": "Next",
          "banner": null,
          "badge": false,
          "className": "docs-version-current",
          "path": "/docs",
          "tagsPath": "/docs/tags",
          "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs",
          "editUrlLocalized": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/i18n/en/docusaurus-plugin-content-docs/current",
          "isLast": true,
          "routePriority": -1,
          "sidebarFilePath": "/Users/flybird/Desktop/my-website/sidebars.js",
          "contentPath": "/Users/flybird/Desktop/my-website/docs",
          "contentPathLocalized": "/Users/flybird/Desktop/my-website/i18n/en/docusaurus-plugin-content-docs/current",
          "docs": [
            {
              "unversionedId": "BIGQUERY",
              "id": "BIGQUERY",
              "title": "Bigquery 快速入门(一)--创建数据集并且查询",
              "description": "to link to Bigquery (二)--在BigQuery中创建授权视图",
              "source": "@site/docs/BIGQUERY.md",
              "sourceDirName": ".",
              "slug": "/BIGQUERY",
              "permalink": "/docs/BIGQUERY",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/BIGQUERY.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "BIGQUERY",
                "title": "Bigquery 快速入门(一)--创建数据集并且查询",
                "sidebar_label": "Style Guide"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "从Dockerhub拉取镜像并且使用命令运行",
                "permalink": "/docs/dockerhub_2"
              },
              "next": {
                "title": "Style Guide",
                "permalink": "/docs/BIGQUERY2"
              }
            },
            {
              "unversionedId": "BIGQUERY2",
              "id": "BIGQUERY2",
              "title": "Bigquery (二)--在BigQuery中创建授权视图",
              "description": "1 创建数据集并对其应用访问权限控制",
              "source": "@site/docs/BIGQUERY2.md",
              "sourceDirName": ".",
              "slug": "/BIGQUERY2",
              "permalink": "/docs/BIGQUERY2",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/BIGQUERY2.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "BIGQUERY2",
                "title": "Bigquery (二)--在BigQuery中创建授权视图",
                "sidebar_label": "Style Guide"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Style Guide",
                "permalink": "/docs/BIGQUERY"
              },
              "next": {
                "title": "使用 BigQuery Storage API 将 BigQuery 数据下载到 Pandas",
                "permalink": "/docs/BIGQUERY3"
              }
            },
            {
              "unversionedId": "BIGQUERY3",
              "id": "BIGQUERY3",
              "title": "使用 BigQuery Storage API 将 BigQuery 数据下载到 Pandas",
              "description": "link to google 原文 参看(https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas))",
              "source": "@site/docs/Bigquery_pandas.md",
              "sourceDirName": ".",
              "slug": "/BIGQUERY3",
              "permalink": "/docs/BIGQUERY3",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Bigquery_pandas.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "BIGQUERY3",
                "title": "使用 BigQuery Storage API 将 BigQuery 数据下载到 Pandas",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Style Guide",
                "permalink": "/docs/BIGQUERY2"
              },
              "next": {
                "title": "在 App Engine 上部署 Python 应用-谷歌云快速入门(六)",
                "permalink": "/docs/GoogleAppEngine"
              }
            },
            {
              "unversionedId": "createlinux",
              "id": "createlinux",
              "title": "Create Virtual Linux by Google 创建linux虚拟机-谷歌云快速入门(一)",
              "description": "author: 招晓贤",
              "source": "@site/docs/createlinux.md",
              "sourceDirName": ".",
              "slug": "/createlinux",
              "permalink": "/docs/createlinux",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/createlinux.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "createlinux",
                "title": "Create Virtual Linux by Google 创建linux虚拟机-谷歌云快速入门(一)",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "对图片进行标签检测-谷歌云快速入门(五)",
                "permalink": "/docs/VisionAPI"
              },
              "next": {
                "title": "Style Guide",
                "permalink": "/docs/doc1"
              }
            },
            {
              "unversionedId": "doc1",
              "id": "doc1",
              "title": "Style Guide",
              "description": "You can write content using GitHub-flavored Markdown syntax.",
              "source": "@site/docs/doc1.md",
              "sourceDirName": ".",
              "slug": "/doc1",
              "permalink": "/docs/doc1",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/doc1.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "doc1",
                "title": "Style Guide",
                "sidebar_label": "Style Guide"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Create Virtual Linux by Google 创建linux虚拟机-谷歌云快速入门(一)",
                "permalink": "/docs/createlinux"
              },
              "next": {
                "title": "gcs command line",
                "permalink": "/docs/doc2"
              }
            },
            {
              "unversionedId": "doc2",
              "id": "doc2",
              "title": "GCP常用命令",
              "description": "Cloud Storage常用命令",
              "source": "@site/docs/doc2.md",
              "sourceDirName": ".",
              "slug": "/doc2",
              "permalink": "/docs/doc2",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/doc2.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "doc2",
                "title": "GCP常用命令",
                "sidebar_label": "gcs command line"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Style Guide",
                "permalink": "/docs/doc1"
              },
              "next": {
                "title": "This is Document Number 3",
                "permalink": "/docs/doc3"
              }
            },
            {
              "unversionedId": "doc3",
              "id": "doc3",
              "title": "This is Document Number 3",
              "description": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In ac euismod odio, eu consequat dui. Nullam molestie consectetur risus id imperdiet. Proin sodales ornare turpis, non mollis massa ultricies id. Nam at nibh scelerisque, feugiat ante non, dapibus tortor. Vivamus volutpat diam quis tellus elementum bibendum. Praesent semper gravida velit quis aliquam. Etiam in cursus neque. Nam lectus ligula, malesuada et mauris a, bibendum faucibus mi. Phasellus ut interdum felis. Phasellus in odio pulvinar, porttitor urna eget, fringilla lectus. Aliquam sollicitudin est eros. Mauris consectetur quam vitae mauris interdum hendrerit. Lorem ipsum dolor sit amet, consectetur adipiscing elit.",
              "source": "@site/docs/doc3.md",
              "sourceDirName": ".",
              "slug": "/doc3",
              "permalink": "/docs/doc3",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/doc3.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "doc3",
                "title": "This is Document Number 3"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "gcs command line",
                "permalink": "/docs/doc2"
              },
              "next": {
                "title": "Container Registry 快速入门快速入门",
                "permalink": "/docs/docker"
              }
            },
            {
              "unversionedId": "docker",
              "id": "docker",
              "title": "Container Registry 快速入门快速入门",
              "description": "使用 Cloud Shell 配置 gcloud 并运行一个容器映像。",
              "source": "@site/docs/docker.md",
              "sourceDirName": ".",
              "slug": "/docker",
              "permalink": "/docs/docker",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/docker.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "docker",
                "title": "Container Registry 快速入门快速入门",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "This is Document Number 3",
                "permalink": "/docs/doc3"
              },
              "next": {
                "title": "Powered by MDX",
                "permalink": "/docs/mdx"
              }
            },
            {
              "unversionedId": "dockerhub",
              "id": "dockerhub",
              "title": "push镜像到自己的dockerhub",
              "description": "link to 谷歌云快速入门(二) 存储文件然后共享",
              "source": "@site/docs/2020-3-26-dockerhub.md",
              "sourceDirName": ".",
              "slug": "/dockerhub",
              "permalink": "/docs/dockerhub",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/2020-3-26-dockerhub.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "docker",
                  "permalink": "/docs/tags/docker"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "dockerhub",
                "title": "push镜像到自己的dockerhub",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud",
                  "docker"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Translate your site",
                "permalink": "/docs/tutorial-extras/translate-your-site"
              },
              "next": {
                "title": "从Dockerhub拉取镜像并且使用命令运行",
                "permalink": "/docs/dockerhub_2"
              }
            },
            {
              "unversionedId": "dockerhub_2",
              "id": "dockerhub_2",
              "title": "从Dockerhub拉取镜像并且使用命令运行",
              "description": "link to 谷歌云快速入门(二) 存储文件然后共享",
              "source": "@site/docs/2020-3-27-dockerhub_2.md",
              "sourceDirName": ".",
              "slug": "/dockerhub_2",
              "permalink": "/docs/dockerhub_2",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/2020-3-27-dockerhub_2.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "docker",
                  "permalink": "/docs/tags/docker"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "dockerhub_2",
                "title": "从Dockerhub拉取镜像并且使用命令运行",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud",
                  "docker"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "push镜像到自己的dockerhub",
                "permalink": "/docs/dockerhub"
              },
              "next": {
                "title": "Style Guide",
                "permalink": "/docs/BIGQUERY"
              }
            },
            {
              "unversionedId": "GCP_shell_python",
              "id": "GCP_shell_python",
              "title": "谷歌安装GCP_shell_python命令",
              "description": "Install virtualenv version 13.1.0 or above if it is not installed already.",
              "source": "@site/docs/谷歌shell安装python环境.md",
              "sourceDirName": ".",
              "slug": "/GCP_shell_python",
              "permalink": "/docs/GCP_shell_python",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/谷歌shell安装python环境.md",
              "tags": [
                {
                  "label": "shell",
                  "permalink": "/docs/tags/shell"
                },
                {
                  "label": "python",
                  "permalink": "/docs/tags/python"
                },
                {
                  "label": "apache-beam",
                  "permalink": "/docs/tags/apache-beam"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "GCP_shell_python",
                "title": "谷歌安装GCP_shell_python命令",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "shell",
                  "python",
                  "apache-beam",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "人工智能项目分享",
                "permalink": "/docs/projects"
              }
            },
            {
              "unversionedId": "GoogleAppEngine",
              "id": "GoogleAppEngine",
              "title": "在 App Engine 上部署 Python 应用-谷歌云快速入门(六)",
              "description": "构建一个显示一条简短消息的 App Engine 小应用。",
              "source": "@site/docs/GoogleAppEngine.md",
              "sourceDirName": ".",
              "slug": "/GoogleAppEngine",
              "permalink": "/docs/GoogleAppEngine",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/GoogleAppEngine.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "tensorflow",
                  "permalink": "/docs/tags/tensorflow"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "GoogleAppEngine",
                "title": "在 App Engine 上部署 Python 应用-谷歌云快速入门(六)",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "tensorflow",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "使用 BigQuery Storage API 将 BigQuery 数据下载到 Pandas",
                "permalink": "/docs/BIGQUERY3"
              },
              "next": {
                "title": "存储文件然后共享-谷歌云快速入门(二)",
                "permalink": "/docs/GoogleCloudStorage"
              }
            },
            {
              "unversionedId": "GoogleCloudStorage",
              "id": "GoogleCloudStorage",
              "title": "存储文件然后共享-谷歌云快速入门(二)",
              "description": "link to 使用GKE(Google Kubernetes Engine)部署容器化应用 -谷歌云快速入门(三)",
              "source": "@site/docs/GoogleCloudStorage.md",
              "sourceDirName": ".",
              "slug": "/GoogleCloudStorage",
              "permalink": "/docs/GoogleCloudStorage",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/GoogleCloudStorage.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "GoogleCloudStorage",
                "title": "存储文件然后共享-谷歌云快速入门(二)",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "在 App Engine 上部署 Python 应用-谷歌云快速入门(六)",
                "permalink": "/docs/GoogleAppEngine"
              },
              "next": {
                "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
                "permalink": "/docs/Kubernetes"
              }
            },
            {
              "unversionedId": "intro",
              "id": "intro",
              "title": "Tutorial Intro",
              "description": "Let's discover Docusaurus in less than 5 minutes.",
              "source": "@site/docs/intro.md",
              "sourceDirName": ".",
              "slug": "/intro",
              "permalink": "/docs/intro",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "next": {
                "title": "Tutorial - Basics",
                "permalink": "/docs/category/tutorial---basics"
              }
            },
            {
              "unversionedId": "Kubernetes",
              "id": "Kubernetes",
              "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
              "description": "link to 谷歌云快速入门(二) 存储文件然后共享",
              "source": "@site/docs/Kubernetes.md",
              "sourceDirName": ".",
              "slug": "/Kubernetes",
              "permalink": "/docs/Kubernetes",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Kubernetes.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "Kubernetes",
                "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "存储文件然后共享-谷歌云快速入门(二)",
                "permalink": "/docs/GoogleCloudStorage"
              },
              "next": {
                "title": "Style Guide",
                "permalink": "/docs/ML4"
              }
            },
            {
              "unversionedId": "mdx",
              "id": "mdx",
              "title": "Powered by MDX",
              "description": "You can write JSX and use React components within your Markdown thanks to MDX.",
              "source": "@site/docs/mdx.md",
              "sourceDirName": ".",
              "slug": "/mdx",
              "permalink": "/docs/mdx",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/mdx.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "mdx",
                "title": "Powered by MDX"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Container Registry 快速入门快速入门",
                "permalink": "/docs/docker"
              },
              "next": {
                "title": "人工智能项目分享",
                "permalink": "/docs/projects"
              }
            },
            {
              "unversionedId": "ML1",
              "id": "ML1",
              "title": "机器学习-- 1 训练与损失",
              "description": "官方文档信息",
              "source": "@site/docs/ML_训练与损失.md",
              "sourceDirName": ".",
              "slug": "/ML1",
              "permalink": "/docs/ML1",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ML_训练与损失.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "ML1",
                "title": "机器学习-- 1 训练与损失",
                "sidebar_label": "Style Guide"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Style Guide",
                "permalink": "/docs/ML3"
              },
              "next": {
                "title": "Style Guide",
                "permalink": "/docs/ML2"
              }
            },
            {
              "unversionedId": "ML2",
              "id": "ML2",
              "title": "机器学习-- 2 降低损失",
              "description": "官方文档信息",
              "source": "@site/docs/ML_降低损失y.md",
              "sourceDirName": ".",
              "slug": "/ML2",
              "permalink": "/docs/ML2",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ML_降低损失y.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "ML2",
                "title": "机器学习-- 2 降低损失",
                "sidebar_label": "Style Guide"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Style Guide",
                "permalink": "/docs/ML1"
              },
              "next": {
                "title": "训练 TensorFlow 模型-谷歌云快速入门(四)",
                "permalink": "/docs/TensorFlow"
              }
            },
            {
              "unversionedId": "ML3",
              "id": "ML3",
              "title": "机器学习-- 3 梯度下降法",
              "description": "官方文档信息",
              "source": "@site/docs/ML_梯度下降法+学习速率+随机梯度下降.md",
              "sourceDirName": ".",
              "slug": "/ML3",
              "permalink": "/docs/ML3",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ML_梯度下降法+学习速率+随机梯度下降.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "ML3",
                "title": "机器学习-- 3 梯度下降法",
                "sidebar_label": "Style Guide"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Style Guide",
                "permalink": "/docs/ML4"
              },
              "next": {
                "title": "Style Guide",
                "permalink": "/docs/ML1"
              }
            },
            {
              "unversionedId": "ML4",
              "id": "ML4",
              "title": "机器学习-- 4 TF基本步骤",
              "description": "官方文档信息",
              "source": "@site/docs/ML_TF基本步骤.md",
              "sourceDirName": ".",
              "slug": "/ML4",
              "permalink": "/docs/ML4",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ML_TF基本步骤.md",
              "tags": [],
              "version": "current",
              "frontMatter": {
                "id": "ML4",
                "title": "机器学习-- 4 TF基本步骤",
                "sidebar_label": "Style Guide"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
                "permalink": "/docs/Kubernetes"
              },
              "next": {
                "title": "Style Guide",
                "permalink": "/docs/ML3"
              }
            },
            {
              "unversionedId": "projects",
              "id": "projects",
              "title": "人工智能项目分享",
              "description": "NLP方向",
              "source": "@site/docs/projects.md",
              "sourceDirName": ".",
              "slug": "/projects",
              "permalink": "/docs/projects",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/projects.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "projects",
                "title": "人工智能项目分享",
                "author": "招晓贤",
                "author_title": "AI Engineer @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "google cloud"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Powered by MDX",
                "permalink": "/docs/mdx"
              },
              "next": {
                "title": "谷歌安装GCP_shell_python命令",
                "permalink": "/docs/GCP_shell_python"
              }
            },
            {
              "unversionedId": "TensorFlow",
              "id": "TensorFlow",
              "title": "训练 TensorFlow 模型-谷歌云快速入门(四)",
              "description": "在本地训练、在云端使用单个工作器训练，以及在云端进行分布式训练。",
              "source": "@site/docs/TensorFlow.md",
              "sourceDirName": ".",
              "slug": "/TensorFlow",
              "permalink": "/docs/TensorFlow",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/TensorFlow.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "tensorflow",
                  "permalink": "/docs/tags/tensorflow"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "TensorFlow",
                "title": "训练 TensorFlow 模型-谷歌云快速入门(四)",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "tensorflow",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Style Guide",
                "permalink": "/docs/ML2"
              },
              "next": {
                "title": "对图片进行标签检测-谷歌云快速入门(五)",
                "permalink": "/docs/VisionAPI"
              }
            },
            {
              "unversionedId": "tutorial-basics/congratulations",
              "id": "tutorial-basics/congratulations",
              "title": "Congratulations!",
              "description": "You have just learned the basics of Docusaurus and made some changes to the initial template.",
              "source": "@site/docs/tutorial-basics/congratulations.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/congratulations",
              "permalink": "/docs/tutorial-basics/congratulations",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/congratulations.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 6,
              "frontMatter": {
                "sidebar_position": 6
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Deploy your site",
                "permalink": "/docs/tutorial-basics/deploy-your-site"
              },
              "next": {
                "title": "Tutorial - Extras",
                "permalink": "/docs/category/tutorial---extras"
              }
            },
            {
              "unversionedId": "tutorial-basics/create-a-blog-post",
              "id": "tutorial-basics/create-a-blog-post",
              "title": "Create a Blog Post",
              "description": "Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...",
              "source": "@site/docs/tutorial-basics/create-a-blog-post.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/create-a-blog-post",
              "permalink": "/docs/tutorial-basics/create-a-blog-post",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/create-a-blog-post.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "sidebar_position": 3
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Create a Document",
                "permalink": "/docs/tutorial-basics/create-a-document"
              },
              "next": {
                "title": "Markdown Features",
                "permalink": "/docs/tutorial-basics/markdown-features"
              }
            },
            {
              "unversionedId": "tutorial-basics/create-a-document",
              "id": "tutorial-basics/create-a-document",
              "title": "Create a Document",
              "description": "Documents are groups of pages connected through:",
              "source": "@site/docs/tutorial-basics/create-a-document.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/create-a-document",
              "permalink": "/docs/tutorial-basics/create-a-document",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/create-a-document.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Create a Page",
                "permalink": "/docs/tutorial-basics/create-a-page"
              },
              "next": {
                "title": "Create a Blog Post",
                "permalink": "/docs/tutorial-basics/create-a-blog-post"
              }
            },
            {
              "unversionedId": "tutorial-basics/create-a-page",
              "id": "tutorial-basics/create-a-page",
              "title": "Create a Page",
              "description": "Add Markdown or React files to src/pages to create a standalone page:",
              "source": "@site/docs/tutorial-basics/create-a-page.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/create-a-page",
              "permalink": "/docs/tutorial-basics/create-a-page",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/create-a-page.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Tutorial - Basics",
                "permalink": "/docs/category/tutorial---basics"
              },
              "next": {
                "title": "Create a Document",
                "permalink": "/docs/tutorial-basics/create-a-document"
              }
            },
            {
              "unversionedId": "tutorial-basics/deploy-your-site",
              "id": "tutorial-basics/deploy-your-site",
              "title": "Deploy your site",
              "description": "Docusaurus is a static-site-generator (also called Jamstack).",
              "source": "@site/docs/tutorial-basics/deploy-your-site.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/deploy-your-site",
              "permalink": "/docs/tutorial-basics/deploy-your-site",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/deploy-your-site.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 5,
              "frontMatter": {
                "sidebar_position": 5
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Markdown Features",
                "permalink": "/docs/tutorial-basics/markdown-features"
              },
              "next": {
                "title": "Congratulations!",
                "permalink": "/docs/tutorial-basics/congratulations"
              }
            },
            {
              "unversionedId": "tutorial-basics/markdown-features",
              "id": "tutorial-basics/markdown-features",
              "title": "Markdown Features",
              "description": "Docusaurus supports Markdown and a few additional features.",
              "source": "@site/docs/tutorial-basics/markdown-features.mdx",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/markdown-features",
              "permalink": "/docs/tutorial-basics/markdown-features",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/markdown-features.mdx",
              "tags": [],
              "version": "current",
              "sidebarPosition": 4,
              "frontMatter": {
                "sidebar_position": 4
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Create a Blog Post",
                "permalink": "/docs/tutorial-basics/create-a-blog-post"
              },
              "next": {
                "title": "Deploy your site",
                "permalink": "/docs/tutorial-basics/deploy-your-site"
              }
            },
            {
              "unversionedId": "tutorial-extras/manage-docs-versions",
              "id": "tutorial-extras/manage-docs-versions",
              "title": "Manage Docs Versions",
              "description": "Docusaurus can manage multiple versions of your docs.",
              "source": "@site/docs/tutorial-extras/manage-docs-versions.md",
              "sourceDirName": "tutorial-extras",
              "slug": "/tutorial-extras/manage-docs-versions",
              "permalink": "/docs/tutorial-extras/manage-docs-versions",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-extras/manage-docs-versions.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Tutorial - Extras",
                "permalink": "/docs/category/tutorial---extras"
              },
              "next": {
                "title": "Translate your site",
                "permalink": "/docs/tutorial-extras/translate-your-site"
              }
            },
            {
              "unversionedId": "tutorial-extras/translate-your-site",
              "id": "tutorial-extras/translate-your-site",
              "title": "Translate your site",
              "description": "Let's translate docs/intro.md to French.",
              "source": "@site/docs/tutorial-extras/translate-your-site.md",
              "sourceDirName": "tutorial-extras",
              "slug": "/tutorial-extras/translate-your-site",
              "permalink": "/docs/tutorial-extras/translate-your-site",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-extras/translate-your-site.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Manage Docs Versions",
                "permalink": "/docs/tutorial-extras/manage-docs-versions"
              },
              "next": {
                "title": "push镜像到自己的dockerhub",
                "permalink": "/docs/dockerhub"
              }
            },
            {
              "unversionedId": "VisionAPI",
              "id": "VisionAPI",
              "title": "对图片进行标签检测-谷歌云快速入门(五)",
              "description": "将图片上传到 Cloud Storage，并对 Cloud Vision API 服务发出标签检测请求。",
              "source": "@site/docs/VisionAPI.md",
              "sourceDirName": ".",
              "slug": "/VisionAPI",
              "permalink": "/docs/VisionAPI",
              "draft": false,
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/VisionAPI.md",
              "tags": [
                {
                  "label": "facebook",
                  "permalink": "/docs/tags/facebook"
                },
                {
                  "label": "hello",
                  "permalink": "/docs/tags/hello"
                },
                {
                  "label": "docusaurus",
                  "permalink": "/docs/tags/docusaurus"
                },
                {
                  "label": "tensorflow",
                  "permalink": "/docs/tags/tensorflow"
                },
                {
                  "label": "google cloud",
                  "permalink": "/docs/tags/google-cloud"
                },
                {
                  "label": "linux",
                  "permalink": "/docs/tags/linux"
                }
              ],
              "version": "current",
              "frontMatter": {
                "id": "VisionAPI",
                "title": "对图片进行标签检测-谷歌云快速入门(五)",
                "author": "招晓贤",
                "author_title": "AI engine @ Facebook",
                "author_url": "https://github.com/flybirdgroup",
                "tags": [
                  "facebook",
                  "hello",
                  "docusaurus",
                  "tensorflow",
                  "google cloud",
                  "linux"
                ]
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "训练 TensorFlow 模型-谷歌云快速入门(四)",
                "permalink": "/docs/TensorFlow"
              },
              "next": {
                "title": "Create Virtual Linux by Google 创建linux虚拟机-谷歌云快速入门(一)",
                "permalink": "/docs/createlinux"
              }
            }
          ],
          "drafts": [],
          "sidebars": {
            "tutorialSidebar": [
              {
                "type": "doc",
                "id": "intro"
              },
              {
                "type": "category",
                "label": "Tutorial - Basics",
                "collapsible": true,
                "collapsed": true,
                "items": [
                  {
                    "type": "doc",
                    "id": "tutorial-basics/create-a-page"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/create-a-document"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/create-a-blog-post"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/markdown-features"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/deploy-your-site"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/congratulations"
                  }
                ],
                "link": {
                  "type": "generated-index",
                  "description": "5 minutes to learn the most important Docusaurus concepts.",
                  "slug": "/category/tutorial---basics",
                  "permalink": "/docs/category/tutorial---basics"
                }
              },
              {
                "type": "category",
                "label": "Tutorial - Extras",
                "collapsible": true,
                "collapsed": true,
                "items": [
                  {
                    "type": "doc",
                    "id": "tutorial-extras/manage-docs-versions"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-extras/translate-your-site"
                  }
                ],
                "link": {
                  "type": "generated-index",
                  "slug": "/category/tutorial---extras",
                  "permalink": "/docs/category/tutorial---extras"
                }
              },
              {
                "type": "doc",
                "id": "dockerhub"
              },
              {
                "type": "doc",
                "id": "dockerhub_2"
              },
              {
                "type": "doc",
                "id": "BIGQUERY",
                "label": "Style Guide"
              },
              {
                "type": "doc",
                "id": "BIGQUERY2",
                "label": "Style Guide"
              },
              {
                "type": "doc",
                "id": "BIGQUERY3"
              },
              {
                "type": "doc",
                "id": "GoogleAppEngine"
              },
              {
                "type": "doc",
                "id": "GoogleCloudStorage"
              },
              {
                "type": "doc",
                "id": "Kubernetes"
              },
              {
                "type": "doc",
                "id": "ML4",
                "label": "Style Guide"
              },
              {
                "type": "doc",
                "id": "ML3",
                "label": "Style Guide"
              },
              {
                "type": "doc",
                "id": "ML1",
                "label": "Style Guide"
              },
              {
                "type": "doc",
                "id": "ML2",
                "label": "Style Guide"
              },
              {
                "type": "doc",
                "id": "TensorFlow"
              },
              {
                "type": "doc",
                "id": "VisionAPI"
              },
              {
                "type": "doc",
                "id": "createlinux"
              },
              {
                "type": "doc",
                "id": "doc1",
                "label": "Style Guide"
              },
              {
                "type": "doc",
                "id": "doc2",
                "label": "gcs command line"
              },
              {
                "type": "doc",
                "id": "doc3"
              },
              {
                "type": "doc",
                "id": "docker"
              },
              {
                "type": "doc",
                "id": "mdx"
              },
              {
                "type": "doc",
                "id": "projects"
              },
              {
                "type": "doc",
                "id": "GCP_shell_python"
              }
            ]
          }
        }
      ]
    }
  },
  "docusaurus-plugin-content-blog": {
    "default": {
      "blogSidebarTitle": "Recent posts",
      "blogPosts": [
        {
          "id": "welcome",
          "metadata": {
            "permalink": "/blog/welcome",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-08-26-welcome/index.md",
            "source": "@site/blog/2021-08-26-welcome/index.md",
            "title": "Welcome",
            "description": "Docusaurus blogging features are powered by the blog plugin.",
            "date": "2021-08-26T00:00:00.000Z",
            "formattedDate": "August 26, 2021",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 0.405,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "Sébastien Lorber",
                "title": "Docusaurus maintainer",
                "url": "https://sebastienlorber.com",
                "imageURL": "https://github.com/slorber.png",
                "key": "slorber"
              },
              {
                "name": "Yangshun Tay",
                "title": "Front End Engineer @ Facebook",
                "url": "https://github.com/yangshun",
                "imageURL": "https://github.com/yangshun.png",
                "key": "yangshun"
              }
            ],
            "frontMatter": {
              "slug": "welcome",
              "title": "Welcome",
              "authors": [
                "slorber",
                "yangshun"
              ],
              "tags": [
                "facebook",
                "hello",
                "docusaurus"
              ]
            },
            "nextItem": {
              "title": "MDX Blog Post",
              "permalink": "/blog/mdx-blog-post"
            }
          },
          "content": "[Docusaurus blogging features](https://docusaurus.io/docs/blog) are powered by the [blog plugin](https://docusaurus.io/docs/api/plugins/@docusaurus/plugin-content-blog).\n\nSimply add Markdown files (or folders) to the `blog` directory.\n\nRegular blog authors can be added to `authors.yml`.\n\nThe blog post date can be extracted from filenames, such as:\n\n- `2019-05-30-welcome.md`\n- `2019-05-30-welcome/index.md`\n\nA blog post folder can be convenient to co-locate blog post images:\n\n![Docusaurus Plushie](./docusaurus-plushie-banner.jpeg)\n\nThe blog supports tags as well!\n\n**And if you don't want a blog**: just delete this directory, and use `blog: false` in your Docusaurus config."
        },
        {
          "id": "mdx-blog-post",
          "metadata": {
            "permalink": "/blog/mdx-blog-post",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-08-01-mdx-blog-post.mdx",
            "source": "@site/blog/2021-08-01-mdx-blog-post.mdx",
            "title": "MDX Blog Post",
            "description": "Blog posts support Docusaurus Markdown features, such as MDX.",
            "date": "2021-08-01T00:00:00.000Z",
            "formattedDate": "August 1, 2021",
            "tags": [
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 0.175,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "Sébastien Lorber",
                "title": "Docusaurus maintainer",
                "url": "https://sebastienlorber.com",
                "imageURL": "https://github.com/slorber.png",
                "key": "slorber"
              }
            ],
            "frontMatter": {
              "slug": "mdx-blog-post",
              "title": "MDX Blog Post",
              "authors": [
                "slorber"
              ],
              "tags": [
                "docusaurus"
              ]
            },
            "prevItem": {
              "title": "Welcome",
              "permalink": "/blog/welcome"
            },
            "nextItem": {
              "title": "通过Composer使用Airflow运行一个workflow",
              "permalink": "/blog/2020/6/12/Airflow_Dataproc"
            }
          },
          "content": "Blog posts support [Docusaurus Markdown features](https://docusaurus.io/docs/markdown-features), such as [MDX](https://mdxjs.com/).\n\n:::tip\n\nUse the power of React to create interactive blog posts.\n\n```js\n<button onClick={() => alert('button clicked!')}>Click me!</button>\n```\n\n<button onClick={() => alert('button clicked!')}>Click me!</button>\n\n:::"
        },
        {
          "id": "/2020/6/12/Airflow_Dataproc",
          "metadata": {
            "permalink": "/blog/2020/6/12/Airflow_Dataproc",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-6-12-Airflow_Dataproc.md",
            "source": "@site/blog/2020-6-12-Airflow_Dataproc.md",
            "title": "通过Composer使用Airflow运行一个workflow",
            "description": "创建Composer by setting up Composer environment >> create a workflow py file >>  创建Dataproc >> Runs Hadoop job on Dataproc >> deletes Dataproc cluster",
            "date": "2020-06-12T00:00:00.000Z",
            "formattedDate": "June 12, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "linux",
                "permalink": "/blog/tags/linux"
              }
            ],
            "readingTime": 6.2,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup"
              }
            ],
            "frontMatter": {
              "id": "Airflow1",
              "title": "通过Composer使用Airflow运行一个workflow",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "tags": [
                "facebook",
                "hello",
                "docusaurus",
                "google cloud",
                "linux"
              ]
            },
            "prevItem": {
              "title": "MDX Blog Post",
              "permalink": "/blog/mdx-blog-post"
            },
            "nextItem": {
              "title": "云上快速搭建WordPress网站",
              "permalink": "/blog/2020/6/12/快速搭建wordpress网站"
            }
          },
          "content": "创建Composer by setting up Composer environment >> create a workflow py file >>  创建Dataproc >> Runs Hadoop job on Dataproc >> deletes Dataproc cluster\n\n## step1 Create Enviroment and set up the Enviroment\n```\nName: highcpu\n\nLocation: us-central1\n\nZone: us-central1-a\n\nMachine type: n1-highcpu-4\n\nPython version 3\n```\n![png](../img/airflow/1-1.png)\n![png](../img/airflow/k-1.png)\n![png](../img/airflow/k-2.png)\n\n## step2 创建GCS bucket\n要记住bucket名字, 因为这个是放DAG file的\n\n\n## step3 了解核心概念\nAirflow是一个以编程方式编写，安排和监视工作流的平台。\n使用Airflow将工作流编写为任务的有向无环图（DAG）, 就是单向的\n\nDAG: 有向无环图是您要运行的所有任务的集合，以反映其关系和依赖性的方式进行组织。\n\nOperator: 就是对单一任务的描述\n\nTask: 操作员的参数化实例；DAG中的一个节点\n\nTask Instance: 任务的特定运行；其特征是：DAG，任务和时间点。它具有指示性状态：运行，成功，失败，跳过\n\n[更多概念可以查看链接](https://airflow.apache.org/docs/stable/concepts.html#)\n\n## step4: 设定一个workflow 工作流\n\n1. Composer workflows是由DAGs组成的. DAGs 就是被定义好的标准python文件, 这些文件都是放在Airflow的DAG_FOLDER中.\n2. Airflow会动态地执行python文件的代码来构建DAGs对象\n3. 您可以根据需要拥有任意数量的DAG，每个DAG描述任意数量的任务。通常，每个DAG应对应一个逻辑工作流程workflow。\n\n```\n\"\"\"Example Airflow DAG that creates a Cloud Dataproc cluster, runs the Hadoop\nwordcount example, and deletes the cluster.\nThis DAG relies on three Airflow variables\nhttps://airflow.apache.org/concepts.html#variables\n这里是在airflow web interface的admin>>varibale设置的\n* gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster.\n* gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be\n  created.\n* gcs_bucket - Google Cloud Storage bucket to use for result of Hadoop job.\n  See https://cloud.google.com/storage/docs/creating-buckets for creating a\n  bucket.\n\"\"\"\n\nimport datetime\nimport os\n\nfrom airflow import models\nfrom airflow.contrib.operators import dataproc_operator\nfrom airflow.utils import trigger_rule\n\n# Output file for Cloud Dataproc job. 输出文件地址\n# 输出的地址是通过gcs+文件名字(wordcount) + 时间 + '/'\noutput_file = os.path.join(\n    models.Variable.get('gcs_bucket'), 'wordcount',\n    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep\n# Path to Hadoop wordcount example available on every Dataproc cluster.\nWORDCOUNT_JAR = (\n    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'\n)\n# Arguments to pass to Cloud Dataproc job.\nwordcount_args = ['wordcount', 'gs://pub/shakespeare/rose.txt', output_file]\n\nyesterday = datetime.datetime.combine(\n    datetime.datetime.today() - datetime.timedelta(1),\n    datetime.datetime.min.time())\n\ndefault_dag_args = {\n    # Setting start date as yesterday starts the DAG immediately when it is\n    # detected in the Cloud Storage bucket.\n    'start_date': yesterday,\n    # To email on failure or retry set 'email' arg to your email and enable\n    # emailing here.\n    'email_on_failure': False,\n    'email_on_retry': False,\n    # If a task fails, retry it once after waiting at least 5 minutes\n    'retries': 1,\n    'retry_delay': datetime.timedelta(minutes=5),\n    'project_id': models.Variable.get('gcp_project')\n}\n\n# [START composer_hadoop_schedule]\nwith models.DAG(\n        'composer_hadoop_tutorial',\n        # Continue to run DAG once per day\n        schedule_interval=datetime.timedelta(days=1),\n        default_args=default_dag_args) as dag:\n    # [END composer_hadoop_schedule]\n\n    # Create a Cloud Dataproc cluster.\n    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(\n        task_id='create_dataproc_cluster',\n        # Give the cluster a unique name by appending the date scheduled.\n        # See https://airflow.apache.org/code.html#default-variables\n        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n        num_workers=2,\n        zone=models.Variable.get('gce_zone'),\n        master_machine_type='n1-standard-1',\n        worker_machine_type='n1-standard-1')\n\n    # Run the Hadoop wordcount example installed on the Cloud Dataproc cluster\n    # master node.\n    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(\n        task_id='run_dataproc_hadoop',\n        main_jar=WORDCOUNT_JAR,\n        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n        arguments=wordcount_args)\n\n    # Delete Cloud Dataproc cluster.\n    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(\n        task_id='delete_dataproc_cluster',\n        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n        # Setting trigger_rule to ALL_DONE causes the cluster to be deleted\n        # even if the Dataproc job fails.\n        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)\n\n    # [START composer_hadoop_steps]\n    # Define DAG dependencies.\n    create_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster\n    # [END composer_hadoop_steps]\n```\n\n## 为了协调这三个工作流程任务，DAG导入了以下运算符：\n\nDataprocClusterCreateOperator：创建一个Cloud Dataproc集群。\nDataProcHadoopOperator：提交Hadoop wordcount作业并将结果写入Cloud Storage存储桶。\nDataprocClusterDeleteOperator：删除群集以避免产生持续的Compute Engine费用。\n\n任务按顺序运行，您可以在文件的此部分中看到\n```\n# Define DAG dependencies.\ncreate_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster\n```\n\nDAG的名称为hadoop_tutorial，并且DAG每天运行一次。\n```\nwith models.DAG(\n        'composer_hadoop_tutorial',\n        # Continue to run DAG once per day\n        schedule_interval=datetime.timedelta(days=1),\n        default_args=default_dag_args) as dag:\n```\n由于start_date传递给default_dag_args的设置为yesterday，因此Cloud Composer安排工作流在DAG上传后立即开始。\n\n## step5 设置airflow变量\n\n1. 去airflow web interface \n\n![png](../img/airflow/1.png)\n\n2. 创建以下Aireflow变量gcp_project，gcs_bucket以及gce_zone：\n![png](../img/airflow/2.png)\n\n## step6 将DAG上载到云存储\n在Cloud Shell中，将hadoop_tutorial.py复制并保存在本地虚拟机上\n```\ngit clone https://github.com/GoogleCloudPlatform/python-docs-samples\n```\n\n转到python-docs-samples目录：\n```\ncd python-docs-samples/composer/workflows\n```\n\n现在，将hadoop_tutorial.py文件的副本上载到Cloud Storage存储桶，该存储桶在创建环境时会自动创建。您可以通过转到Composer > 环境进行检查。单击您先前创建的环境，这将带您进入所创建环境的描述。查找DAGs folder，复制要替换的值，DAGs_folder_path并在以下命令中上传文件：\n```\ngsutil cp hadoop_tutorial.py DAGs_folder_path\n```\n![png](../img/airflow/3.png)\n\n\n探索DAG运行\n当您将DAG文件上传到dagsCloud Storage中的文件夹时，Cloud Composer会解析该文件。如果未找到错误，则工作流的名称将显示在DAG列表中，并且该工作流已排队等待立即运行。\n\n确保您在Airflow Web界面的DAG选项卡上。此过程需要几分钟才能完成。刷新浏览器以确保您正在查看最新信息。\n![png](../img/airflow/4.png)\n![png](../img/airflow/5.png)\n![png](../img/airflow/6.png)\n\n## step7 重新运行工作流程\n1. 单击create_dataproc_cluster图形。\n2. 单击清除以重置三个任务。\n3. 然后单击确定进行确认。请注意，create_dataproc_cluster周围的颜色已更改，状态为“正在运行”。\n![png](../img/airflow/7.png)\n\n我们可以看到dataproc具体的运行情况\n![png](../img/airflow/d-1.png)\n![png](../img/airflow/d-2.png)\n![png](../img/airflow/d-3.png)\n\njob完成后,GSC的情况\n![png](../img/airflow/gs-1.png)\n![png](../img/airflow/gs-2.png)\n![png](../img/airflow/gs-3.png)"
        },
        {
          "id": "/2020/6/12/快速搭建wordpress网站",
          "metadata": {
            "permalink": "/blog/2020/6/12/快速搭建wordpress网站",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-6-12-快速搭建wordpress网站.md",
            "source": "@site/blog/2020-6-12-快速搭建wordpress网站.md",
            "title": "云上快速搭建WordPress网站",
            "description": "我们将会在这台服务器上搭建和部署LAMP环境，然后安装WordPress网站，最后向大家展示如何在WordPress网站使用微博挂件和网站统计平台",
            "date": "2020-06-12T00:00:00.000Z",
            "formattedDate": "June 12, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "linux",
                "permalink": "/blog/tags/linux"
              }
            ],
            "readingTime": 3.785,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup"
              }
            ],
            "frontMatter": {
              "id": "WordPress",
              "title": "云上快速搭建WordPress网站",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "tags": [
                "facebook",
                "hello",
                "docusaurus",
                "google cloud",
                "linux"
              ]
            },
            "prevItem": {
              "title": "通过Composer使用Airflow运行一个workflow",
              "permalink": "/blog/2020/6/12/Airflow_Dataproc"
            },
            "nextItem": {
              "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
              "permalink": "/blog/2020/5/1/Kubernetes"
            }
          },
          "content": "我们将会在这台服务器上搭建和部署LAMP环境，然后安装WordPress网站，最后向大家展示如何在WordPress网站使用微博挂件和网站统计平台\n\n## step1 Login the vm instance\n\n![png](../img/aliyun/wordpress/1.png)\n\n## step2.1 搭建开发环境\n```\nyum -y install httpd\nyum -y install httpd-manual mod_ssl mod_perl mod_auth_mysql #安装 apache 的扩展文件。\nservice httpd start #启动 apache http 服务。\nchkconfig httpd on 设置开机自动启动 apache http 服务。\n```\n\n## step2.2 校验\n去公网ip查看\n\n## step2.3: 下载和安装MySQL数据库\n\n```\nyum -y install mysql mysql-server\nservice mysqld start #启动服务\nmysql_secure_installation #修改 MySQL 数据库 root 用户的密码，并提高 MySQL 数据库的安全性。\n```\n```\n# 输入以下命令\nmysql -uroot -p123\n\nshow databases;\n\ncreate database wordpress;\n\nshow databases;\n\nexit\n\nchkconfig mysqld on #设置开机自动启动MySQL服务\n```\n![png](../img/aliyun/wordpress/2.png)\n\n\n## step2.4: 安装PHP语言环境\n```\nyum -y install php php-mysql\nyum -y install gd php-gd gd-devel php-xml php-common php-mbstring php-ldap php-pear php-xmlrpc php-imap #安装 php 常用扩展包\nservice httpd restart #重启服务,这步很重要,必须重启\n\necho \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php #执行此命令，创建一个 php 页面，测试 PHP 环境\n```\n\n## step2.5 在浏览器测试php页面\n```\n弹性ip/phpinfo.php\n```\n![png](../img/aliyun/wordpress/3.png)\n\n## step3 安装部署wordpress\n下载中文版WordPress安装包，请点击链接 https://cn.wordpress.org/，这个是WordPress中文官网，可以找到最新的版本并下载安装； 下载完成后,解压\n```\ntar -xzf wordpress-4.7.4-zh_CN.tar.gz\nls\n```\n### 备份 WordPress 配置文件，并将原有的示例配置文件样本保留。\n```\ncd wordpress\ncp wp-config-sample.php wp-config.php\n```\n### 进入 wp-config.php 的编辑页面：\n```\nvim wp-config.php\n```\n### 按键盘 i ，进入编辑状态，修改配置文件的数据库信息：修改 DB_NAME 的参数值 database_name_here 为之前创建的数据库 wordpress：\n```\ndefine('DB_NAME', 'wordpress');\n```\n\n### 修改 DB_USER 的参数值 username_here 为 root :\n```\ndefine('DB_USER', 'root');\n```\n\n### 修改 DB_PASSWORD 的参数值 password_here 为 123 :\n\n### 修改完毕后，点击 esc ，退出编辑状态，然后输入 :wq ，保存修改信息并退出配置文件\n\n### 在 Apache 的根目录 /var/www/html 下，创建一个 wp-blog 文件夹。\n```\nmkdir /var/www/html/wp-blog\n```\n### 然后，将 wordpress 迁移到这个新建文件夹中。\n```\nmv * /var/www/html/wp-blog/\n```\n\n### 完成如上配置后，返回浏览器，并访问 http://xxx.xxx.xx.x/wp-blog/wp-admin/install.php ，其中 xxx.xxx.xx.x 为 ECS 实例的 弹性IP ，填写如下信息，完成后，点击页面底部的 安装WordPress ，开始安装 WordPress 。\n![png](../img/aliyun/wordpress/4.png)\n\n获得密码: MsLE1ppUhzV!95xOyq\n\n![png](../img/aliyun/wordpress/5.png)\n![png](../img/aliyun/wordpress/6.png)\n\n## step4 使用CNZZ帮你成为合格“站长”\n1. 本小节介绍主要：借助 CNZZ  平台观察 WordPress 网站一天有多少 IP 访问，那些 IP 都是从哪个页面进入到自己网站的等内容。\n\n2. 点击链接 https://web.umeng.com/main.php?c=user&a=login 进行注册、登录。\n\n登录网址(https://workbench.umeng.com/)\n\n3.  登录 CNZZ 数据统计专家网站后，填写以下信息，完成后点击 确认添加站点 。\nhttps://web.umeng.com/main.php?c=site&a=add\n\n![png](../img/aliyun/wordpress/7.png)\n![png](../img/aliyun/wordpress/8.png)\n![png](../img/aliyun/wordpress/9.png)\n\n4. 切换回 WordPress 网站的主页面，点击 外观 ，并选择子菜单下的 小工具\n![png](../img/aliyun/wordpress/10.png) \n5. 嵌入代码\n![png](../img/aliyun/wordpress/11.png) \n![png](../img/aliyun/wordpress/12.png) \n![png](../img/aliyun/wordpress/13.png) \n![png](../img/aliyun/wordpress/14.png)  \n\n6. 删除统计\n![png](../img/aliyun/wordpress/15.png)"
        },
        {
          "id": "/2020/5/1/Kubernetes",
          "metadata": {
            "permalink": "/blog/2020/5/1/Kubernetes",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-5-1-Kubernetes.md",
            "source": "@site/blog/2020-5-1-Kubernetes.md",
            "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
            "description": "link to 谷歌云快速入门(二) 存储文件然后共享",
            "date": "2020-05-01T00:00:00.000Z",
            "formattedDate": "May 1, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "linux",
                "permalink": "/blog/tags/linux"
              }
            ],
            "readingTime": 3.695,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup"
              }
            ],
            "frontMatter": {
              "id": "Kubernetes1",
              "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "tags": [
                "facebook",
                "hello",
                "docusaurus",
                "google cloud",
                "linux"
              ]
            },
            "prevItem": {
              "title": "云上快速搭建WordPress网站",
              "permalink": "/blog/2020/6/12/快速搭建wordpress网站"
            },
            "nextItem": {
              "title": "linux高级用法",
              "permalink": "/blog/2020/4/30/linux命令"
            }
          },
          "content": "link to [谷歌云快速入门(二) 存储文件然后共享](GoogleCloudStorage)\n\nlink to [谷歌云快速入门(四) 训练TensorFlow模型](TensorFlow)\n\nlink to [Container Registry 快速入门快速入门](docker)\n\nlink to [docker知识1](dockerhub)\n\nlink to [docker知识2](dockerhub_2)\n\n## step 1 选择项目,启动Kubernetes Engine页面\n![png](../img/kubernetes/1_create_project.png)\n<!--truncate-->\n![png](../img/kubernetes/2_create_API.png)\n\n![png](../img/kubernetes/3_Kubernetes.png)\n\n![png](../img/kubernetes/4_activate_API.png)\n\n\n![png](../img/kubernetes/5_kubernetes.png)\n\n\n## step 2 配置gcloud工具的默认设置\n如需设置默认项目，请在 Cloud Shell 中运行以下命令：\n```python\ngcloud config set project project-id\n```\n如需设置默认计算地区，请运行以下命令：\n```python\ngcloud config set compute/zone compute-zone\n```\n![png](../img/kubernetes/2.png)\n\n## step3 创建GKE集群\n一个集群包含至少一台群主实例机器和多台工作器机器,这些工作机器称为\"节点\".节点是运行Kubernetes进程的Compute Engine虚拟机实例,如下图\n![png](../img/kubernetes/10_cluster.png)\n\n如需创建集群，请运行以下命令, 其中，cluster-name 是集群选择的名称。：\n```python\ngcloud container clusters create cluster-name\n```\n\n#### 获取用于该集群的身份验证凭据\n创建集群后,需要获取身份验证凭据以便与该集群交互,命令为:\n```python\ngcloud container clusters get-credentials cluster-name\n```\n此命令将 kubectl 配置为使用您创建的集群。\n\n### step4 创建Deployment\n\n如果需要在集群中运行应用,需要运行一下命令:\n```python\nkubectl create deployment abc-server --image=gcr.io/clean-mountain-272313/flybirdgroup/classifier:latest\n```\n这个Kubernetes命令kubectl create deployment 会创建名为 abc-server 的 Deployment. 这个Deployment的Pod在其容器中运行hello-app映像\n\n在此命令中:\n--image 指定了要部署的容器镜像. 这个命令会从Container Registry(私有容器映像注册表)拉取gcr.io/clean-mountain-272313/flybirdgroup/classifier:latest\n\n如何创建镜像和上传到私有容器镜像注册表\n\nlink to [Container Registry 快速入门快速入门](docker)\n\nlink to [docker知识1](dockerhub)\n\nlink to [docker知识2](dockerhub_2)\n\n### step 5 公开deployment\n\n部署应用后,需要将其公开到互联网,以便用户访问该应用.我们可以通过创建Service来公开应用,这是一种Kubernetes资源,可以将应用公开给外部流量.\n```python\nkubectl expose deployment abc-server --type LoadBalancer \\\n  --port 80 --target-port 8080 (这里请注意,如果是flask应用,target-port 选择5000)\n```\n#### 检查和查看应用\n```python\nkubectl get pods (查看正在运行的pod)\n```\n![png](../img/kubernetes/3.png)\n如果status 是 Running 和 Ready的状态是1/1, 就可以进行下一步\n\n#### 使用 kubectl get service 检查 abc-server Service：\n```python\nkubectl get service abc-server \n```\n通过这个命令可以得到external-ip,复制service的外部ip地址,替换external-ip\n```python\nhttp://external-ip/\n```\n这样就想GKE(google Kebernetes Engine部署了一个容器化web应用)\n\n![png](../img/kubernetes/7.png)\n\n![png](../img/kubernetes/8.png)\n\n# 查看service\n可以查看kubernetes的所有service\n```kubernetes\nkubectl get service # \n```\n这些service非常重要,因为pod之间的相连就是通过这些service的\n\n# 删除pod\n```\nkubectl delete pod jenkins2-8698b5449c-grbdm(pod的名字)\nkubectl get pod \n```\n我们会发现pod没有被删除,这时候我们要输入一下命令\n```\nkubectl get deployment\nkubectl delete deployment 名字name\nkubectl get pod\n```\n就完全删除了"
        },
        {
          "id": "/2020/4/30/linux命令",
          "metadata": {
            "permalink": "/blog/2020/4/30/linux命令",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-30-linux命令.md",
            "source": "@site/blog/2020-4-30-linux命令.md",
            "title": "linux高级用法",
            "description": "linux一些好玩的命令比如循环,重定向",
            "date": "2020-04-30T00:00:00.000Z",
            "formattedDate": "April 30, 2020",
            "tags": [
              {
                "label": "linux",
                "permalink": "/blog/tags/linux"
              },
              {
                "label": "command line",
                "permalink": "/blog/tags/command-line"
              },
              {
                "label": "cat",
                "permalink": "/blog/tags/cat"
              },
              {
                "label": "<<EOF",
                "permalink": "/blog/tags/eof"
              },
              {
                "label": "loop",
                "permalink": "/blog/tags/loop"
              }
            ],
            "readingTime": 0.78,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "linux",
              "title": "linux高级用法",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "linux",
                "command line",
                "cat",
                "<<EOF",
                "loop"
              ]
            },
            "prevItem": {
              "title": "使用GKE(Google Kubernetes Engine)部署容器化应用 谷歌云快速入门(三)",
              "permalink": "/blog/2020/5/1/Kubernetes"
            },
            "nextItem": {
              "title": "python读取bigquery",
              "permalink": "/blog/2020/4/30/python读取Biggquery"
            }
          },
          "content": "linux一些好玩的命令比如循环,重定向\n\n# 循环\n```linux\nfor i in {1..50}\ndo\necho i\ndone\n```\n<!--truncate-->\n# 重定向 |, <<, >>, >, >>是a,累积写入, >是w,覆盖写入\n就是如果一开始没有这个文件,执行>>和>是一样的,但是如果再执行一次,执行>后,文件内容一样,但是>>就有两次原来的内容,\n```linxus\ncat <<EOF > b.txt\nhaha\nlala\nmama\nbaba\ngege\njiejie\nEOF\n\ncat <<EOF >> b.txt\nhaha\nlala\nmama\nbaba\ngege\njiejie\nEOF\n```\n所以我们这里可以用循环来\n```\nfor i in {1..50}\ndo\ncat <<EOF > b.txt\nfield_$i,decimal,\"12,49\"\nEOF\ndone\n```\n\n```\nfor i in {1..50}\ndo\ncat <<EOF >> b.txt\nfield_$i,decimal,\"12,49\"\nEOF\ndone\n```"
        },
        {
          "id": "/2020/4/30/python读取Biggquery",
          "metadata": {
            "permalink": "/blog/2020/4/30/python读取Biggquery",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-30-python读取Biggquery.md",
            "source": "@site/blog/2020-4-30-python读取Biggquery.md",
            "title": "python读取bigquery",
            "description": "安装客户端",
            "date": "2020-04-30T00:00:00.000Z",
            "formattedDate": "April 30, 2020",
            "tags": [
              {
                "label": "linux",
                "permalink": "/blog/tags/linux"
              },
              {
                "label": "command line",
                "permalink": "/blog/tags/command-line"
              },
              {
                "label": "cat",
                "permalink": "/blog/tags/cat"
              },
              {
                "label": "<<EOF",
                "permalink": "/blog/tags/eof"
              },
              {
                "label": "loop",
                "permalink": "/blog/tags/loop"
              }
            ],
            "readingTime": 0.47,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "bigquery1",
              "title": "python读取bigquery",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "linux",
                "command line",
                "cat",
                "<<EOF",
                "loop"
              ]
            },
            "prevItem": {
              "title": "linux高级用法",
              "permalink": "/blog/2020/4/30/linux命令"
            },
            "nextItem": {
              "title": "设立Jenkins在GKE上",
              "permalink": "/blog/2020/4/30/创建jenkins在GKE"
            }
          },
          "content": "## 安装客户端\n```\npip install --upgrade google-cloud-bigquery[bqstorage,pandas]\n```\n<!--truncate-->\n## 导入工具包,创建客户端对象\n```\nfrom google.cloud import bigquery\n\nclient = bigquery.Client()\n```\n## 运行查询\n```\nquery_job = client.query(\"\"\"\n    SELECT\n      CONCAT(\n        'https://stackoverflow.com/questions/',\n        CAST(id as STRING)) as url,\n      view_count\n    FROM `bigquery-public-data.stackoverflow.posts_questions`\n    WHERE tags like '%google-bigquery%'\n    ORDER BY view_count DESC\n    LIMIT 10\"\"\")\n\nresults = query_job.result()  # Waits for job to complete.\n```\n\n## 转成dataframe\n```\ndf = results.to_dataframe()\n```\n\n## 显示查询结果\n```\ndf.iloc[:,:5] #显示dataframe\n```"
        },
        {
          "id": "/2020/4/30/创建jenkins在GKE",
          "metadata": {
            "permalink": "/blog/2020/4/30/创建jenkins在GKE",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-30-创建jenkins在GKE.md",
            "source": "@site/blog/2020-4-30-创建jenkins在GKE.md",
            "title": "设立Jenkins在GKE上",
            "description": "设置zone和下载代码",
            "date": "2020-04-30T00:00:00.000Z",
            "formattedDate": "April 30, 2020",
            "tags": [
              {
                "label": "Jenkins",
                "permalink": "/blog/tags/jenkins"
              },
              {
                "label": "GKE",
                "permalink": "/blog/tags/gke"
              }
            ],
            "readingTime": 1.91,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "jenkins",
              "title": "设立Jenkins在GKE上",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "Jenkins",
                "GKE"
              ]
            },
            "prevItem": {
              "title": "python读取bigquery",
              "permalink": "/blog/2020/4/30/python读取Biggquery"
            },
            "nextItem": {
              "title": "Bigquery常用命令",
              "permalink": "/blog/2020/4/29/bq_常用命令"
            }
          },
          "content": "设置zone和下载代码\n```\ngcloud config set compute/zone us-east1-d\ngit clone https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.git\ncd continuous-deployment-on-kubernetes\n```\n<!--truncate-->\n创建GKE集群\n```\ngcloud container clusters create jenkins-cd \\\n--num-nodes 2 \\\n--machine-type n1-standard-2 \\\n--scopes \"https://www.googleapis.com/auth/projecthosting,cloud-platform\"\n```\n查询cluster是否运行\n```\ngcloud container clusters list\n```\n\n获取凭证去连接你的GKE\n```\ngcloud container clusters get-credentials jenkins-cd\n```\n确认是否能连接到集群\n```\nkubectl clusters-info\n```\n\n安装Helm\n```\nwget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz\n```\n解压\n```\ntar zxfv helm-v2.9.1-linux-amd64.tar.gz\ncp linux-amd64/helm .\n```\n\n添加自己作为云的管理者🙆给jenkins权限到集群\n```\nkubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)\n```\n\n给Tiller集群管理者权限\n```\nkubectl create serviceaccount tiller --namespace kube-system\nkubectl create clusterrolebinding tiller-admin-binding --\nclusterrole=cluster-admin --serviceaccount=kube-system:tiller\n```\n\n初始化helm,这样可以保证Helm(Tiller)服务端是正确安装到集群上\n```\n./helm init --service-account=tiller\n./helm repo update\n```\n\n确认helm是否安装成功,应该看到出现在服务端和客户端是v2.9.1\n```\n./helm version\n```\n\n\n用Helm CLI命令去部署设置\n```\n./helm install -n cd stable/jenkins -f jenkins/values.yaml --version 0.16.6 --wait\n```\n\n确认jenkins pod运行\n```\nkubectil get pods\n```\n\n运行命令去设置jenkins UI界面\n```\nexport POD_NAME=$(kubectl get pods -l \"component=cd-jenkins-master\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl port-forward $POD_NAME 8080:8080 >> /dev/null &\n```\n\n测试Jenkins Service是否创建正确\n```\nkubectl get svc\n```\n\n连接到Jenkins-Jenkins会自动创建密码,获取它,运行一下命令:\n```\nprintf $(kubectl get secret cd-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n```\n\n我这边出现的密码是\n```\nJrK3zwoGFI\n```\n然后去gcloud shell 的Web Preview上选择Preview on Port 8080,\n![png](../img/Jenkins/1.png)\n然后输入\n```\nusername: admin\npassword: JrK3zwoGFI\n```\n![png](../img/Jenkins/2.png)\n\n\n## 删除命令:\n```\nkubectl get deployment\nkubectl delete deployment 名字name\nkubectl get pod \n\n#删除pod后,删除clusters\n```\ngcloud containers clusters delete name\n```\n![png](../img/Jenkins/3.png)"
        },
        {
          "id": "/2020/4/29/bq_常用命令",
          "metadata": {
            "permalink": "/blog/2020/4/29/bq_常用命令",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-29-bq_常用命令.md",
            "source": "@site/blog/2020-4-29-bq_常用命令.md",
            "title": "Bigquery常用命令",
            "description": "查看数据集的schema 使用bq show 数据集",
            "date": "2020-04-29T00:00:00.000Z",
            "formattedDate": "April 29, 2020",
            "tags": [
              {
                "label": "gcp",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "command line",
                "permalink": "/blog/tags/command-line"
              },
              {
                "label": "github",
                "permalink": "/blog/tags/github"
              }
            ],
            "readingTime": 2.435,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "bq_command_line",
              "title": "Bigquery常用命令",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "gcp",
                "command line",
                "github"
              ]
            },
            "prevItem": {
              "title": "设立Jenkins在GKE上",
              "permalink": "/blog/2020/4/30/创建jenkins在GKE"
            },
            "nextItem": {
              "title": "GCS 读写 by python",
              "permalink": "/blog/2020/4/29/_GCS读写by python"
            }
          },
          "content": "## 查看数据集的schema 使用bq show 数据集\n\n<!--truncate-->\n\n# 增 bq mk, bk load [具体可以参考](https://cloud.google.com/bigquery/docs/loading-data-local#cli_1)\n```bq\nbq mk babaynames 创建新的数据集,在自己项目中命名\n#### 创建表格 bq load datasetID tableID data_file Schema\nbq load 数据集ID.表格ID(如果空就自己创建一个)  数据文件(比如txt) schema类型\nbq load datasetID.tableid your_file name:string,gender:string,count:integer(your schema)\nbq load --source_format=AVRO fakedatas.customer \"gs://zz_bucket/avro/cust.avro\" (avro文件)\n```\n还可以改用 --autodetect 标志，而无需提供架构定义。\n\n# 删 bq rm -r\n## 删除数据集 bq rm\nbq rm -r 数据集id\n```\nbq rm -r babynames\n```\n# 显示 bq ls, bq show\n### 检测表格是否创建成功或者更新成功\n创建后,可以通过bq ls 数据集id 查看是否创建成功\n也可以通过bq show 数据集id.tableid 查看schema\n```\nbq ls 显示所有dataset\nbq ls babaynames 显示datasetid的所有table\nbq show datasetid 显示dataset的ACL(权限) 比如可以看到 谁是owners,writers,readers\nbq show babynames.names2010 显示table的schema类型,多少文本数,字段\n```\n\n# 查 bq query --use_legacy_sql=false 'select * from dataset.table'\n```\n## 运行sql语句命令\n总体格式就是 bq query --use_legacy_sql=false 'select 字段 耦合字段(比如count(*)) from datasetid.tableid where 条件 order by 字段 (做排序ASC,DESC) Limit 数字(限制条数)'\n注意的地方是 \n### use_legacy_sql=false 表示使用标准sql语句\n### 条件的时候可以使用双引号做区分\"\"\n```sql\nbq query \"select name,count from babynames.names2010 where gender = 'F' Order by count desc limit 5\"\n\nbq query --use_legacy_sql=false 'SELECT word,SUM(word_count) AS count FROM `bigquery-public-data`.samples.shakespeare WHERE word LIKE \"%raisin%\" GROUP BY word'\n\n第二个例子是选择从bq公共集中选择samples这个dataset,然后从这个dataset的shakespeare表中选择word字段, 执行条件是 word字段必须等于\"huzzah\"\nbq query --use_legacy_sql=false 'SELECT word FROM `bigquery-public-data`.samples.shakespeare WHERE word = \"huzzah\"'\n```\n\n## 运行帮助命令\n```\nbq help query\n```\n\n### 下载数据到项目中\n使用wget 数据源连接(比如zip,txt,csv等)\n```wget\nwget http://www.ssa.gov/OACT/babynames/names.zip\n```\n### 解压缩zip文件到项目中\n```\nunzip names.zip\n```"
        },
        {
          "id": "/2020/4/29/_GCS读写by python",
          "metadata": {
            "permalink": "/blog/2020/4/29/_GCS读写by python",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-29_GCS读写by python.md",
            "source": "@site/blog/2020-4-29_GCS读写by python.md",
            "title": "GCS 读写 by python",
            "description": "我们可以使用谷歌的Python客户端API将文件上传到谷歌云存储。",
            "date": "2020-04-29T00:00:00.000Z",
            "formattedDate": "April 29, 2020",
            "tags": [
              {
                "label": "terraform",
                "permalink": "/blog/tags/terraform"
              },
              {
                "label": "gcp",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "compute engine",
                "permalink": "/blog/tags/compute-engine"
              },
              {
                "label": "vm",
                "permalink": "/blog/tags/vm"
              }
            ],
            "readingTime": 1.14,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "GCS1",
              "title": "GCS 读写 by python",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "terraform",
                "gcp",
                "compute engine",
                "vm"
              ]
            },
            "prevItem": {
              "title": "Bigquery常用命令",
              "permalink": "/blog/2020/4/29/bq_常用命令"
            },
            "nextItem": {
              "title": "gsutil常用命令",
              "permalink": "/blog/2020/4/29/_gsutil常用命令"
            }
          },
          "content": "我们可以使用谷歌的Python客户端API将文件上传到谷歌云存储。\n\n\n# 方法一: \n首先，如下安装api客户端。\n\n>\n```\npip install --upgrade google-api-python-client \n```\n然后，启用api身份验证以获取应用程序默认凭据。\n\n>\n```\ngcloud beta auth application-default login \n```\n以下是使用应用程序默认凭据将本地文件上传到Google云存储的示例代码。\n\n```\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\n\ncredentials = GoogleCredentials.get_application_default()\nservice = discovery.build('storage', 'v1', credentials=credentials)\n\nfilename = './account_id_schema_new.csv'\n\nbucket = 'bq-pandas-bucket'\n\nbody = {'name': 'dest_file_name.csv'}\n\nreq = service.objects().insert(bucket=bucket, body=body, media_body=filename)\nesp = req.execute()\n```\n\n# 方法二--仅仅需要安装两个包\n```\npip install dask[dataframe] --upgrade --user\npip install gcsfs --user\n```\n\n## 例子\n![png](../img/google/gcs/gcs1/1.png)\n\n原来无论用pandavro或者pandas都是读取不了GCS的文件的\n\n但是安装包后,就可以读取GCS文件了\n![png](../img/google/gcs/gcs1/2.png)\n\n然后写入gcs,就想平时那样,直接to_csv就可以了\n![png](../img/google/gcs/gcs1/3.png)\n\n```\nimport dask.bag as db\nb = db.read_avro('gs://zz_mm_bucket/account_id_schema_new.avro')\ndf = b.to_dataframe()\n```"
        },
        {
          "id": "/2020/4/29/_gsutil常用命令",
          "metadata": {
            "permalink": "/blog/2020/4/29/_gsutil常用命令",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-29_gsutil常用命令.md",
            "source": "@site/blog/2020-4-29_gsutil常用命令.md",
            "title": "gsutil常用命令",
            "description": "gsutil命令快速入门",
            "date": "2020-04-29T00:00:00.000Z",
            "formattedDate": "April 29, 2020",
            "tags": [
              {
                "label": "terraform",
                "permalink": "/blog/tags/terraform"
              },
              {
                "label": "gcp",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "compute engine",
                "permalink": "/blog/tags/compute-engine"
              },
              {
                "label": "vm",
                "permalink": "/blog/tags/vm"
              }
            ],
            "readingTime": 1.12,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "gsutil",
              "title": "gsutil常用命令",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "terraform",
                "gcp",
                "compute engine",
                "vm"
              ]
            },
            "prevItem": {
              "title": "GCS 读写 by python",
              "permalink": "/blog/2020/4/29/_GCS读写by python"
            },
            "nextItem": {
              "title": "使用terraform基本语法和创建vm",
              "permalink": "/blog/2020/4/27/_terraform"
            }
          },
          "content": "[gsutil命令快速入门](https://cloud.google.com/storage/docs/quickstart-gsutil)\n\n<!--truncate-->\n```\nmb \ngsutil mb -l us-east1 gs://my-awesome-bucket/ #创建bucket\n\ncp 复制下载\ngsutil cp Desktop/kitten.png gs://my-awesome-bucket 复制文件到bucket\ngsutil cp gs://my-awesome-bucket/kitten.png Desktop/kitten2.png 从bucket上下载文件到本地\ngsutil cp gs://my-awesome-bucket/kitten.png gs://my-awesome-bucket/just-a-folder/kitten3.png 复制bucket里的文件到bucket里面的文件夹中\n```\n<!--truncate-->\n```\nls 列举\ngsutil ls gs://my-awesome-bucket 列举bucket内容\ngsutil ls -l gs://my-awesome-bucket/kitten.png 列举bucket文件的详细信息\n\nacl account credential limit 权限\ngsutil acl ch -u AllUsers:R gs://my-awesome-bucket/kitten.png\n使用 gsutil acl ch 命令向所有用户授予存储在存储分区中的对象的读取权限\ngsutil acl ch -d AllUsers gs://my-awesome-bucket/kitten.png\n使用 gsutil acl ch 命令向所有用户移除存储在存储分区中的对象的读取权限\n\niam -向某人授予和移除您的存储分区的访问权限\ngsutil iam ch user:jane@gmail.com:objectCreator,objectViewer gs://my-awesome-bucket\ngsutil iam ch -d user:jane@gmail.com:objectCreator,objectViewer gs://my-awesome-bucket\n\nrm 清除\ngsutil rm gs://my-awesome-bucket/kitten.png 删除文件\n\ngsutil rm -r gs://my-awesome-bucket 清除bucket\n```"
        },
        {
          "id": "/2020/4/27/_terraform",
          "metadata": {
            "permalink": "/blog/2020/4/27/_terraform",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-27_terraform.md",
            "source": "@site/blog/2020-4-27_terraform.md",
            "title": "使用terraform基本语法和创建vm",
            "description": "我觉得terraform的基本语法就是字典对象式语法.",
            "date": "2020-04-27T00:00:00.000Z",
            "formattedDate": "April 27, 2020",
            "tags": [
              {
                "label": "terraform",
                "permalink": "/blog/tags/terraform"
              },
              {
                "label": "gcp",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "compute engine",
                "permalink": "/blog/tags/compute-engine"
              },
              {
                "label": "vm",
                "permalink": "/blog/tags/vm"
              }
            ],
            "readingTime": 9.965,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "terraform1",
              "title": "使用terraform基本语法和创建vm",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "terraform",
                "gcp",
                "compute engine",
                "vm"
              ]
            },
            "prevItem": {
              "title": "gsutil常用命令",
              "permalink": "/blog/2020/4/29/_gsutil常用命令"
            },
            "nextItem": {
              "title": "关于nginx和port知识总结",
              "permalink": "/blog/2020/4/26/关于nginx和port知识总结"
            }
          },
          "content": "我觉得terraform的基本语法就是字典对象式语法.\n\n总体结构就是 \n1. instance.tf 用来创作资源的文件,比如vm instance,storage bucket,bigquery等\n\n2. variables.tf 参数变量文件,用来写那些变量的,变量的格式为 variable \"project\" {} 就是: variable + 名字 + {}\n\n3. terraform.tfvars 用来写变量具体等于什么的, 比如 project = \"terraform-45110831\"\n\n4. output.tf 在创建实例后,打印输出我们觉得重要的参数, 具体格式 output 名字 {value = \"${变量}\"} 如output \"ip\" {value = \"${google_compute_address.vm_static_ip.address}\"} 口诀: 2美元1file4引号,简称\"214\"!!\n\n<!--truncate-->\n\n## 其中详细的用法我们可以去[官网9步教程学习](https://learn.hashicorp.com/terraform/gcp/change),非常简单,但是很多坑,但是走完坑后,感觉就基本上手了\n\n## 其中5个点需要注意的是: \n\n1. 变量的写法 \"${var.project}\"\n   \n2. 如果变量是个文件的话: \"${file(\"${var.jupyter_sh}\")}\"  ${file(\"${var.jupyter.sh}\")} 也就是 \"${file(\"${变量}\")}\"\n   \n3. 还有就是字典属性都是用.来引用\n   \n4. tag的使用,因为tag属性的添加对于vm instance尤其重要\n\n5. 使用meta_startup_script脚本,可以在创建vm instance后通过脚本运行程序,非常有用\n   \n## 使用vs code连接远程服务器\n\n在vs code 界面 按fn+F1, \n```\nF1\nssh -i ~/.ssh/id_rsa flybird@34.73.166.222 (连接远程服务器)\n```\n连接成功后,我们就可以在服务器准备创建vm instance了\n# 使用terraform创建VM\n使用terraform可以很快的,可复制性的配置一个vm机器\n<!--truncate-->\n# 1 首先我们要安装terraform,安装的具体教程请看[terraform安装](https://learn.hashicorp.com/terraform/gcp/install)\n\n大致步骤是\n```\n下载 terraform_0.12.24_linux_amd64.zip\nunzip terraform_0.12.24_linux_amd64.zip\nsudo snap install terraform 安装terraform\nterraform 确认terraform是否安装成功\n```\n\nterraform安装好后,我们去GCP>>APIs$Services>>Credentials创建service account,下载json文件,保存好\nterraform创建教程我们可以去[terraform入门教程](https://learn.hashicorp.com/terraform/gcp/change)看到\n\n## 2 我们这里会创建一个instance.tf文件\n```\nprovider \"google\" {\n  credentials = \"terraform-45110831-450974fa2608.json\"\n  project = \"terraform-45110831\"\n  region  = \"us-central1\"\n  zone    = \"us-central1-a\"\n}\nresource \"google_compute_network\" \"vpc_network\" {\n  name = \"terraform-network\"\n}\nresource \"google_compute_instance\" \"default\" {\n  project      = \"terraform-45110831\"\n  name         = \"terraform\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n\n  boot_disk {\n    initialize_params {\n      image = \"debian-cloud/debian-9\"\n    }\n  }\n\n  network_interface {\n    network = \"default\"\n    access_config {\n    }\n  }\n}\n```\n\nresource 后面跟两个字段,第一个是resource type字段和resource name, 这里的resource type是google_compute_instance, name是 terraform, google_compute_instance 会自动告诉terraform这个是谷歌的provider\n\n## 3 写好配置后,我们输入一下命令\n```\nterraform init\n```\n这个命令或初始化很多后续会被用到的命令的本地设置和数据\ngoogle provider plugin 会被下载和安装\n\n## 4 初始化后,我们可以查看整体计划,查看设置是否符合我们的要求,输入这个命令的时候,GCP不会部署vm\n```\nterraform plan\n```\n\n## 5 觉得没有问题,我们就可以应用了\n```\nterraform apply\n```\n这个命令就会执行之前plan的东西\n```\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # google_compute_instance.default will be created\n  + resource \"google_compute_instance\" \"default\" {\n      + can_ip_forward       = false\n      + cpu_platform         = (known after apply)\n      + deletion_protection  = false\n      + guest_accelerator    = (known after apply)\n      + id                   = (known after apply)\n      + instance_id          = (known after apply)\n      + label_fingerprint    = (known after apply)\n      + machine_type         = \"n1-standard-1\"\n      + metadata_fingerprint = (known after apply)\n      + name                 = \"terraform\"\n      + project              = \"qwiklabs-gcp-42390cc9da8a4c4b\"\n      + self_link            = (known after apply)\n      + tags_fingerprint     = (known after apply)\n      + zone                 = \"us-central1-a\"\n\n      + boot_disk {\n          + auto_delete                = true\n          + device_name                = (known after apply)\n          + disk_encryption_key_sha256 = (known after apply)\n          + kms_key_self_link          = (known after apply)\n          + source                     = (known after apply)\n\n          + initialize_params {\n              + image  = \"debian-cloud/debian-9\"\n              + labels = (known after apply)\n              + size   = (known after apply)\n              + type   = (known after apply)\n            }\n        }\n\n      + network_interface {\n          + address            = (known after apply)\n          + name               = (known after apply)\n          + network            = \"default\"\n          + network_ip         = (known after apply)\n          + subnetwork         = (known after apply)\n          + subnetwork_project = (known after apply)\n\n          + access_config {\n              + assigned_nat_ip = (known after apply)\n              + nat_ip          = (known after apply)\n              + network_tier    = (known after apply)\n            }\n        }\n\n      + scheduling {\n          + automatic_restart   = (known after apply)\n          + on_host_maintenance = (known after apply)\n          + preemptible         = (known after apply)\n\n          + node_affinities {\n              + key      = (known after apply)\n              + operator = (known after apply)\n              + values   = (known after apply)\n            }\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value:\n```\n\n我们看到 +, 这意味Terraform会创建这个资源,我们能看到很多属性,如果有的value是显示computed,这意味着这value会等到resource创建后才知道\n\n## 6 显示vm 的配置\n```\nterraform show\n```\n\n## 7.1 我们可以使用terraform去修改resources\n比如我们可以在 resource google_compute_instance 上添加tags\n### 敲黑板!!! 如果tags上添加\"http-server\",\"https-server\"后,Firewalls防火墙就allow http traffic和 https traffic了\n\n```\nresource \"google_compute_instance\" \"default\" {\n  project      = \"terraform-45110831\"\n  name         = \"terraform\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  tags         = [\"web\", \"dev\",\"http-server\",\"https-server\"]\n```\n然后在终端输入terraform init, terraform plan后,就会出现\n```\nTerraform will perform the following actions:\n\n  ~ google_compute_instance.default\n    tags.#:         \"0\" => \"2\"\n    tags.292811013: \"\" => \"dev\"\n    tags.365508689: \"\" => \"web\"\n```\nterraform apply 就会对resource进行修改\n\n## 7.2 也支持破坏性修改,比如更换磁盘镜像,比如从Debian 9映像更改为使用Google的Container-Optimized OS\n```\nboot_disk {\n    initialize_params {\n      image = \"cos-cloud/cos-stable\"\n    }\n  }\n```\nterrform init> terrform plan >terrform apply后\n我们可以看到internal IP 和 external ip改变了,然后我们登陆进vm instance后,我们之前的所以资料都没有了\n<h1> 所以如果进行destructive change前,一定要注意 </h1>\n\n\n## 8 terraform destroy\n输入命令后:\n```\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  - destroy\n\nTerraform will perform the following actions:\n\n  - google_compute_instance.default\n\n  - google_compute_network.vpc_network\n```\nterraform destroy命令销毁资源,该命令类型于terraform apply,terraform确定必须销毁物件的顺序,如果还有资源,GCP将不允许删除VPC网络,因此Terraform会等到实例被销毁后再销毁网络,terraform执行操作的时候,terraform将建一个依赖关系图已确定销毁顺序.在具有多个资源的更复杂的情况下,Terraform将在安全的情况下并行执行操作.\n\n\n## GCP也可以删除\n我们可以去GCP shell中输入一下命令\n```\ngcloud compute instances delete instance名字\n```\n\n我们还可以使resource与resource之间相互依赖,使用depends_on = [\"\"]\n比如我可以现在创建一个storage bucket, 我们可以设置在创建storage bucket前一定要创建 vm先\n\nresource \"google\n```\nresource \"google_compute_instance\" \"default\" {\n  project      = \"terraform-45110831\"\n  name         = \"terraform\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  tags         = [\"web\", \"dev\"]\n}\n\nresource \"google_storage_bucket\" \"example_bucket\" {\n  depends_on = [\"google_compute_instance.default\"]\n  ...\n}\n```\n\n## 9 可以在本终端输入命令[Provisioners](https://www.terraform.io/docs/provisioners/index.html)\n```\nresource \"google_compute_instance\" \"vm_instance\" {\n  name         = \"terraform-instance\"\n  machine_type = \"f1-micro\"\n  tags         = [\"web\", \"dev\"]\n\n  provisioner \"local-exec\" {\n    command = \"echo ${google_compute_instance.vm_instance.name}:  ${google_compute_instance.vm_instance.network_interface.0.access_config.0.nat_ip} >> ip_address.txt\"\n  }\n```\n## 每个vm实例可以具有多个网络接口,因此我们的network interface.0就是引用第一个,每个网络接口也具有多个access_config快,我们选择acess_config.0指定引用第一个\n# terraform的核心在于变量,我个人感觉就像是javascript的字典对象,所以属性都用.来引用, 另外变量的表示方法为美元符号$ + {变量} 就是 ${google_compute_instance.vm_instance}\n## 但是你发现ip_address.txt在terraform apply后没有创建成功\n因为预配器provisioner只是在创建资源时运行,但是添加provisioner不会强制销毁和重新创建该资源,所以我们要用terraform taint告诉terraform,需要重新创建实例\n```\nterraform taint google_compute_instance.vm_instance\nterraform apply\ncat ip_address.txt\n```\n我们可以看到txt文件内容了\n\n\n## 10.1 创建vm instance成功后,我们需要添加sshkey,然后我们可以连接external ip,通过以下命令:\n```linux\nssh-keygen 产生id-rsa.pub, 复制其内容,在vm的meta data上粘贴好\nssh username@external_ip\n```\n## 10.2 如果出现permisson dennied,可以直接在vm机上把ssh key删掉,然后在本地重新创建ssh key,然后复制粘贴到 meta data上\n\n## 11 起始脚本metadata_startup_script\n我们在创建resouce google_compute_instance 的时候,我们可以添加一个metadata_startup_script属性,这个是用来运行在创建的vm虚拟机上的,比如我们可以通过这个脚本创建[jupyter和python的工具(请看案例)](jupyterhub_GCP)等,脚本是在root目录运行,\n\n### 如果我们想在home目录运行,请cd home/用户名\n```linux\nmetadata_startup_script = \"${file(\"${var.jupyter_sh}\")}\" \n```"
        },
        {
          "id": "/2020/4/26/关于nginx和port知识总结",
          "metadata": {
            "permalink": "/blog/2020/4/26/关于nginx和port知识总结",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-26关于nginx和port知识总结.md",
            "source": "@site/blog/2020-4-26关于nginx和port知识总结.md",
            "title": "关于nginx和port知识总结",
            "description": "port是service服务的端口",
            "date": "2020-04-26T00:00:00.000Z",
            "formattedDate": "April 26, 2020",
            "tags": [
              {
                "label": "nginx",
                "permalink": "/blog/tags/nginx"
              },
              {
                "label": "port",
                "permalink": "/blog/tags/port"
              },
              {
                "label": "target",
                "permalink": "/blog/tags/target"
              },
              {
                "label": "load-balance",
                "permalink": "/blog/tags/load-balance"
              }
            ],
            "readingTime": 3.155,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "nginx",
              "title": "关于nginx和port知识总结",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "nginx",
                "port",
                "target",
                "load-balance"
              ]
            },
            "prevItem": {
              "title": "使用terraform基本语法和创建vm",
              "permalink": "/blog/2020/4/27/_terraform"
            },
            "nextItem": {
              "title": "读取avro文件的schema",
              "permalink": "/blog/2020/4/21/python读取avro的schema"
            }
          },
          "content": "##  port是service服务的端口\n## targetport是pod也就是容器的端口\n## nodeport是容器所在宿主机的端口(通过service暴露给宿主机,而port去没有)\n<!--truncate-->\nport的主要作用是clusters集群里面的一个pod访问其他pod的时候,需要端口port,比如集群里面nginx需要访问mysql,那么久需要mysql的target port,\n```mysql\napiVersion: v1\n kind: Service\n metadata:\n  name: mysql-service\n spec:\n  ports:\n  - port: 33306\n    targetPort: 3306\n  selector:\n   name: mysql-pod\n```\n比如nginx通过访问service的33306端口,service根据selector中的name,将请求转发到mysql-pod这个pod的3306端口\n通过POST创建 service\n```\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"name\": \"http\",\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            },\n            {\n                \"name\": \"https\",\n                \"protocol\": \"TCP\",\n                \"port\": 443,\n                \"targetPort\": 9377\n            }\n        ]\n    }\n}\n```\n对于每个运行的pod,kubernete将其添加给现有的service的全局变量, 比如叫做\"redis-master\"的service,对外映射6379端口,\nservice要想被pod使用,必须先比pod建立\n所以总体流程就pod之间都是通过service来相互访问,所以先service的port,然后service通过selector找到name,再把请求发送到name对应的target port\n\n# [Nginx基础知识](https://www.cnblogs.com/mq0036/p/9794540.html)\n\n## nginx能做的事情:\n1 [正向,方向代理](https://www.jianshu.com/p/ae76c223c6ef)\n2 负载均衡\n3 http服务器-动静分离\n\n![png](../img/kubernetes/nginx/nginx.png)\n\n### 正向代理是nginx proxy给client代理,然后对接Server,获取内容\n### 方向代理是proxy给Server做代理,clients访问proxy获取内容\n\n### nginx的负载均衡有一下几个策略:\n1 RR- 按照请求时间顺序分配到不同的后端服务器,后端服务器挂掉,就自动剔除\n2 权重- 给不同服务器赋予权重,权重大的,就承当更多访问\n3 ip_hash- 因为客户登录信息保存在session中,如果跳转到另外一台服务器的时候,需要重新登录,所以ip_hash的方法让一个客户只访问一台服务器\n4 fair(第三方)- 按后端服务器的响应时间来分配请求,响应越短,就越先分配\n5 url_hash- 就是设定方向url连接是访问那台服务器,后端服务器为缓存时比较有效\n\n### 动静分离\nnginx本身就是一台服务器,所以可以保存一些静态资源,也就是我们可以把动态网站里的动态网页按照一定规则把不变的资源和经常变的资源区分开来,动静"
        },
        {
          "id": "/2020/4/21/python读取avro的schema",
          "metadata": {
            "permalink": "/blog/2020/4/21/python读取avro的schema",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-21-python读取avro的schema.md",
            "source": "@site/blog/2020-4-21-python读取avro的schema.md",
            "title": "读取avro文件的schema",
            "description": "首先安装包",
            "date": "2020-04-21T00:00:00.000Z",
            "formattedDate": "April 21, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "avro",
                "permalink": "/blog/tags/avro"
              },
              {
                "label": "bigquery",
                "permalink": "/blog/tags/bigquery"
              }
            ],
            "readingTime": 0.35,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "avro",
              "title": "读取avro文件的schema",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "avro",
                "bigquery"
              ]
            },
            "prevItem": {
              "title": "关于nginx和port知识总结",
              "permalink": "/blog/2020/4/26/关于nginx和port知识总结"
            },
            "nextItem": {
              "title": "把avro文件放到bigquery",
              "permalink": "/blog/2020/4/21/把avro文件放到bigquery"
            }
          },
          "content": "首先安装包\n```\npip install avro-python3\n```\n然后在py文件倒入包\n```\nfrom avro.datafile import DataFileReader, DataFileWriter\nimport avro.io\nreader = avro.datafile.DataFileReader(open('./account_id_schema_new.avro',\"rb\"),avro.io.DatumReader())\nschema = reader.meta\nprint(schema)\n```\n<!--truncate-->\n显示的效果如下:\n```\n{'avro.schema': b'{\"fields\": [{\"type\": [\"null\", \"string\"], \"name\": \"ACNO\"}, {\"type\": [\"null\", \"double\"], \"name\": \"FIELD_1\"}, {\"type\": [\"null\", \"double\"], \"name\": \"FIELD_2\"}, {\"type\": [\"null\", \"double\"], \"name\": \"FIELD_3\"}], \"type\": \"record\", \"name\": \"Root\"}',\n 'avro.codec': b'null'}\n ```"
        },
        {
          "id": "/2020/4/21/把avro文件放到bigquery",
          "metadata": {
            "permalink": "/blog/2020/4/21/把avro文件放到bigquery",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-21-把avro文件放到bigquery.md",
            "source": "@site/blog/2020-4-21-把avro文件放到bigquery.md",
            "title": "把avro文件放到bigquery",
            "description": "去到控制台,确认storage和bigquery API已启用",
            "date": "2020-04-21T00:00:00.000Z",
            "formattedDate": "April 21, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "avro",
                "permalink": "/blog/tags/avro"
              },
              {
                "label": "bigquery",
                "permalink": "/blog/tags/bigquery"
              }
            ],
            "readingTime": 0.915,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "avro",
              "title": "把avro文件放到bigquery",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "avro",
                "bigquery"
              ]
            },
            "prevItem": {
              "title": "读取avro文件的schema",
              "permalink": "/blog/2020/4/21/python读取avro的schema"
            },
            "nextItem": {
              "title": "用yaml配置文件传参数给pyspark,然后再dataproc运行",
              "permalink": "/blog/2020/4/18/dataproc+spark+yaml"
            }
          },
          "content": "去到控制台,确认storage和bigquery API已启用\n打开gcs\n设置project id\n```\ngcloud config set project project_id\n```\n<!--truncate-->\n\n在storage上创建bucket\n```\ngsutil mb gs://bucket_name\ngsutil ls #查看创建是否成功\n```\n```\n上传文件夹或者文件到storage bucket, 如果storage bucket没有这个文件名,就会创建一个\n```\n```\ngsutil cp -r faker_demo/data gs://z_bucket/sub_file\ngsutil ls gs://z_bucket/sub_file/* 参看所有文件\ngsutil rm gs://z_bucket/sub_file/.DS_Store #删除文件\n```\n去big query 创建dataset\n```\nbq mk fake_data\nbq ls #查看命令\n```\n创建biq query 表格 csv, 或者avro,注意--source_format 要大写AVRO,CSV, 如果csv类型的schema可以用--autodetect,\n```\nbq load --source_format=AVRO fake_data.account_id_schema \"gs://z_bucket/sub_file/avro_output/accountID.avro\"\n\nbq load --source_format=CSV fake_data.account_id_schema  \"gs://z_bucket/sub_file/input/account_id_schema.csv\" \n\nbq load --autodetect --source_format=CSV fake_data.account_id_schema \"gs://z_bucket/sub_file/output/accountID.csv\"\n```\n查看字段\n\n```\nbq show fake_data.account_id_schema\n```\n\n查询\n\n```\nbq query \"select * from fake_data.account_id_schema limit 5\"\n```"
        },
        {
          "id": "/2020/4/18/dataproc+spark+yaml",
          "metadata": {
            "permalink": "/blog/2020/4/18/dataproc+spark+yaml",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-18-dataproc+spark+yaml.md",
            "source": "@site/blog/2020-4-18-dataproc+spark+yaml.md",
            "title": "用yaml配置文件传参数给pyspark,然后再dataproc运行",
            "description": "首先我们要学Yaml语法:",
            "date": "2020-04-18T00:00:00.000Z",
            "formattedDate": "April 18, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 2.73,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataproc6",
              "title": "用yaml配置文件传参数给pyspark,然后再dataproc运行",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "把avro文件放到bigquery",
              "permalink": "/blog/2020/4/21/把avro文件放到bigquery"
            },
            "nextItem": {
              "title": "yaml语法学习",
              "permalink": "/blog/2020/4/18/yaml语法学习"
            }
          },
          "content": "首先我们要学Yaml语法:\n具体yaml语法可以参考[yaml语法详情](yaml1)\n\n整体思路\n## 准备yaml文件和pyspark\n1. 导入工具包\n```\n#!/usr/bin/env python\nfrom pyspark.sql import SparkSession\nimport sys,yaml,datetime\nimport os\nimport pathlib\nimport google.cloud.storage as gcs\n```\n2. 我们写了一个Yaml文件作为config文件\n3. 我们在我们的pyspark文件读取yaml文件,这里要注意的是,因为本地和GCS会有不同,本地是可以直接读取的,但是如果yaml文件在GCS,yaml文件就是object,是不可改写的,所以我们不能直接open(yaml文件,\"r\")\n4. 我们需要在pyspark文件上创建gcs客户端,然后创建设置一个本地文件路径,然后通过客户端读取yaml文件内容并且下载到本地,然后再通过本地使用with open方法读取yaml文件内容\n```\nclient = gcs.Client()\n\n#set target file to write to\ntarget = pathlib.Path(\"local_file.yaml\")\n\nconfig_file = sys.argv[1] +\"config.yaml\"\n\n#set file to download\nFULL_FILE_PATH = config_file\n\n#open filestream with write permissions\nwith target.open(mode=\"wb\") as downloaded_file:\n\n        #download and write file locally\n        client.download_blob_to_file(FULL_FILE_PATH, downloaded_file)\n\nconfig_file=\"local_file.yaml\"\n```\n\n读取后,我们就可以操作一下代码\n```\nfor job in config[\"jobs\"]:\n    print(\"Creating source views...\")\n    for source in job[\"sources\"]:\n        print(source)\n        if source.get(\"table\") is not None:\n            print(\"Creating view %s from table %s...\" % (source[\"view\"], source[\"table\"]))\n            df = spark.table(source[\"table\"])\n            df.show(5)\n            print('table now')\n        else:\n            print(\"Creating view %s from object %s...\" % (source[\"view\"], source[\"object\"]))\n            df = spark.read.format(source['object'][source['object'].rfind('.')+1:]).option(\"header\",\"true\").load(source['object'])\n            df.show(5)\n        if source.get(\"columns\") is not None:\n            # columns listed, select given columns\n            df = df.select(source[\"columns\"])\n            df.show(5)\n        if source.get('Fillna') is not None:\n            print(source['Fillna'][\"fields\"])\n            print('hah',type(source['Fillna']))\n            df = df.fillna({source['Fillna'][\"fields\"]:source['Fillna'][\"num\"]})\n            df.show(5)\n        if source.get(\"filters\") is not None:\n            df.filter(source[\"filters\"])\n        if source.get(\"union\") is not None:\n            df_union = spark.sql(\"select * from %s\"%(source['union']))\n            df.union(df_union)\n            df.show(1)\n        if source.get(\"join\") is not None:\n            cur = df.select(source['Key'])\n            pre = spark.sql(\"select * from %s\"%(source['right']))\n            df = cur.join(pre,[source['Key']],source['how'])\n            df.show(5)\n        df.createOrReplaceTempView(source[\"view\"])\n    print(\"Performing SQL Transformations...\")\n    if job.get(\"transforms\") is not None:\n        for transform in job[\"transforms\"]:\n            spark.sql(transform[\"sql\"])\n            print(df.count())\n    if job.get(\"targets\") is not None:\n        print(\"Writing out final object to %s...\" % (job[\"targets\"][\"target_location\"]))\n        start = datetime.datetime.now()\n        final_df = spark.table(job[\"targets\"][\"final_object\"])\n        final_df.write.mode(job[\"targets\"][\"mode\"]).format(job[\"targets\"][\"format\"]).save(job[\"targets\"][\"target_location\"])\n        finish = datetime.datetime.now()\n        print(\"Finished writing out target object...\")\n```\n## 这端代码的逻辑就是循环config里面的jobs,jobs里面包括读取文件或者table,fillna,union,transoform等etl作业\n\n## 准备启动dataproc代码\n```\nCLUSTER_NAME=newnew\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n    --region=global \\\n    --zone=us-central1-b \\\n    --worker-machine-type n1-standard-1 \\\n    --num-workers 2 \\\n    --image-version 1.4-debian \\\n    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n    --metadata 'PIP_PACKAGES=google-cloud-storage PyYAML pathlib avro-python3 dask[dataframe] gcsfs fastavro' \\\n    --enable-component-gateway \\\n    --worker-boot-disk-size=40 \\\n    --optional-components=ANACONDA \\\n    --enable-component-gateway\nBUCKET_NAME=zz_michael\ngcloud config set dataproc/region global\ngcloud dataproc jobs submit pyspark dyyaml.py --cluster newnew \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--jars=https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/spark-avro_2.11-2.4.4.jar \\\n-- gs://${BUCKET_NAME}/yaml/ \n```\n\n## job完成后需要删除dataproc clusters\n```\nCLUSTER_NAME=newnew\ngcloud dataproc clusters delete $CLUSTER_NAME\n```"
        },
        {
          "id": "/2020/4/18/yaml语法学习",
          "metadata": {
            "permalink": "/blog/2020/4/18/yaml语法学习",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-18-yaml语法学习.md",
            "source": "@site/blog/2020-4-18-yaml语法学习.md",
            "title": "yaml语法学习",
            "description": "首先我们要学Yaml语法:",
            "date": "2020-04-18T00:00:00.000Z",
            "formattedDate": "April 18, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 3.445,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "yaml1",
              "title": "yaml语法学习",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "用yaml配置文件传参数给pyspark,然后再dataproc运行",
              "permalink": "/blog/2020/4/18/dataproc+spark+yaml"
            },
            "nextItem": {
              "title": "dataproc--dataproc+GCS+Bigquery+Pyspark",
              "permalink": "/blog/2020/4/17/dataproc+GCS+Bigquery+pyspark"
            }
          },
          "content": "首先我们要学Yaml语法:\n具体yaml语法可以参考[yaml语法详情]([yaml1](https://learn-the-web.algonquindesign.ca/topics/yaml/))\n\n# yaml语法的核心我觉得也是字典语言\n\n## 缩减空格:\nYAML对此非常严格:仅缩进2个空格\n\n## 结构化数据-言外之意就是字典对象\n两个主要结构:\n1. ```对象 类似javascript的对象 或者html <dl>```\n2. ```数组 类似javascript的数组 类似html<ul>```\n\n## 读取yaml文件\n```\nconfig_file  = './test.yaml'\nwith open(config_file, 'r') as stream:\n    config = yaml.load(stream.read(),Loader=yaml.FullLoader)\n```\n\n### 对象\nYAML中的对象以单词/术语开头,后跟冒号和空格\n\n对象包含对象\n```\ndimensions:\n  width: 3 metres\n  height: 8 metres\n  length: 12 metres\n  weight: 4 tonnes\n```\n我们可以dimension['width']获取wdith数值\n```\nfor key,values in config['dimensions']:\n    print(key,values)\n```\n\n### 数组\n```\nlikes_to_eat:\n  - Other dinosaurs\n  - Meat\n  - More meat\n  - Not plants\n```\n```\nfor key,values in enumerate(config['likes_to_eat']):\n    print(key,values)\n```\n\n### 数据包含对象\n```\nPerson:\n    - name: T. rex\n      period: Late Cretaceous Period\n    - name: Stegosaurus\n      period: Late Jurassic Period\n    - name: Velociraptor\n      period: Cretaceous Period\n```\n\"-\"可以看出列表[],然后如果里面有对象,就是列表嵌套对象的意思\n\n## 以上就是yaml最基本的用法\n\n### 文字区块\n如果您的YAML中有超大文本块，则可以使用竖线（|）或大于符号（>）的方式将文本分开，并允许更多格式。\n大于号使您可以在多行上编写文本，因为当您解析YAML时，这些行将折叠为一行。\n```\ndesc: >\n  Tyrannosaurus (/tɨˌrænəˈsɔrəs/ or /taɪˌrænəˈsɔrəs/ (\"tyrant lizard\", from the Ancient Greek tyrannos (τύραννος), \"tyrant\", and sauros (σαῦρος), \"lizard\"[1])) is a genus of coelurosaurian theropod dinosaur.\n  The species Tyrannosaurus rex (rex meaning \"king\" in Latin), commonly abbreviated to T. rex, is one of the most well-represented of the large theropods.\n```\n\n## 验证\n我们可以去yaml语法验证网站验证(http://www.yamllint.com/)\n\n我的一个实例:\n```\njobs: \n  - \n    name: \"read_file and conbine as one dataframe5\"\n    sources: \n      - \n        Fillna: \n          fields: NUM_OF_MTHS_PD_30\n          num: 0\n        JobDescription: \"read Curr_RD file\"\n        object: \"gs://zz_michael/yaml/Curr_RD.avro\"\n        view: Curr_RD\n      - \n        Fillna: \n          fields: NUM_OF_MTHS_PD_30\n          num: 0\n        JobDescription: \"read Prev_RD file\"\n        object: \"gs://zz_michael/yaml/Prev_RD.avro\"\n        view: Prev_RD\n      - \n        JobDescription: \"left join\"\n        Key: ARNG_ID\n        how: left_outer\n        join: left_outer\n        left: Curr_RD\n        right: Prev_RD\n        table: Curr_RD\n        view: df5\n  - \n    name: \"filter Df5 df\"\n    sources: \n      - \n        JobDescription: \"filter df5\"\n        filters: \"NUM_OF_MTHS_PD_30 >=1\"\n        table: df5\n        view: df6\n    transforms: \n      - \n        sql: \"CREATE OR REPLACE TEMPORARY VIEW df7 as select * from df5 where NUM_OF_MTHS_PD_30 is null or NUM_OF_MTHS_PD_30 <1\"\n  - \n    name: fillna\n    sources: \n      - \n        Fillna: \n          fields: NUM_OF_MTHS_PD_30\n          num: 0\n        JobDescription: fillna\n        table: df7\n        view: df8\n  - \n    name: union\n    sources: \n      - \n        JobDescription: union\n        table: df6\n        union: df8\n        view: df9\n  - \n    name: write to avro file\n    sources: \n      - \n        JobDescription: \"left join\"\n        Key: ARNG_ID\n        how: left_outer\n        join: left_outer\n        left: Curr_RD\n        right: df9\n        table: Curr_RD\n        view: df10\n    targets: \n      final_object: df10\n      format: csv\n      mode: overwrite\n      target_location: \"gs://zz_michael/yaml/output\"\n    transforms: \n      - \n        sql: \"CREATE OR REPLACE TEMPORARY VIEW df10 as select * from df10\"\nname: haha\n```\n## 解析上面YAML文件\n1. 我们可以看到Jobs是对象 jobs(object)\n2. 从'-'可以看出,Jobs对象嵌套列表 类似 [i for i in config[\"jobs\"]],所以我们这里可以遍历jobs\n3. 列表里面嵌套对象: [{\"name\":\"\",\"sources\":\"\"},{\"name\":\"\",\"sources\":\"\",\"transforms\":\"\"},{\"name\":\"\",\"sources\":\"\"},{\"name\":\"\",\"sources\":\"\"},{\"name\":\"\",\"sources\":\"\",\"transforms\":\"\",'targets':\"\"}]\n4. 我们看到source也是有嵌套的,也是嵌套对象[{},{}]\n5. Fillna对象里面也是有嵌套对象"
        },
        {
          "id": "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
          "metadata": {
            "permalink": "/blog/2020/4/17/dataproc+GCS+Bigquery+pyspark",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-17-dataproc+GCS+Bigquery+pyspark.md",
            "source": "@site/blog/2020-4-17-dataproc+GCS+Bigquery+pyspark.md",
            "title": "dataproc--dataproc+GCS+Bigquery+Pyspark",
            "description": "流程很简单",
            "date": "2020-04-17T00:00:00.000Z",
            "formattedDate": "April 17, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 1.585,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataproc4",
              "title": "dataproc--dataproc+GCS+Bigquery+Pyspark",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "yaml语法学习",
              "permalink": "/blog/2020/4/18/yaml语法学习"
            },
            "nextItem": {
              "title": "dataproc参数化跑spark和读写avro文件",
              "permalink": "/blog/2020/4/17/dataproc参数化跑spark和读写avro"
            }
          },
          "content": "## 流程很简单\n首先我们从GCS那里读取avro数据,然后我们读取avro数据变成dask.Dataframe,然后对dask.Dataframe操作,再转成pandas Dataframe,然后变成Spark Dataframe,最后通过Spark 与 bigquery 的connector对接起来,写入big query\n\n## 安装刚才的思路写python.py\n```\nimport dask.bag as db # 导入工具包\ndef run():\n    b = db.read_avro('gs://zz_mm_bucket/account_id_schema_new.avro') #从GCS读取avro文件\n    df = b.to_dataframe() # 转成Dataframe\n    df_values = df.compute().values.tolist() #转成pandas的dataframe\n    df_columns = list(df.columns)\n\n    import pandas as pd\n    from pyspark.sql import SparkSession #spark初始化\n    spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n    bucket = \"haha_mm_bucket\" #设置bucket\n    spark.conf.set('temporaryGcsBucket', bucket) #给spark初始化设置bucket零时存放数据的gcs\n\n    spark_df = spark.createDataFrame(df_values, df_columns) 把dataframe转成spark的dataframe\n    spark_df.show(10) #对spark的dataframe进行操作\n    spark_df.write.format('bigquery').option('table','query-11:newdata.newdata').save() # 写入bigquery\n\nif __name__ == '__main__':\n    run()\n```\n###  去到终端输入命令,提交spark job\n```\ngcloud dataproc jobs submit pyspark wordcount.py \\\n    --cluster cluster-name \\\n    --region cluster-region (example: \"us-central1\") \\\n    --jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n```\n主要格式: gcloud dataproc jobs submit pyspark python.py(python文件) \\\n        --cluster cluster-name \\\n        --region cluster-region(比如:us-central1,一定要对应dataproc集群的region)\n        --jars 与biguqery连接的包\n注意这里的jars:\nIf you are using Dataproc image 1.5, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\nIf you are using Dataproc image 1.4 or below, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n\n\n```\ngcloud config set dataproc/region us-central1\nBUCKET_NAME=haha_mm_bucket\ninput=new.avro\ngcloud dataproc jobs submit pyspark wordcount3.py \\\n--cluster cluster-662b \\\n-- gs://${BUCKET_NAME}/${input} \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--packages com.databricks:spark-avro_2.11:4.0.0\n```"
        },
        {
          "id": "/2020/4/17/dataproc参数化跑spark和读写avro",
          "metadata": {
            "permalink": "/blog/2020/4/17/dataproc参数化跑spark和读写avro",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-17-dataproc参数化跑spark和读写avro.md",
            "source": "@site/blog/2020-4-17-dataproc参数化跑spark和读写avro.md",
            "title": "dataproc参数化跑spark和读写avro文件",
            "description": "1. spark 初始化,因为要读取成dataframe或者sql形式,导入SparkSession",
            "date": "2020-04-17T00:00:00.000Z",
            "formattedDate": "April 17, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 2.74,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataproc5",
              "title": "dataproc参数化跑spark和读写avro文件",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "dataproc--dataproc+GCS+Bigquery+Pyspark",
              "permalink": "/blog/2020/4/17/dataproc+GCS+Bigquery+pyspark"
            },
            "nextItem": {
              "title": "dataproc自动伸缩和运行spark job",
              "permalink": "/blog/2020/4/17/dataproc自动伸缩和运行sparkjob"
            }
          },
          "content": "### 1. spark 初始化,因为要读取成dataframe或者sql形式,导入SparkSession\n```\nfrom pyspark.sql import SparkSession\nimport sys\n```\n### 2. 创建spark对象\n```\nspark = SparkSession \\\n  .builder \\\n  .master('yarn') \\\n  .appName('gcs-sparkdataframe-sql-avro') \\\n  .getOrCreate()\n```\n### 参数判断和参数设置\n```\nif len(sys.argv) != 4:\n  raise Exception(\"Exactly 3 arguments are required: <inputUri> <table1><table2>\")\n\ninputUri=sys.argv[1]\ntable1=sys.argv[2]\ntable2=sys.argv[3]\n```\n### 4 读取avro文件\n```\ndf = spark.read.format('avro').load(inputUri)\n```\n### 5 注册视图,实行查询语句\n```\ndf1 = spark.sql(\"select ACNO,%s from bigtable\" % (\",\".join(df.columns[1:round(len(df.columns) / 2)])))\ndf2 = spark.sql(\"select ACNO,%s from bigtable\" % (\",\".join(df.columns[round(len(df.columns) / 2):])))\ndf1.show(10)\ndf2.show(10)\n```\n\n### 6 处理好的dataframe对象写成avro文件 (注意,用sql处理过后的还是dataframe对象)\n```\ndf1.write.format('avro').save(table1,'avro')\n\ndf2.write.format('avro').save(table2,'avro')\n```\n\n### 7 去到终端输入命令,创建dataproc集群,然后提交spark job\n```\nCLUSTER_NAME=newnew\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n    --region=global \\\n    --zone=us-central1-b \\\n    --worker-machine-type n1-standard-1 \\\n    --num-workers 2 \\\n    --image-version 1.4-debian \\\n    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n    --metadata 'PIP_PACKAGES=google-cloud-storage avro-python3 dask[dataframe] gcsfs fastavro' \\\n    --enable-component-gateway \\\n    --worker-boot-disk-size=40 \\\n    --optional-components=ANACONDA \\\n    --enable-component-gateway\nBUCKET_NAME=zz_mm_bucket\ngcloud config set dataproc/region global\ngcloud dataproc jobs submit pyspark avrosqlargs.py --cluster newnew \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--jars=https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/spark-avro_2.11-2.4.4.jar \\\n-- gs://${BUCKET_NAME}/input/gs://zz_mm_bucket/input/ gs://${BUCKET_NAME}/output/table1 gs://${BUCKET_NAME}/output/table2\n```\n## 这里注意的是gcloud dataproc jobs sumbit的参数格式是 pyspark.py文件, files\n所以例子中我们的参数总共有4个\n1 avrosqlargs.py\n\n2 gs://${BUCKET_NAME}/input/gs://zz_mm_bucket/input/\n\n3 gs://${BUCKET_NAME}/output/table1 \n\n4 gs://${BUCKET_NAME}/output/table2\n\njars和cluster都不算为参数\n\n还有就是files的是文件夹形式而不能是文件形式,所以读入文件夹后,可以根据需要读取你需要的文件,比如sys.argv+'文件名'\n\n所以整体可以改成:\n```\n#!/usr/bin/python\n\"\"\"BigQuery I/O PySpark example.\"\"\"\nfrom pyspark.sql import SparkSession\nimport sys\n\n\n\nspark = SparkSession \\\n  .builder \\\n  .master('yarn') \\\n  .appName('gcs-sparkdataframe-sql-avro') \\\n  .getOrCreate()\n\n# get spark datafrom from avro file in GCS\n\nif len(sys.argv) != 4:\n  raise Exception(\"Exactly 3 arguments are required: <inputUri> <table1><table2>\")\n\ninputUri=sys.argv[1]\ntable1=sys.argv[2]\ntable2=sys.argv[3]\n\nfile = inputUri+'account_id_schema_new.avro'\ndf = spark.read.format('avro').load(file)\n\n\n#create temp table\ndf.createOrReplaceTempView('bigtable')\n\n# split temp table into 2 spark dataframes\ndf1 = spark.sql(\"select ACNO,%s from bigtable\" % (\",\".join(df.columns[1:round(len(df.columns) / 2)])))\ndf2 = spark.sql(\"select ACNO,%s from bigtable\" % (\",\".join(df.columns[round(len(df.columns) / 2):])))\ndf1.show(10)\ndf2.show(10)\n\n# Saving the dataframes into avro files and dump avro files into GCS\n\ndf1.write.mode(\"overwrite\").format('avro').save(table1,'avro')\n\ndf2.write.mode(\"overwrite\").format('avro').save(table2,'avro')\n```\n\n## 关于生成文件,因为spark是基于hadoop的,所以文件也会分布式存储,所以我们可以看到\n```\ndf = spark.read.format('avro').load(sys.argv[3])\n```\n## 一般是分区是会根据你的电脑的cpu核数自动分配,我的电脑是core i5,也就是四核的,所以默认是4\n我们可以重分区:\n```\ndf.repartition(10) # 就是分10区\ndf.rdd.getNumPartitions() #查看分区数\ndf.coalesce(1)\n```"
        },
        {
          "id": "/2020/4/17/dataproc自动伸缩和运行sparkjob",
          "metadata": {
            "permalink": "/blog/2020/4/17/dataproc自动伸缩和运行sparkjob",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-17-dataproc自动伸缩和运行sparkjob.md",
            "source": "@site/blog/2020-4-17-dataproc自动伸缩和运行sparkjob.md",
            "title": "dataproc自动伸缩和运行spark job",
            "description": "我们运用数据分析的时候,通常都是脏数据,我们需要清洗才能被使用.",
            "date": "2020-04-17T00:00:00.000Z",
            "formattedDate": "April 17, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 2.51,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataproc2",
              "title": "dataproc自动伸缩和运行spark job",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "dataproc参数化跑spark和读写avro文件",
              "permalink": "/blog/2020/4/17/dataproc参数化跑spark和读写avro"
            },
            "nextItem": {
              "title": "dataproc--跑pyspark(从big query获取数据)",
              "permalink": "/blog/2020/4/17/dataproc跑pyspark"
            }
          },
          "content": "我们运用数据分析的时候,通常都是脏数据,我们需要清洗才能被使用.\n我们可以加载Big query的数据放到Dataproc中,通过spark集群来etl有用的数据,然后把一些processed data比如zipped csv file放到google gloud storage上\n![png](../img/dataproc/dataproc2/1.png)\n\n# 准备阶段\n## 去到控制台,确认cloud dataproc API已启用\n<!--truncate-->\n![png](../img/dataproc/dataproc1/1.png)\n![png](../img/dataproc/dataproc1/2.png)\n![png](../img/dataproc/dataproc1/3.png)\n\n# 我们会使用命令去dataproc创建集群\n\n## 首先我们创建一个yaml文件,对集群做自动伸缩的定义\n[关于spark自动伸缩原理](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#autoscaling_and_spark_structured_streaming)\n```\nworkerConfig:\n  minInstances: 2\n  maxInstances: 100\n  weight: 1\nsecondaryWorkerConfig:\n  minInstances: 0\n  maxInstances: 100\n  weight: 1\nbasicAlgorithm:\n  cooldownPeriod: 4m\n  yarnConfig:\n    scaleUpFactor: 1\n    scaleDownFactor: 1.0\n    scaleUpMinWorkerFraction: 0.0\n    scaleDownMinWorkerFraction: 0.0\n    gracefulDecommissionTimeout: 1h\n```\n\n然后我们可以先创建自动伸缩政策\n```\ngcloud beta dataproc autoscaling-policies import policy-name --source policy-file.yaml\n```\n[具体的command命令](https://cloud.google.com/sdk/gcloud/reference/beta/dataproc/autoscaling-policies)\n\n## 创建集群\n```\nCLUSTER_NAME=newnew\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n    --region=global \\\n    --zone=us-central1-b \\\n    --worker-machine-type n1-standard-1 \\\n    --num-workers 2 \\\n    --image-version 1.4-debian \\\n    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n    --metadata 'PIP_PACKAGES=google-cloud-storage avro-python3 dask[dataframe] gcsfs fastavro' \\\n    --enable-component-gateway \\\n    --worker-boot-disk-size=40 \\\n    --optional-components=JUPYTER,ANACONDA \\\n    --enable-component-gateway \\\n    --autoscaling-policy=global\n```\n\n## 创建成功后,Dataprocmenu上查看,同时我们需要创建一个GCS,这样我们处理好的数据就可以存在在GCS上了\n```\nBUCKET_NAME=<bucket_name>\ngsutil mb gs://${BUCKET_NAME}\n```\n\n## 我们可以去bigquery的public data找数据,查看表的数据结构\n![png](../img/dataproc/dataproc2/2.png)\n\n## 运行 pyspark job\n```\ncd\ngit clone https://github.com/GoogleCloudPlatform/cloud-dataproc\n```\n```\ncd ~/cloud-dataproc/codelabs/spark-bigquery\ngcloud dataproc jobs submit pyspark --cluster ${CLUSTER_NAME} \\\n    --jars gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \\\n    --driver-log-levels root=FATAL \\\n    counts_by_subreddit.py\n```\n我们需要提供集群名字,提供jars的参数,这个jar包能够允许我们通过saprk-bigquery-connector连接到我们的job,\ndriver-log-levels root=FATAL能够抑制日志输出除了错误信息,因为spark logs.\n![png](../img/dataproc/dataproc2/3.png)\n![png](../img/dataproc/dataproc2/4.png)\n![png](../img/dataproc/dataproc2/5.png)\n\n## 我们查看Dataproc和spark\n![png](../img/dataproc/dataproc2/6.png)\n![png](../img/dataproc/dataproc2/7.png)\n![png](../img/dataproc/dataproc2/8.png)\n![png](../img/dataproc/dataproc2/9.png)\n\n## 我们跑一个任务,加载数据到memory,提取必须要的信息,然后输出到GCS里面.我们这里是提取title,body和timestamp created,然后我们把这些data转成csv文件,存入gcs里面\n```\ncd ~/cloud-dataproc/codelabs/spark-bigquery\nbash backfill.sh ${CLUSTER_NAME} ${BUCKET_NAME}\n```\n\n查看\n```\ngsutil ls gs://${BUCKET_NAME}/reddit_posts/*/*/food.csv.gz\n```\n\n(类似的实验可以参考)(https://codelabs.developers.google.com/codelabs/pyspark-bigquery/index.html?index=..%2F..index#1)"
        },
        {
          "id": "/2020/4/17/dataproc跑pyspark",
          "metadata": {
            "permalink": "/blog/2020/4/17/dataproc跑pyspark",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-17-dataproc跑pyspark.md",
            "source": "@site/blog/2020-4-17-dataproc跑pyspark.md",
            "title": "dataproc--跑pyspark(从big query获取数据)",
            "description": "流程很简单",
            "date": "2020-04-17T00:00:00.000Z",
            "formattedDate": "April 17, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 1.83,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataproc3",
              "title": "dataproc--跑pyspark(从big query获取数据)",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "dataproc自动伸缩和运行spark job",
              "permalink": "/blog/2020/4/17/dataproc自动伸缩和运行sparkjob"
            },
            "nextItem": {
              "title": "5分钟在谷歌云上使用Dataproc运行Apache Spark集群",
              "permalink": "/blog/2020/4/17/dataproc运行Apache Spark集群"
            }
          },
          "content": "## 流程很简单\n我们要用spark读取从bigquery读取table,然后我们对这个table做一个简单的处理,再分成两dataframe对象,然后把两个对象写入bigquery\n\n### 1. spark 初始化,因为要读取成dataframe或者sql形式,导入SparkSession\n```\n#!/usr/bin/python\nfrom pyspark.sql import SparkSession\n```\n### 2. 创建spark对象\n```\nspark = SparkSession.builder.master('yarn').appName('your app name').getOrCreate()\n```\n### 3 我们通过connector连接一个google storage bucket 给Bigquery输出数据临时用\n```\nbucket = \"haha_mm_bucket\"\nspark.conf.set('temporaryGcsBucket',bucket)\n```\n### 4 配置好tmp bucket,我们可以开始读取数据,并且把数据框注册为视图\n```\ndf = spark.read.format('bigquery').option('table','datasetid:tableid').load()\ndf.createTempView(\"temp table name(比如words)\")\n也可以是df.createOrReplaceTempView('words') 这样就可以覆盖原来同样名字的临时视图\n```\n### 5 开始使用sql语句\n```\nlefttable = spark.sql(\"SELECT ACNO, FIELD_1, FIELD_2 FROM words\")\nrighttable = spark.sql(\"SELECT ACNO, FIELD_3, FIELD_4 FROM words\")\nlefttable.show()\nlefttable.printSchema()\nrighttable.show()\nrighttable.printSchema()\n```\n\n### 6 处理好的dataframe对象写入bigquery (注意,用sql处理过后的还是dataframe对象)\n```\nlefttable.write.format('bigquery').option('table','query-11:newdata.lefttable').save()\nrighttable.write.format('bigquery').option('table','query-11:newdata.righttable').save()\n```\n\n### 7 去到终端输入命令,提交spark job\n```\ngcloud dataproc jobs submit pyspark wordcount.py \\\n    --cluster cluster-name \\\n    --region cluster-region (example: \"us-central1\") \\\n    --jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n```\n主要格式: gcloud dataproc jobs submit pyspark python.py(python文件) \\\n        --cluster cluster-name \\\n        --region cluster-region(比如:us-central1,一定要对应dataproc集群的region)\n        --jars 与biguqery连接的包\n注意这里的jars:\nIf you are using Dataproc image 1.5, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\nIf you are using Dataproc image 1.4 or below, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n\n\n```\ngcloud config set dataproc/region us-central1\nBUCKET_NAME=haha_mm_bucket\ninput=new.avro\ngcloud dataproc jobs submit pyspark wordcount3.py \\\n--cluster cluster-662b \\\n-- gs://${BUCKET_NAME}/${input} \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--packages com.databricks:spark-avro_2.11:4.0.0\n```"
        },
        {
          "id": "/2020/4/17/dataproc运行Apache Spark集群",
          "metadata": {
            "permalink": "/blog/2020/4/17/dataproc运行Apache Spark集群",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-17-dataproc运行Apache Spark集群.md",
            "source": "@site/blog/2020-4-17-dataproc运行Apache Spark集群.md",
            "title": "5分钟在谷歌云上使用Dataproc运行Apache Spark集群",
            "description": "png",
            "date": "2020-04-17T00:00:00.000Z",
            "formattedDate": "April 17, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 5.165,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataproc1",
              "title": "5分钟在谷歌云上使用Dataproc运行Apache Spark集群",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "dataproc--跑pyspark(从big query获取数据)",
              "permalink": "/blog/2020/4/17/dataproc跑pyspark"
            },
            "nextItem": {
              "title": "google script + excel+ google drive",
              "permalink": "/blog/2020/4/16/bq_脚本制作presentation"
            }
          },
          "content": "<!--truncate-->\n![png](../img/dataproc/dataproc1/1.png)\n![png](../img/dataproc/dataproc1/2.png)\n![png](../img/dataproc/dataproc1/3.png)\n\n# 去dataproc->cluster->create cluster\n![png](../img/dataproc/dataproc1/10.png)\n\n# create cluster后在以下字段输入对应值\n![png](../img/dataproc/dataproc1/4.png)\n\n具体如下:\n\n![png](../img/dataproc/dataproc1/5.png)\n\n# 以上控制台的步骤,可以直接通过gcloud terminal代码实现\n```\ngcloud set config project project ID # 设置指定项目id\ngcloud config set dataproc/region global #在dataproc设置region,这一步很重要\ngcloud dataproc clusters create example-cluster\n```\n![png](../img/dataproc/dataproc1/1.png)\n## 这样gcp就会帮你搭建cluster了\n\n# 创建集群成功后,运行job\n## 去job,选择 sumbit job\n\n![png](../img/dataproc/dataproc1/6.png)\n\n### 在对应字段输入对应值\n![png](../img/dataproc/dataproc1/7.png)\n### 具体如下,输入完成后点击sumbit\n![png](../img/dataproc/dataproc1/8.png)\n### 我们可以点击job id 查看结果和日志\n![png](../img/dataproc/dataproc1/9.png)\n\n# 以上步骤,我们同样可以用代码在GCPterminal上操作\n```\ngcloud dataproc jobs submit spark --cluster example-cluster \\\n  --class org.apache.spark.examples.SparkPi \\\n  --jars file:///usr/lib/spark/examples/jars/spark-examples.jar -- 1000\n```\n我们把job打包成jar,然后用submit spark 来运行\n命令 gcloud dataproc jobs submit spark --cluster 集群名称 \\ --class 类名 --jars jar包路径 --task数量\n\n![png](../img/dataproc/dataproc1/2.png)\n![png](../img/dataproc/dataproc1/3.png)\n\n## 我们可以可以更新worker数量\ngcloud dataproc clusters update 集群名字 -num-workers=数量\n```\ngcloud dataproc clusters update example-cluster --num-workers= 4\n```\n![png](../img/dataproc/dataproc1/4.png)\n# 我们可以参考集群设置\n```\ngcloud dataproc clusters describe example-cluster\n```\n\n![png](../img/dataproc/dataproc1/5.png)\n\n## 关于spark jar包和class问题\n[参考链接1](https://www.jianshu.com/p/2c7bcee7001a)\n\n[参看链接2](https://blog.csdn.net/u014234504/article/details/82812343)\n\n[class的写法](https://wenku.baidu.com/view/a0dd882fa8956bec0975e397.html)\n\n\n# 用dataproc创建集群和运行spark job 完成!!\n关于[spark job,stage,task](https://zhuanlan.zhihu.com/p/50752866)的理解和参考\n\n\n# 用gcloud来启cluster命令\n\n[一个NLP实验](https://codelabs.developers.google.com/codelabs/spark-nlp/index.html?index=..%2F..index&_ga=2.18367290.242813027.1588130089-996489118.1587002962&_gac=1.182983892.1588923163.Cj0KCQjwhtT1BRCiARIsAGlY51KEFc2-GGnxsXVhFZsKWExLJckKepaugZrKbbr2cvW0KLPRtubd7vAaAkA2EALw_wcB#0)\n\n\n# 在dataproc创建集群\n```\nCLUSTER_NAME=my-cluster\nZONE=us-east1-b\nBUCKET_NAME=bm_reddit\nREGION=us-east1\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n     --zone=${ZONE} \\\n     --metadata 'PIP_PACKAGES=google-cloud-storage' \\\n     --worker-machine-type n1-standard-1 \\\n     --num-workers 2 \\\n     --image-version 1.4-debian9 \\\n     --initialization-actions gs://dataproc-initialization-actions/spark-nlp/spark-nlp.sh,gs://dataproc-initialization-actions/python/pip-install.sh \\\n     --region=${REGION} \\\n     --optional-components=JUPYTER,ANACONDA \\\n     --enable-component-gateway \\\n     --worker-boot-disk-size=30\n```\n## 参数说明:\nmetadata指源数据,比如想要安装CONDA_PACKAGE,我们\n```\n--metadata 'CONDA_PACKAGES=scipy=1.0.0 tensorflow' \\ \n```\n如果想安装pip_package,我们可以\n```\n--metadata 'PIP_PACKAGES=pandas==0.23.0 scipy==1.1.0' \\ \n```\n详细可以[参考链接](https://cloud.google.com/dataproc/docs/tutorials/python-configuration)\n\n## Install PyPI packages\n```\n--initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh\n```\n## Install Conda packages\n```\n--initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/conda-install.sh\n```\n\n--initialization-actions是用在安装metadata的包\n```\n--initialization-actions \\ \n    gs://goog-dataproc-initialization-actions-${REGION}/conda/bootstrap-conda.sh,gs://goog-dataproc-initialization-actions-${REGION}/conda/install-conda-env.sh\n```\n下载代码\n```\ncd\ngit clone https://github.com/GoogleCloudPlatform/cloud-dataproc\ncd cloud-dataproc/codelabs/spark-nlp\n```\n\n运行\n```\ngcloud dataproc jobs submit pyspark --cluster ${CLUSTER_NAME}\\\n    --properties=spark.jars.packages=JohnSnowLabs:spark-nlp:2.0.8 \\\n    --driver-log-levels root=FATAL \\\n    topic_model.py \\\n    -- ${BUCKET_NAME}\n```\n\nCLUSTER_NAME=newnew\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n    --region=global \\\n    --zone=us-east1-b \\\n    --worker-machine-type n1-standard-2 \\\n    --num-workers 2 \\\n    --image-version 1.5-debian \\\n    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n    --metadata 'PIP_PACKAGES=google-cloud-storage avro-python3 dask[dataframe] gcsfs fastavro' \\\n    --enable-component-gateway \\\n    --worker-boot-disk-size=40 \\\n    --optional-components=JUPYTER,ANACONDA \\\n    --enable-component-gateway \\\n    --autoscaling-policy=global\n\n\nworkerConfig:\n  minInstances: 2\n  maxInstances: 100\n  weight: 1\nsecondaryWorkerConfig:\n  minInstances: 0\n  maxInstances: 100\n  weight: 1\nbasicAlgorithm:\n  cooldownPeriod: 4m\n  yarnConfig:\n    scaleUpFactor: 0.05\n    scaleDownFactor: 1.0\n    scaleUpMinWorkerFraction: 0.0\n    scaleDownMinWorkerFraction: 0.0\n    gracefulDecommissionTimeout: 1h\n\n## 常见错误\n\n案例1:cpus的限额问题\nMultiple validation errors: - Insufficient 'CPUS' quota. Requested 12.0, available 7.0. - Insufficient 'CPUS_ALL_REGIONS' quota. Requested 12.0, available 11.0. - This request exceeds CPU quota. Some things to try: request fewer workers (a minimum of 2 is required), use smaller master and/or worker machine types (such as n1-standard-2).\n\n\n这里的insufficent 'CPUS' 只的是master和worker所有的node的cpus数量总和\nInsufficient 'CPUS_ALL_REGIONS' quota是在全球的cpus限额,如果region选择global, 那么这里限额就是11(available 11), 如果region选择时某个地区,比如us-central1,那么我这里的quota就是7.为什么是7呢,因为谷歌的试用账号最多只能有8CPUs,我这个project里面已经在一个地区区域创建了一个vm,这个vm的machining type是(n1-standard-1 (1 vCPU, 3.75 GB memory)),所以占用了一个cpu,所以地方region的cpus可用额度就是8-1=7\n同理,全球的话是12cpu,我已经用了1个了,所以全球剩下的就是12-1=11.\n\n案例2: yarn core数 和 yarn memory\nyarn core = worker的node个数 * vcpu\nyarn memory =  worker的node个数 * memoery * 0.8\n\n![png](../img/dataproc/dataproc1/11.png)\n\ngoogle对yarn memoery的定义: \nThe number of worker nodes times the amount of memory on each node times the fraction given to YARN (0.8)\n\ngoogle对yarn coreds的定义:\nthe number of worker nodes times the number of vCPUs per node\n\n## 创建集群后,使用jupyter, 去到cluster,然后去到web interfaces,选择jupyter\n如果运行spark的时候,出现 Py4JJavaError错误\n可以安装\n```\nconda install -c cyclus java-jdk\n```\n\n如果想安装python3\n```\n!conda create -n py36 python=3.6 anaconda\n```\n\n## Dataproc image version 1.3 或者之前的话默认是python2.7\n所以我们最好安装Dataproc image version 1.4+,默认就是python3,具体详细如下\n```\nMiniconda3 is installed on Dataproc 1.4+ clusters. The default Python interpreter is Python 3.6 for Dataproc 1.4 and Python 3.7 for Dataproc 1.5, located on the VM instance at /opt/conda/miniconda3/bin/python3.6 and /opt/conda/miniconda3/bin/python3.7, respectively. Python 2.7 is also available at /usr/bin/python2.7.\n```\n\n## 具体配置cluster配置的请查看\n[参考链接](https://cloud.google.com/dataproc/docs/tutorials/python-configuration)"
        },
        {
          "id": "/2020/4/16/bq_脚本制作presentation",
          "metadata": {
            "permalink": "/blog/2020/4/16/bq_脚本制作presentation",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-16-bq_脚本制作presentation.md",
            "source": "@site/blog/2020-4-16-bq_脚本制作presentation.md",
            "title": "google script + excel+ google drive",
            "description": "我们的目标是创建一个脚本可以读取数据,导出成表格形式而且制作出图标,最后做成一个ppt幻灯演示,看起来是不是很牛,这些动作只需要写个脚本就能完成啦",
            "date": "2020-04-16T00:00:00.000Z",
            "formattedDate": "April 16, 2020",
            "tags": [
              {
                "label": "script",
                "permalink": "/blog/tags/script"
              },
              {
                "label": "excel",
                "permalink": "/blog/tags/excel"
              },
              {
                "label": "google",
                "permalink": "/blog/tags/google"
              }
            ],
            "readingTime": 15.43,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "bigquery_presentation",
              "title": "google script + excel+ google drive",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "script",
                "excel",
                "google"
              ]
            },
            "prevItem": {
              "title": "5分钟在谷歌云上使用Dataproc运行Apache Spark集群",
              "permalink": "/blog/2020/4/17/dataproc运行Apache Spark集群"
            },
            "nextItem": {
              "title": "Bigquery中sql语句常用命令",
              "permalink": "/blog/2020/4/15/bq_sql常用短语"
            }
          },
          "content": "### 我们的目标是创建一个脚本可以读取数据,导出成表格形式而且制作出图标,最后做成一个ppt幻灯演示,看起来是不是很牛,这些动作只需要写个脚本就能完成啦\n\n#### 请看演示\n<!--truncate-->\n#### 我们进入script.google.com,点击getting started,然后再点击Apps script\n![png](../img/bigquery/bigquery_presentation/1.png)\n\n#### 我们进入scripts中,我们可以改名成slides demo\n![png](../img/bigquery/bigquery_presentation/2.png)\n\n#### 然后我们去到控制台查看big query api 是否已经启动\n#### 确认后,我们点击Resources的advanced google services\n![png](../img/bigquery/bigquery_presentation/3.png)\n\n#### 我们开启BigQuery API服务\n![png](../img/bigquery/bigquery_presentation/4.png)\n\n#### 修改代码文件名字,改成bs-sheets-slides.gs\n这里用得是javascript来写\n![png](../img/bigquery/bigquery_presentation/5.png)\n具体代码如下\n```javascript\n/**\n * Copyright 2018 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at apache.org/licenses/LICENSE-2.0.\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Filename for data results\nvar QUERY_NAME = \"Most common words in all of Shakespeare's works\";\n// Replace this value with your Google Cloud API project ID\nvar PROJECT_ID = '<YOUR_PROJECT_ID>';\nif (!PROJECT_ID) throw Error('Project ID is required in setup');\n\n/**\n * Runs a BigQuery query; puts results into Sheet. You must enable\n * the BigQuery advanced service before you can run this code.\n * @see http://developers.google.com/apps-script/advanced/bigquery#run_query\n * @see http://github.com/gsuitedevs/apps-script-samples/blob/master/advanced/bigquery.gs\n *\n * @returns {Spreadsheet} Returns a spreadsheet with BigQuery results\n * @see http://developers.google.com/apps-script/reference/spreadsheet/spreadsheet\n */\nfunction runQuery() {\n  // Replace sample with your own BigQuery query.\n  var request = {\n    query:\n        'SELECT ' +\n            'LOWER(word) AS word, ' +\n            'SUM(word_count) AS count ' +\n        'FROM [bigquery-public-data:samples.shakespeare] ' +\n        'GROUP BY word ' +\n        'ORDER BY count ' +\n        'DESC LIMIT 10'\n  };\n  var queryResults = BigQuery.Jobs.query(request, PROJECT_ID);\n  var jobId = queryResults.jobReference.jobId;\n\n  // Wait for BQ job completion (with exponential backoff).\n  var sleepTimeMs = 500;\n  while (!queryResults.jobComplete) {\n    Utilities.sleep(sleepTimeMs);\n    sleepTimeMs *= 2;\n    queryResults = BigQuery.Jobs.getQueryResults(PROJECT_ID, jobId);\n  }\n\n  // Get all results from BigQuery.\n  var rows = queryResults.rows;\n  while (queryResults.pageToken) {\n    queryResults = BigQuery.Jobs.getQueryResults(PROJECT_ID, jobId, {\n      pageToken: queryResults.pageToken\n    });\n    rows = rows.concat(queryResults.rows);\n  }\n\n  // Return null if no data returned.\n  if (!rows) {\n    return Logger.log('No rows returned.');\n  }\n\n  // Create the new results spreadsheet.\n  var spreadsheet = SpreadsheetApp.create(QUERY_NAME);\n  var sheet = spreadsheet.getActiveSheet();\n\n  // Add headers to Sheet.\n  var headers = queryResults.schema.fields.map(function(field) {\n    return field.name.toUpperCase();\n  });\n  sheet.appendRow(headers);\n\n  // Append the results.\n  var data = new Array(rows.length);\n  for (var i = 0; i < rows.length; i++) {\n    var cols = rows[i].f;\n    data[i] = new Array(cols.length);\n    for (var j = 0; j < cols.length; j++) {\n      data[i][j] = cols[j].v;\n    }\n  }\n\n  // Start storing data in row 2, col 1\n  var START_ROW = 2;      // skip header row\n  var START_COL = 1;\n  sheet.getRange(START_ROW, START_COL, rows.length, headers.length).setValues(data);\n\n  Logger.log('Results spreadsheet created: %s', spreadsheet.getUrl());\n}\n```\n该查询通过查看莎士比亚的作品（属于 BigQuery的公共数据集），得出他所有作品中出现次数最多的前10个单词，并按流行程度从高到低排序。试想一下，手动执行此操作会有多大的乐趣，您应该对BigQuery的有用性有所了解。\n### 点击runQuery运行,期间可能需要需要授权认证\n![png](../img/bigquery/bigquery_presentation/6.png)\n![png](../img/bigquery/bigquery_presentation/7.png)\n\n### 成功运行后,我们可以去drive.google.com中找到excel表格的url链接,点击Most common words那块条框\n![png](../img/bigquery/bigquery_presentation/8.png)\n\n### 点击进入后,我们可以看到bigquery的公共集查询的数据现在用表格形式出现了\n![png](../img/bigquery/bigquery_presentation/9.png)\n\n### 我们可以去big query 用sql语句操作一遍,得到的数据也很excel表格一样\n![png](../img/bigquery/bigquery_presentation/10.png)\n\n### 下一步,我们要通过数据创建图表📈\n##### 1 到目前为止,我们编写了一个查询莎士比亚的应用程序,进行了排序,然后将结构显示在表格中,我们现在要createColumnChart()功能,{在最后一行代码之后}bq-sheets-slides.gsrunQuery()\n```\n/**\n * Uses spreadsheet data to create columnar chart.\n * @param {Spreadsheet} Spreadsheet containing results data\n * @returns {EmbeddedChart} visualizing the results\n * @see http://developers.google.com/apps-script/reference/spreadsheet/embedded-chart\n */\nfunction createColumnChart(spreadsheet) {\n  // Retrieve the populated (first and only) Sheet.\n  var sheet = spreadsheet.getSheets()[0];\n  // Data range in Sheet is from cell A2 to B11\n  var START_CELL = 'A2';  // skip header row\n  var END_CELL = 'B11';\n  // Place chart on Sheet starting on cell E5.\n  var START_ROW = 5;      // row 5\n  var START_COL = 5;      // col E\n  var OFFSET = 0;\n\n  // Create & place chart on the Sheet using above params.\n  var chart = sheet.newChart()\n     .setChartType(Charts.ChartType.COLUMN)\n     .addRange(sheet.getRange(START_CELL + ':' + END_CELL))\n     .setPosition(START_ROW, START_COL, OFFSET, OFFSET)\n     .build();\n  sheet.insertChart(chart);\n}\n```\n#### 2 返回电子表格:在上面的代码中, createColumnChart()函数需要电子表格对象,因此调整应用程序以返回spreadsheet对象,以便可以将其传递给createColumnChart().所以我们在runQuery()函数最后一行添加一下代码\n```\nLogger.log('Results spreadsheet created: %s', spreadsheet.getUrl());\n\n  // Return the spreadsheet object for later use.\n  return spreadsheet;\n}\n```\n#### 3 执行createBigQueryPresentation()功能,我们把BigQuery和chart-creation 功能分开是不错的注意.我们可以创建一个createBigQueryPresentation()功能去驱动app,调用两者和调用createColumnChart()函数\n```\n/**\n * Runs a BigQuery query, adds data and a chart in a Sheet.\n */\nfunction createBigQueryPresentation() {\n  var spreadsheet = runQuery();\n  createColumnChart(spreadsheet);\n}\n```\n我们把这块代码放到以下代码下面\n```\n// Filename for data results\nvar QUERY_NAME = \"Most common words in all of Shakespeare's works\";\n// Replace this value with your Google Cloud API project ID\nvar PROJECT_ID = 'project-id-4323960745859879834';\nif (!PROJECT_ID) throw Error('Project ID is required in setup');\n```\n\n### 为了使得代码可以复用性,我们有两步要做\n1: 返回spreadsheet对象\n2: 创建一个驱动函数\n同时如果一个同事需要复用runQuery()函数,但是不想要连接登录呢\n我们可以代码修改一下,具体如下:\n```javascript\n/**\n * Runs a BigQuery query, adds data and a chart in a Sheet.\n */\nfunction createBigQueryPresentation() {\n  var spreadsheet = runQuery();\n  Logger.log('Results spreadsheet created: %s', spreadsheet.getUrl());\n  createColumnChart(spreadsheet);\n}\n```\n#### 具体呈现如下:\n\n![png](../img/bigquery/bigquery_presentation/11.png)\n\n整段代码具体如下:\n``` javascript\n/**\n * Copyright 2018 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at apache.org/licenses/LICENSE-2.0.\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Filename for data results\nvar QUERY_NAME = \"Most common words in all of Shakespeare's works\";\n// Replace this value with your Google Cloud API project ID\nvar PROJECT_ID = 'qwiklabs-gcp-5c0cf6ad321746e4';\nif (!PROJECT_ID) throw Error('Project ID is required in setup');\n\n/**\n * Runs a BigQuery query, adds data and a chart in a Sheet.\n */\nfunction createBigQueryPresentation() {\n  var spreadsheet = runQuery();\n  Logger.log('Results spreadsheet created: %s', spreadsheet.getUrl());\n  createColumnChart(spreadsheet);\n}\n\n/**\n * Runs a BigQuery query; puts results into Sheet. You must enable\n * the BigQuery advanced service before you can run this code.\n * @see http://developers.google.com/apps-script/advanced/bigquery#run_query\n * @see http://github.com/gsuitedevs/apps-script-samples/blob/master/advanced/bigquery.gs\n *\n * @returns {Spreadsheet} Returns a spreadsheet with BigQuery results\n * @see http://developers.google.com/apps-script/reference/spreadsheet/spreadsheet\n */\nfunction runQuery() {\n  // Replace sample with your own BigQuery query.\n  var request = {\n    query:\n        'SELECT ' +\n            'LOWER(word) AS word, ' +\n            'SUM(word_count) AS count ' +\n        'FROM [bigquery-public-data:samples.shakespeare] ' +\n        'GROUP BY word ' +\n        'ORDER BY count ' +\n        'DESC LIMIT 10'\n  };\n  var queryResults = BigQuery.Jobs.query(request, PROJECT_ID);\n  var jobId = queryResults.jobReference.jobId;\n\n  // Wait for BQ job completion (with exponential backoff).\n  var sleepTimeMs = 500;\n  while (!queryResults.jobComplete) {\n    Utilities.sleep(sleepTimeMs);\n    sleepTimeMs *= 2;\n    queryResults = BigQuery.Jobs.getQueryResults(PROJECT_ID, jobId);\n  }\n\n  // Get all results from BigQuery.\n  var rows = queryResults.rows;\n  while (queryResults.pageToken) {\n    queryResults = BigQuery.Jobs.getQueryResults(PROJECT_ID, jobId, {\n      pageToken: queryResults.pageToken\n    });\n    rows = rows.concat(queryResults.rows);\n  }\n\n  // Return null if no data returned.\n  if (!rows) {\n    return Logger.log('No rows returned.');\n  }\n\n  // Create the new results spreadsheet.\n  var spreadsheet = SpreadsheetApp.create(QUERY_NAME);\n  var sheet = spreadsheet.getActiveSheet();\n\n  // Add headers to Sheet.\n  var headers = queryResults.schema.fields.map(function(field) {\n    return field.name.toUpperCase();\n  });\n  sheet.appendRow(headers);\n\n  // Append the results.\n  var data = new Array(rows.length);\n  for (var i = 0; i < rows.length; i++) {\n    var cols = rows[i].f;\n    data[i] = new Array(cols.length);\n    for (var j = 0; j < cols.length; j++) {\n      data[i][j] = cols[j].v;\n    }\n  }\n\n  // Start storing data in row 2, col 1\n  var START_ROW = 2;      // skip header row\n  var START_COL = 1;\n  sheet.getRange(START_ROW, START_COL, rows.length, headers.length).setValues(data);\n\n  Logger.log('Results spreadsheet created: %s', spreadsheet.getUrl());\n\n  // Return the spreadsheet object for later use.\n  return spreadsheet;\n}\n\n/**\n * Uses spreadsheet data to create columnar chart.\n * @param {Spreadsheet} Spreadsheet containing results data\n * @returns {EmbeddedChart} visualizing the results\n * @see http://developers.google.com/apps-script/reference/spreadsheet/embedded-chart\n */\nfunction createColumnChart(spreadsheet) {\n  // Retrieve the populated (first and only) Sheet.\n  var sheet = spreadsheet.getSheets()[0];\n  // Data range in Sheet is from cell A2 to B11\n  var START_CELL = 'A2';  // skip header row\n  var END_CELL = 'B11';\n  // Place chart on Sheet starting on cell E5.\n  var START_ROW = 5;      // row 5\n  var START_COL = 5;      // col E\n  var OFFSET = 0;\n\n  // Create & place chart on the Sheet using above params.\n  var chart = sheet.newChart()\n     .setChartType(Charts.ChartType.COLUMN)\n     .addRange(sheet.getRange(START_CELL + ':' + END_CELL))\n     .setPosition(START_ROW, START_COL, OFFSET, OFFSET)\n     .build();\n  sheet.insertChart(chart);\n}\n```\n\n然后我们通过选run-createBigQueryPresentation()函数,我们去到google drive去看我们表格,图片\n\n最后一部分,我们将创建一个新的google ppt,然后再标题幻灯片上填充标题和副标题,然后添加2张新slide,一个用于数据单元格,另外一个用于图表\n#### 1 创建幻灯片\n``` javascript\n/**\n * Create presentation with spreadsheet data & chart\n * @param {Spreadsheet} Spreadsheet with results data\n * @param {EmbeddedChart} Sheets chart to embed on slide\n * @returns {Presentation} Slide deck with results\n */\nfunction createSlidePresentation(spreadsheet, chart) {\n  // Create the new presentation.\n  var deck = SlidesApp.create(QUERY_NAME);\n\n  // Populate the title slide.\n  var [title, subtitle] = deck.getSlides()[0].getPageElements();\n  title.asShape().getText().setText(QUERY_NAME);\n  subtitle.asShape().getText().setText('via GCP and G Suite APIs:\\n' +\n    'Google Apps Script, BigQuery, Sheets, Slides');\n```\n\n#### 2 添加数据表: 我们通过createSlidePresentation()把单元格数据从google表格导入我们的新ppt中\n``` java\n  // Data range to copy is from cell A1 to B11\n  var START_CELL = 'A1';  // include header row\n  var END_CELL = 'B11';\n  // Add the table slide and insert an empty table on it of\n  // the dimensions of the data range; fails if Sheet empty.\n  var tableSlide = deck.appendSlide(SlidesApp.PredefinedLayout.BLANK);\n  var sheetValues = spreadsheet.getSheets()[0].getRange(\n      START_CELL + ':' + END_CELL).getValues();\n  var table = tableSlide.insertTable(sheetValues.length, sheetValues[0].length);\n\n  // Populate the table with spreadsheet data.\n  for (var i = 0; i < sheetValues.length; i++) {\n    for (var j = 0; j < sheetValues[0].length; j++) {\n      table.getCell(i, j).getText().setText(String(sheetValues[i][j]));\n    }\n  }\n```\n\n#### 3 导入图表,在createSlidePresentation()函数中再创建一张ppt,从电子表格中导入图表,返回Presentation对象deck\n``` javascript\n  // Add a chart slide and insert the chart on it.\n  var chartSlide = deck.appendSlide(SlidesApp.PredefinedLayout.BLANK);\n  chartSlide.insertSheetsChart(chart);\n\n  // Return the presentation object for later use.\n  return deck;\n}\n```\n#### 4 返回图: 我们需要让createColumnChart()返回对象,所以我们在尾端接收createColumnChart():\n```javascript\n // Return chart object for later use\n  return chart;\n}\n```\n\n#### 更新createBigQueryPresentation(),因为createColumnChart()返回了图表,所以我们要把这个图表保存到变量,然后把电子表格和图表都传递给createSlidePresentation()函数\n```javascript\n/**\n * Runs a BigQuery query, adds data and a chart in a Sheet,\n * and adds the data and chart to a new slide presentation.\n */\nfunction createBigQueryPresentation() {\n  var spreadsheet = runQuery();\n  Logger.log('Results spreadsheet created: %s', spreadsheet.getUrl());\n  var chart = createColumnChart(spreadsheet);\n  var deck = createSlidePresentation(spreadsheet, chart);\n  Logger.log('Results slide deck created: %s', deck.getUrl());\n}\n```\n\n最后的版本如下:\n``` javescript\n// Filename for data results\nvar QUERY_NAME = \"Most common words in all of Shakespeare's works\";\n// Replace this value with your Google Cloud API project ID\nvar PROJECT_ID = '<YOUR_PROJECT_ID>';\nif (!PROJECT_ID) throw Error('Project ID is required in setup');\n\n/**\n * Runs a BigQuery query; puts results into Sheet. You must enable\n * the BigQuery advanced service before you can run this code.\n * @see http://developers.google.com/apps-script/advanced/bigquery#run_query\n * @see http://github.com/gsuitedevs/apps-script-samples/blob/master/advanced/bigquery.gs\n *\n * @returns {Spreadsheet} Returns a spreadsheet with BigQuery results\n * @see http://developers.google.com/apps-script/reference/spreadsheet/spreadsheet\n */\nfunction runQuery() {\n  // Replace sample with your own BigQuery query.\n  var request = {\n    query:\n        'SELECT ' +\n            'LOWER(word) AS word, ' +\n            'SUM(word_count) AS count ' +\n        'FROM [bigquery-public-data:samples.shakespeare] ' +\n        'GROUP BY word ' +\n        'ORDER BY count ' +\n        'DESC LIMIT 10'\n  };\n  var queryResults = BigQuery.Jobs.query(request, PROJECT_ID);\n  var jobId = queryResults.jobReference.jobId;\n\n  // Wait for BQ job completion (with exponential backoff).\n  var sleepTimeMs = 500;\n  while (!queryResults.jobComplete) {\n    Utilities.sleep(sleepTimeMs);\n    sleepTimeMs *= 2;\n    queryResults = BigQuery.Jobs.getQueryResults(PROJECT_ID, jobId);\n  }\n\n  // Get all results from BigQuery.\n  var rows = queryResults.rows;\n  while (queryResults.pageToken) {\n    queryResults = BigQuery.Jobs.getQueryResults(PROJECT_ID, jobId, {\n      pageToken: queryResults.pageToken\n    });\n    rows = rows.concat(queryResults.rows);\n  }\n\n  // Return null if no data returned.\n  if (!rows) {\n    return Logger.log('No rows returned.');\n  }\n\n  // Create the new results spreadsheet.\n  var spreadsheet = SpreadsheetApp.create(QUERY_NAME);\n  var sheet = spreadsheet.getActiveSheet();\n\n  // Add headers to Sheet.\n  var headers = queryResults.schema.fields.map(function(field) {\n    return field.name.toUpperCase();\n  });\n  sheet.appendRow(headers);\n\n  // Append the results.\n  var data = new Array(rows.length);\n  for (var i = 0; i < rows.length; i++) {\n    var cols = rows[i].f;\n    data[i] = new Array(cols.length);\n    for (var j = 0; j < cols.length; j++) {\n      data[i][j] = cols[j].v;\n    }\n  }\n\n  // Start storing data in row 2, col 1\n  var START_ROW = 2;      // skip header row\n  var START_COL = 1;\n  sheet.getRange(START_ROW, START_COL, rows.length, headers.length).setValues(data);\n\n  // Return the spreadsheet object for later use.\n  return spreadsheet;\n}\n\n/**\n * Uses spreadsheet data to create columnar chart.\n * @param {Spreadsheet} Spreadsheet containing results data\n * @returns {EmbeddedChart} visualizing the results\n * @see http://developers.google.com/apps-script/reference/spreadsheet/embedded-chart\n */\nfunction createColumnChart(spreadsheet) {\n  // Retrieve the populated (first and only) Sheet.\n  var sheet = spreadsheet.getSheets()[0];\n  // Data range in Sheet is from cell A2 to B11\n  var START_CELL = 'A2';  // skip header row\n  var END_CELL = 'B11';\n  // Place chart on Sheet starting on cell E5.\n  var START_ROW = 5;      // row 5\n  var START_COL = 5;      // col E\n  var OFFSET = 0;\n\n  // Create & place chart on the Sheet using above params.\n  var chart = sheet.newChart()\n     .setChartType(Charts.ChartType.COLUMN)\n     .addRange(sheet.getRange(START_CELL + ':' + END_CELL))\n     .setPosition(START_ROW, START_COL, OFFSET, OFFSET)\n     .build();\n  sheet.insertChart(chart);\n\n  // Return the chart object for later use.\n  return chart;\n}\n\n/**\n * Create presentation with spreadsheet data & chart\n * @param {Spreadsheet} Spreadsheet with results data\n * @param {EmbeddedChart} Sheets chart to embed on slide\n * @returns {Presentation} Returns a slide deck with results\n * @see http://developers.google.com/apps-script/reference/slides/presentation\n */\nfunction createSlidePresentation(spreadsheet, chart) {\n  // Create the new presentation.\n  var deck = SlidesApp.create(QUERY_NAME);\n\n  // Populate the title slide.\n  var [title, subtitle] = deck.getSlides()[0].getPageElements();\n  title.asShape().getText().setText(QUERY_NAME);\n  subtitle.asShape().getText().setText('via GCP and G Suite APIs:\\n' +\n    'Google Apps Script, BigQuery, Sheets, Slides');\n\n  // Data range to copy is from cell A1 to B11\n  var START_CELL = 'A1';  // include header row\n  var END_CELL = 'B11';\n  // Add the table slide and insert an empty table on it of\n  // the dimensions of the data range; fails if Sheet empty.\n  var tableSlide = deck.appendSlide(SlidesApp.PredefinedLayout.BLANK);\n  var sheetValues = spreadsheet.getSheets()[0].getRange(\n      START_CELL + ':' + END_CELL).getValues();\n  var table = tableSlide.insertTable(sheetValues.length, sheetValues[0].length);\n\n  // Populate the table with spreadsheet data.\n  for (var i = 0; i < sheetValues.length; i++) {\n    for (var j = 0; j < sheetValues[0].length; j++) {\n      table.getCell(i, j).getText().setText(String(sheetValues[i][j]));\n    }\n  }\n\n  // Add a chart slide and insert the chart on it.\n  var chartSlide = deck.appendSlide(SlidesApp.PredefinedLayout.BLANK);\n  chartSlide.insertSheetsChart(chart);\n\n  // Return the presentation object for later use.\n  return deck;\n}\n\n/**\n * Runs a BigQuery query, adds data and a chart in a Sheet,\n * and adds the data and chart to a new slide presentation.\n */\nfunction createBigQueryPresentation() {\n  var spreadsheet = runQuery();\n  Logger.log('Results spreadsheet created: %s', spreadsheet.getUrl());\n  var chart = createColumnChart(spreadsheet);\n  var deck = createSlidePresentation(spreadsheet, chart);\n  Logger.log('Results slide deck created: %s', deck.getUrl());\n}\n```\n\n## 执行后,我们就会看到以下效果:\n\n![png](../img/bigquery/bigquery_presentation/12.png)\n![png](../img/bigquery/bigquery_presentation/13.png)\n![png](../img/bigquery/bigquery_presentation/14.png)\n![png](../img/bigquery/bigquery_presentation/15.png)\n![png](../img/bigquery/bigquery_presentation/16.png)\n\n# 是不是超酷呢??"
        },
        {
          "id": "/2020/4/15/bq_sql常用短语",
          "metadata": {
            "permalink": "/blog/2020/4/15/bq_sql常用短语",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-15-bq_sql常用短语.md",
            "source": "@site/blog/2020-4-15-bq_sql常用短语.md",
            "title": "Bigquery中sql语句常用命令",
            "description": "连接数据库",
            "date": "2020-04-15T00:00:00.000Z",
            "formattedDate": "April 15, 2020",
            "tags": [
              {
                "label": "gcp",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "command line",
                "permalink": "/blog/tags/command-line"
              },
              {
                "label": "github",
                "permalink": "/blog/tags/github"
              }
            ],
            "readingTime": 3.475,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "bq_sql",
              "title": "Bigquery中sql语句常用命令",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "gcp",
                "command line",
                "github"
              ]
            },
            "prevItem": {
              "title": "google script + excel+ google drive",
              "permalink": "/blog/2020/4/16/bq_脚本制作presentation"
            },
            "nextItem": {
              "title": "谷歌常用命令",
              "permalink": "/blog/2020/4/15/gcp_常用命令"
            }
          },
          "content": "## 连接数据库\n```sql\ngcloud sql connect  qwiklabs-demo --user=root\n```\n\n## 合并两个表格union\n<!-- truncate -->\n```sql\nSELECT start_station_name AS top_stations, num FROM london1 WHERE num>100000\nUNION\nSELECT end_station_name, num FROM london2 WHERE num>100000\nORDER BY top_stations DESC;\n```\n参考 [Uninon和join的区别](https://blog.csdn.net/qq_41359051/article/details/98469387#UNION_157)\n\n[UNION](https://www.cnblogs.com/CraryPrimitiveMan/p/3665154.html)中间的关键字通过将“ london2”数据与“ london1”同化来组合这些查询的输出。由于将“ london1”与“ london2”结合在一起，因此列名优先为“ top_stations”和“ num”。\n\nORDER BY 将按照“ top_stations”列值的字母顺序和降序对最终的联合表进行排序。\n![png](../img/mysql/bq_mysql.png)\n\n## 添加数据  insert into 表 (字段,字段) values (值,值);\n```sql\nINSERT INTO london1 (start_station_name, num) VALUES (\"test destination\", 1);\n```\n## 运行查询命令\nbq query --use_legacy_sql=false 'select 字段 from 表格 where 条件'\n注意的地方是 \n### use_legacy_sql=false 表示使用标准sql语句\n### 条件的时候可以使用双引号做区分\"\"\n\n``` sql\n#standardSQL\nSELECT  FROM `data-to-inghts.ecommerce.rev_transactions` LIMIT 1000\n```\n```\nWhat's wrong with the previous query to view 1000 items? check\nThere is a typo in the dataset name check\nWe have not specified any columns in the SELECT\nThere is a typo in the table name\nWe are using legacy SQL\n```\n\nwhat about this updated query?\n```sql\n#standardSQL\nSELECT * FROM [data-to-insights:ecommerce.rev_transactions] LIMIT 1000\n'''\nwe are using legacy sql\n\nwhat about this query that uses standard SQL\n```sql\n#standardSQL\nSELECT FROM `data-to-insights.ecommerce.rev_transactions`\n```\nno columns defined in select\n\nwhat about now?\n```sql\n#standardSQL\nSELECT\nfullVisitorId\nFROM `data-to-insights.ecommerce.rev_transactions`\n```\nwithout aggregations,limits or sorting, this query is not insightful\n\nwhat about now?\n```sql\n#standardSQL\nSELECT fullVisitorId hits_page_pageTitle\nFROM `data-to-insights.ecommerce.rev_transactions` LIMIT 1000\n```\nit can be excuated.\n\nwhat about now?\n```sql\n#standardSQL\nSELECT\n  fullVisitorId\n  , hits_page_pageTitle\nFROM `data-to-insights.ecommerce.rev_transactions` LIMIT 1000\n```\nthis returns result, but visitors maybe counted twice.\n\nwhat about this? an aggregation function, count(), was added.\n```sql\n#standardSQL\nSELECT\nCOUNT(fullVisitorId) AS visitor_count\n, hits_page_pageTitle\nFROM `data-to-insights.ecommerce.rev_transactions`\n```\n没去重,the count()function does not de-deduplicate the same fullvisitorid\nit is missing a group by clause\n\nin this next query, group by and distinct statements were added\n```sql\n#standardSQL\nSELECT\nCOUNT(DISTINCT fullVisitorId) AS visitor_count\n, hits_page_pageTitle\nFROM `data-to-insights.ecommerce.rev_transactions`\nGROUP BY hits_page_pageTitle\n```\n\nwe can add filter 'where' to filter results\n```sql\n#standardSQL\nSELECT\nCOUNT(DISTINCT fullVisitorId) AS visitor_count\n, hits_page_pageTitle\nFROM `data-to-insights.ecommerce.rev_transactions`\nWHERE hits_page_pageTitle = \"Checkout Confirmation\"\nGROUP BY hits_page_pageTitle\n```\n\nList the cities with the most transactions with your ecommerce site\n```sql\nSELECT\ngeoNetwork_city,\nsum(totals_transactions) as totals_transactions,\nCOUNT( DISTINCT fullVisitorId) AS distinct_visitors\nFROM\n`data-to-insights.ecommerce.rev_transactions`\nGROUP BY geoNetwork_city\nOrder by distinct_visitors Desc\n```\n\nwhats wrong with the following query?\n```sql\n#standardSQL\nSELECT\ngeoNetwork_city,\nSUM(totals_transactions) AS total_products_ordered,\nCOUNT( DISTINCT fullVisitorId) AS distinct_visitors,\nSUM(totals_transactions) / COUNT( DISTINCT fullVisitorId) AS avg_products_ordered\nFROM\n`data-to-insights.ecommerce.rev_transactions`\nWHERE avg_products_ordered > 20\nGROUP BY geoNetwork_city\nORDER BY avg_products_ordered DESC\n```\nwe cannot filter aggregated fields in the 'where' clause ( use 'Having' instead) 不可以用where来聚合函数的字段,要用having\nwe cannot filter on aliased fields within the 'where' clause\nwhere过滤句中不能使用别名\n\npossible solution\n```sql\nselect geoNetwork_city, SUM(totals_transactions) as total_products_ordered, count(distinct fullvisitorID) as distinct_visitors,\nsum(totals_transactions) / count(distinct fullVisitorId) As avg_products_ordered\nfrom\n`data-to-insights.ecommerce.rev_transactions`\nGroup by geoNetwork_city\nHaving avg_products_ordered > 20\norder by avg_products_ordered\n```\n\n```sql\n#standardSQL\nSELECT\nCOUNT(hits_product_v2ProductName) as number_of_products,\nhits_product_v2ProductCategory\nFROM `data-to-insights.ecommerce.rev_transactions`\nWHERE hits_product_v2ProductName IS NOT NULL\nGROUP BY hits_product_v2ProductCategory\nORDER BY number_of_products DESC\n```\n这里的问题是count()函数里面的字段没有做distinct,有可能导致重复\nThe count() function is not the distinct number of products in each category\n\npossible solution\n```sql\n#standardSQL\nSELECT\nCOUNT(DISTINCT hits_product_v2ProductName) as number_of_products,\nhits_product_v2ProductCategory\nFROM `data-to-insights.ecommerce.rev_transactions`\nWHERE hits_product_v2ProductName IS NOT NULL\nGROUP BY hits_product_v2ProductCategory\nORDER BY number_of_products DESC\nLIMIT 5\n```"
        },
        {
          "id": "/2020/4/15/gcp_常用命令",
          "metadata": {
            "permalink": "/blog/2020/4/15/gcp_常用命令",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-15-gcp_常用命令.md",
            "source": "@site/blog/2020-4-15-gcp_常用命令.md",
            "title": "谷歌常用命令",
            "description": "列出活动账号名称",
            "date": "2020-04-15T00:00:00.000Z",
            "formattedDate": "April 15, 2020",
            "tags": [
              {
                "label": "gcp",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "command line",
                "permalink": "/blog/tags/command-line"
              },
              {
                "label": "github",
                "permalink": "/blog/tags/github"
              }
            ],
            "readingTime": 0.355,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "gcp_command_line",
              "title": "谷歌常用命令",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "gcp",
                "command line",
                "github"
              ]
            },
            "prevItem": {
              "title": "Bigquery中sql语句常用命令",
              "permalink": "/blog/2020/4/15/bq_sql常用短语"
            },
            "nextItem": {
              "title": "dataflow简单入门-apache beam 基本概念",
              "permalink": "/blog/2020/4/11/dataflow-apache beam基本概念"
            }
          },
          "content": "列出活动账号名称\n```python\ngcloud auth list\n```\n<!--truncate-->\n列出项目id\n```\ngcloud config list project\n```\n\n设置项目id \n```\ngcloud config set project project ID\n```\n\n列举所有项目\n```\ngcloud projects list\n```\n\n查询有哪些可以开启的api,并且开启\n```\ngcloud services list\ngcloud services enable bigquery.googleapis.com\n```"
        },
        {
          "id": "/2020/4/11/dataflow-apache beam基本概念",
          "metadata": {
            "permalink": "/blog/2020/4/11/dataflow-apache beam基本概念",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-11-dataflow-apache beam基本概念.md",
            "source": "@site/blog/2020-4-11-dataflow-apache beam基本概念.md",
            "title": "dataflow简单入门-apache beam 基本概念",
            "description": "1 Pipeline 管道",
            "date": "2020-04-11T00:00:00.000Z",
            "formattedDate": "April 11, 2020",
            "tags": [
              {
                "label": "dataflow",
                "permalink": "/blog/tags/dataflow"
              },
              {
                "label": "bigquery",
                "permalink": "/blog/tags/bigquery"
              },
              {
                "label": "subpub",
                "permalink": "/blog/tags/subpub"
              },
              {
                "label": "steaming",
                "permalink": "/blog/tags/steaming"
              }
            ],
            "readingTime": 3.46,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataflow4",
              "title": "dataflow简单入门-apache beam 基本概念",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataflow",
                "bigquery",
                "subpub",
                "steaming"
              ]
            },
            "prevItem": {
              "title": "谷歌常用命令",
              "permalink": "/blog/2020/4/15/gcp_常用命令"
            },
            "nextItem": {
              "title": "dataflow简单入门-使用apache_beam创建,运行作业",
              "permalink": "/blog/2020/4/11/dataflow创建作业"
            }
          },
          "content": "## 1 Pipeline 管道\n\n## 2 Pcollection\n\n## 3 Ptransform\n\n<!--truncate-->\n总体流程就是 设置pipeline >> read data >> pcollection >> ptransform >> pcollection \n\n其中ptransform有ParDo和耦合函数功能\n\nParDo()里面只能接收的是DoFn类或集成DoFn类的对象的函数,ParDo操作的是每一行的数据,就好像dataframe里面的一行\n# 敲黑板时间!!\n## 读取Csv和txt文件,Pipeline读取得每一行为字符串,也就是,属于pandas里面的seris, 也就是只有一列,如果要分开几列,我们就要split字符串,然后做成key:value字典格式.\n## 读取avro文件时候,Pipeline读取的每一行为字典{},也就是pandas里面的一行dataframe,如果要取值,我们需要element[列名]就可以取到\n\n聚合函数操作后,就会返回一个整体的新的Pcollection\n记住字典格式要做聚合函数要变成列表或者元组\n## Groupbykey--对象是一个Pcollection,首先选择要做聚合的key和值,然后tranform一个pcollection格式为turple,里面为(key,value),最后通过管道 | beam.GroupByKey()\n就会生成按key分类, 以下这样的效果,具体说明[例子](https://beam.apache.org/documentation/programming-guide/#core-beam-transforms)\n```\ncat, 1\ndog, 5\nand, 1\njump, 3\ntree, 2\ncat, 5\ndog, 2\nand, 2\ncat, 9\nand, 6\n...\n```\n变成\n```\ncat, [1,5,9]\n\ndog, [5,2]\n\nand, [1,2,6]\n\njump, [3]\n\ntree, [2]               \n...\n```\n\n##CoGroupKey--操作对象Pcollection,把两个Pcollection通过key连接起来,\n比如Pcollection1:  是宠物名字和年龄\n'''\n\"Amy\", 9\n\n\"Tom\", 3\n\n\"Shierly\", 3\n\n\"Miccle\", 4\n\n\"Dockey\", 4\n```\nPcollection2: 是宠物名字和主人名字\n```\n\"Amy\", \"michael\"\n\n\"Tom\", \"Tommy\"\n\n\"Shierly\", \"Darren\"\n\n\"Miccle\", \"Cherry\"\n\n\"Dockey\", \"Dick\"\n```\n```\nage_list = [(\"Amy\", 9),\n\n       (\"Tom\", 3),\n\n      (\"Shierly\", 3)\n\n      (\"Miccle\", 4)\n\n      (\"Dockey\", 4])]\n\nOwner_list = [(\"Amy\", \"michael\"),\n\n(\"Tom\", \"Tommy\"),\n\n(\"Shierly\", \"Darren\"),\n\n(\"Miccle\", \"Cherry\"),\n\n(\"Dockey\", \"Dick\")]\n\n然后我们开始创建两个Pcollections\n\nage = P |\"create age\" >> beam.Create(age_list)\nowner = P | \"create owner\" >> beam.Create(owner_list)\n\n我们用CoGroupbyKey的时候,是使用key,value的字典格式作为输入\n\n格式为 results = {\"Pcollection1名字\":Pcollection1, \"Pcollection2名字\":Pcollection2} | beam.CoGroupByKey()\n\n得到的效果是 [(Key1,{\"Pcollection1名字\":Pcollection1, \"Pcollection2名字\":Pcollection2}), (Key2,{\"Pcollection1名字\":Pcollection1, \"Pcollection2名字\":Pcollection2}),(Key3,{\"Pcollection1名字\":Pcollection1, \"Pcollection2名字\":Pcollection2})]\n\n\n呈现效果是:\n[\"Tom\" , {'age':3, \"owner\":\"Tommy\"}\n\"Shierly\" ,{'age':3,\"owner\":\"Darren\"}\n\"Amy\",{'age':9,\"owner\":\"michael\"}\n...\n]\n```\n```\n## 总结CoGroupByKey就是把两个Pcollection通过共同的Key连接起来,然后用元组(key,value)显示出来,values是一个字典格式,包含Pcollection名字:value_list\n\n# 一句话表示: \n## COGroupByKey就是元组key包含字典pcollection与定义的value\n## GroubByKey就是元组key包含value_list\n\n## CombinePerKey(beam.combiners.MeanCombineFn)\n就是把groupbykey 再对每个key的value做加权平均\n\n## Flatten 把多个PCollection 变成一个 PCollection, \n## 说白了就是把多个列表的值放到一个列表\n```\n# Flatten takes a tuple of PCollection objects.\n# Returns a single PCollection that contains all of the elements in the PCollection objects in that tuple.\nmerged = (\n    (pcoll1, pcoll2, pcoll3)\n    # A list of tuples can be \"piped\" directly into a Flatten transform.\n    | beam.Flatten())\n```"
        },
        {
          "id": "/2020/4/11/dataflow创建作业",
          "metadata": {
            "permalink": "/blog/2020/4/11/dataflow创建作业",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-11-dataflow创建作业.md",
            "source": "@site/blog/2020-4-11-dataflow创建作业.md",
            "title": "dataflow简单入门-使用apache_beam创建,运行作业",
            "description": "1. 在本地创建来测试运行",
            "date": "2020-04-11T00:00:00.000Z",
            "formattedDate": "April 11, 2020",
            "tags": [
              {
                "label": "dataflow",
                "permalink": "/blog/tags/dataflow"
              },
              {
                "label": "bigquery",
                "permalink": "/blog/tags/bigquery"
              },
              {
                "label": "subpub",
                "permalink": "/blog/tags/subpub"
              },
              {
                "label": "steaming",
                "permalink": "/blog/tags/steaming"
              }
            ],
            "readingTime": 4.76,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataflow2",
              "title": "dataflow简单入门-使用apache_beam创建,运行作业",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataflow",
                "bigquery",
                "subpub",
                "steaming"
              ]
            },
            "prevItem": {
              "title": "dataflow简单入门-apache beam 基本概念",
              "permalink": "/blog/2020/4/11/dataflow-apache beam基本概念"
            },
            "nextItem": {
              "title": "dataflow简单入门-流数据输入到bigquery",
              "permalink": "/blog/2020/4/11/dataflow流数据输入到bigquery"
            }
          },
          "content": "1. 在本地创建来测试运行\n2. 放到dataflow上运行\n<!--truncate-->\n## 本地创建job\n1 首先是安装我们需要打工具包,因为我是将来是要运行到GCP上的,所以我们安装的是\n```\npip install apache_beam[gcp]\n```\n2 导入各种包\n```\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.options.pipeline_options import GoogleCloudOptions\nfrom apache_beam.options.pipeline_options import StandardOptions\nfrom apache_beam.io.textio import ReadFromText, WriteToText #用来读写文件\n```\n3 设置配置\n```\n# 输入输出路径\ninput_filename = \"./input.txt\"\noutput_filename = \"./output.txt\"\n\n#指定执行选项,以告诉Pipeline运行位置和运行方式\noptions = PipelineOptions()\noptions.view_as(StandardOptions).runner = \"direct\" #表示本地运行\n\n# 写功能类\n#DoFn就是把类,转换,callable的功能集合在一起,我们可以直接继承,方便后面管道使用\n#所有ParDo的操作都必须要跟DoFn类的函数,比如ParDo(DoFn())\n\n\nclass Split(beam.DoFn):\n    def process(self, element):\n        \"\"\"\n        Splits each row on commas and returns a dictionary representing the row\n        我们这里做的事情就是类似mapper, 将所有元素变成字典\n        \"\"\"\n        country,duration,user = element.split(\",\")\n        print(len(element))# element就是每行的数据,跟hdfs一样,视力有问题\n        return [\n            {\n                'country':country,\n                'duration':duration,\n                'user':user\n            }\n        ]\n    \nclass CollectTimings(beam.DoFn):\n    def process(self,element):\n        result = [element('country'),element('duration')]\n        return result\n\nclass CollectUsers(beam.DoFn):\n    def process(self,element):\n        \"\"\"\n        Returns a list of tuples containing country and user name\n        \"\"\"\n        return [element('country'),element('user')]\n\nclass WriteToCSV(beam.DoFn):\n    def process(self,element):\n        \"\"\"\n        Prepares each row to be written in the csv\n        \"\"\"\n        result = [\"%s,%s,%s\"%(element[0],element[1]['user'][0],element[1]['timings'][0])]\n        return result\n\n#创建管道对象, 创建变量接收Pcollection, 一定要加上(),防止歧义,如果变成 rows = P 然后 再管 | ReadFromText(input_filename), 很容易报错\nwith beam.Pipeline(options=options) as p:\n    rows = (\n        P | ReadFromText(input_filename) | beam.ParDo(SPlit())\n    )\n    timings = (\n        rows |\n        beam.ParDo(CollectTimings()) |\n        \"Grouping timings\" >> beam.GroupByKey() | \n        \"Calculating average\" >> beam.CombineValues(\n            beam.combiners.MeanCombineFn()\n        )\n    )\n    users = (\n        rows |\n        beam.ParDo(CollectUsers()) |\n        \"Grouping users\" >> beam.GroupByKey() |\n        \"Counting users\" >> beam.CombineValues(\n            beam.combiners.CountCombineFn()\n        )\n    )\n    to_be_joined = (\n        {\n            'timings': timings,\n            'users': users\n        } |\n        beam.CoGroupByKey() |\n        beam.ParDo(WriteToCSV()) |\n        WriteToText(output_filename)\n    )\n\n\n#这里的格式为pvalue | \"label\" >> transform\n为什么要为什么要用\"label\" >>,其实如果任务不重复的时候,是可以不用的,但是比如这里耦合函数groupbykey出现已经在pipeline了,如果没有label就会报错,执行users时候就会报错\n\nGroupByKey是把key相同的拼为为一组,CombineValues是把值累积相加\nCoGroupByKey是根据key拼接一起\n```\n\n# 好的,本地测试好后, 我们要放到dataflow上跑了\n\n1 我们需要改的就是 input,ouput 路径,记住storage bucket的权限\n```\ninput_filename = \"gs://dataflow_s/input.txt\"\noutput_filename = \"gs://dataflow_s/output.txt\"\n```\n\n2 options\n```\ndataflow_options = ['--project=query-11','--job_name=test-job','--temp_location=gs://dataflow_s/tmp','--region=us-central1']\ndataflow_options.append('--staging_location=gs://dataflow_s/stage')\noptions = PipelineOptions(dataflow_options)\ngcloud_options = options.view_as(GoogleCloudOptions)\noptions.view_as(StandardOptions).runner = \"dataflow\" # 指定后端跑在dataflow\n```\n这里有个坑,如果你的apache beam是2.15版本以上的话,是需要写region这个参数的\n然后其他的都很本地一样,整体代码如下:\n```\nimport logging\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.options.pipeline_options import SetupOptions\nfrom apache_beam.options.pipeline_options import GoogleCloudOptions\nfrom apache_beam.options.pipeline_options import StandardOptions\nfrom apache_beam.io.textio import ReadFromText, WriteToText\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ninput_filename = \"gs://dataflow_s/input.txt\"\noutput_filename = \"gs://dataflow_s/output.txt\"\n\n\ndataflow_options = ['--project=query-11','--job_name=test-job','--temp_location=gs://dataflow_s/tmp']\ndataflow_options.append('--staging_location=gs://dataflow_s/stage')\noptions = PipelineOptions(dataflow_options)\ngcloud_options = options.view_as(GoogleCloudOptions)\n\n# gcloud_options.job_name = \"test-job\"\n\n\noptions.view_as(StandardOptions).runner = \"dataflow\"\n\n\nclass Split(beam.DoFn):\n\n    def process(self, element):\n        \"\"\"\n        Splits each row on commas and returns a dictionary representing the\n        row\n        \"\"\"\n        country, duration, user = element.split(\",\")\n\n        return [{\n            'country': country,\n            'duration': float(duration),\n            'user': user\n        }]\n\n\nclass CollectTimings(beam.DoFn):\n\n    def process(self, element):\n        \"\"\"\n        Returns a list of tuples containing country and duration\n        \"\"\"\n\n        result = [\n            (element['country'], element['duration'])\n        ]\n        return result\n\n\nclass CollectUsers(beam.DoFn):\n\n    def process(self, element):\n        \"\"\"\n        Returns a list of tuples containing country and user name\n        \"\"\"\n        result = [\n            (element['country'], element['user'])\n        ]\n        return result\n\n\nclass WriteToCSV(beam.DoFn):\n\n    def process(self, element):\n        \"\"\"\n        Prepares each row to be written in the csv\n        \"\"\"\n        result = [\n            \"{},{},{}\".format(\n                element[0],\n                element[1]['users'][0],\n                element[1]['timings'][0]\n            )\n        ]\n        return result\n\n\n\n\nwith beam.Pipeline(options=options) as p:\n    rows = (\n        p |\n        ReadFromText(input_filename) |\n        beam.ParDo(Split())\n    )\n\n    timings = (\n        rows |\n        beam.ParDo(CollectTimings()) |\n        \"Grouping timings\" >> beam.GroupByKey() |\n        \"Calculating average\" >> beam.CombineValues(\n            beam.combiners.MeanCombineFn()\n        )\n    )\n\n    users = (\n        rows |\n        beam.ParDo(CollectUsers()) |\n        \"Grouping users\" >> beam.GroupByKey() |\n        \"Counting users\" >> beam.CombineValues(\n            beam.combiners.CountCombineFn()\n        )\n    )\n\n    to_be_joined = (\n        {\n            'timings': timings,\n            'users': users\n        } |\n        beam.CoGroupByKey() |\n        beam.ParDo(WriteToCSV()) |\n        WriteToText(output_filename)\n    )\n```\n\n# 然后把这段代码放到gcloud上\n使用命令启动,具体是安装虚拟环境,进入虚拟环境,安装apache beam包,运行python文件\n```\npip3 install --upgrade virtualenv --user\npython3 -m virtualenv env\nsource env/bin/activate\npip3 install --quiet apache-beam[gcp]\npython dataflow.py\n```\n\n# 然后我们就可以去job上看到运行情况"
        },
        {
          "id": "/2020/4/11/dataflow流数据输入到bigquery",
          "metadata": {
            "permalink": "/blog/2020/4/11/dataflow流数据输入到bigquery",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-11-dataflow流数据输入到bigquery.md",
            "source": "@site/blog/2020-4-11-dataflow流数据输入到bigquery.md",
            "title": "dataflow简单入门-流数据输入到bigquery",
            "description": "概念: streaming 就是 动态的意思, streaming data就是动态数据, job status is streaming 就是这个作业是一直持续的",
            "date": "2020-04-11T00:00:00.000Z",
            "formattedDate": "April 11, 2020",
            "tags": [
              {
                "label": "dataflow",
                "permalink": "/blog/tags/dataflow"
              },
              {
                "label": "bigquery",
                "permalink": "/blog/tags/bigquery"
              },
              {
                "label": "subpub",
                "permalink": "/blog/tags/subpub"
              },
              {
                "label": "steaming",
                "permalink": "/blog/tags/steaming"
              }
            ],
            "readingTime": 1.89,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataflow1",
              "title": "dataflow简单入门-流数据输入到bigquery",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataflow",
                "bigquery",
                "subpub",
                "steaming"
              ]
            },
            "prevItem": {
              "title": "dataflow简单入门-使用apache_beam创建,运行作业",
              "permalink": "/blog/2020/4/11/dataflow创建作业"
            },
            "nextItem": {
              "title": "5分钟在谷歌云上使用dataflow来运行job",
              "permalink": "/blog/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2"
            }
          },
          "content": "概念: streaming 就是 动态的意思, streaming data就是动态数据, job status is streaming 就是这个作业是一直持续的\n<!--truncate-->\n## 大体流程是\n\ncreate dataset,table in Bigquery >> create topic in SubPub \n\n| >> create job and run >> public message in topic \n\n| >> go to your job to check wrte SucessfulRecords \n\n| >> go to your bigquery to check your data\n\n<!--truncate-->\n\n## 具体流程是\n\n1. 首先去project 创建dataset和table\n\n![png](../img/dataflow/dataflow_1/1.png)\n\n2. 然后我们去到Dataflow, create job from template\n\n![png](../img/dataflow/dataflow_1/2.png)\n\n3. 这里有几点我们要填:\n  \njob name: 作业名字\n\ncloud pub/sub input topic: 输入话题,dataflow输入数据到bigquery创建一个topic\n\n所以,我们这里去到Pub/Sub,创建topic,把topic name 放到 cloud pub/sub input topic\n![png](../img/dataflow/dataflow_1/3.png)\n\nBigquery output: Bigquery接收数据的table表,这里我们去到我们第一步创建的页面,复制表格整体路径\n\nTemporary location: 这里是创建数据临时放的地方,这个文件可以放在gs://my_bucket/tmp中, 注意,这个tmp文件夹一定要存在, 这个tmp文件夹可以给多个job存放\n\n4. 然后run job\n   \n5. run job成功后,我们先去到我们的topic中,然后选择PUBLISH MESSAGE,\n\n![png](../img/dataflow/dataflow_1/4.png)\n\n然后我们在Message body上输入我们需要输入的数据,数据格式是json格式{\"key\":\"value\",\"k\",\"v\"},然后publish\n\n![png](../img/dataflow/dataflow_1/5.png)\n\n6. 然后我们回到job,查看job detail, 我们点击writesuccessfulRecords,可以看到右边Elements added 出现了你添加了多少条,比如我添加了两条,他就显示2条\n\n![png](../img/dataflow/dataflow_1/6.png)\n\n7. 最后我们可以去bigquery查看\n   \n![png](../img/dataflow/dataflow_1/7.png)"
        },
        {
          "id": "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2",
          "metadata": {
            "permalink": "/blog/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-11-dataflow用Apache Beam python运行 Apache Beam copy 2.md",
            "source": "@site/blog/2020-4-11-dataflow用Apache Beam python运行 Apache Beam copy 2.md",
            "title": "5分钟在谷歌云上使用dataflow来运行job",
            "description": "png",
            "date": "2020-04-11T00:00:00.000Z",
            "formattedDate": "April 11, 2020",
            "tags": [
              {
                "label": "dataproc",
                "permalink": "/blog/tags/dataproc"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "Spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "Hadoop",
                "permalink": "/blog/tags/hadoop"
              }
            ],
            "readingTime": 0.5,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataflow",
              "title": "5分钟在谷歌云上使用dataflow来运行job",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataproc",
                "GCP",
                "Spark",
                "Hadoop"
              ]
            },
            "prevItem": {
              "title": "dataflow简单入门-流数据输入到bigquery",
              "permalink": "/blog/2020/4/11/dataflow流数据输入到bigquery"
            },
            "nextItem": {
              "title": "5分钟在谷歌云上创建jupyterhub",
              "permalink": "/blog/2020/4/11/jupyterhub_GCP"
            }
          },
          "content": "```\ngcloud config set project query-11\n```\n# 创建虚拟环境,并且激活\n```python\npip3 install --upgrade virtualenv --user\npython3 -m virtualenv env\nsource env/bin/activate\n```\n<!--truncate-->\n# 安装samples和apache Beam SDK\n```python\npip3 install --quiet apache-beam[gcp]\n```\n# 建立一个cloud storage bucket\n```python\ngustil mb gs://query-11\n```\n\n# 在dataflow开启pipeline\n```python\npython3 -m \\\n    apache_beam.examples.wordcount \\\n    --project query-11 --runner \\\n    DataflowRunner --temp_location \\\n    gs://query-11/temp --output \\\n    gs://query-11/results/output \\\n    --job_name dataflow-intro\n```\n\n# 我们可以去到dataflow,点击flow\n![png](../img/dataflow/1.png)\n![png](../img/dataflow/2.png)\n![png](../img/dataflow/3.png)"
        },
        {
          "id": "/2020/4/11/jupyterhub_GCP",
          "metadata": {
            "permalink": "/blog/2020/4/11/jupyterhub_GCP",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-11-jupyterhub_GCP.md",
            "source": "@site/blog/2020-4-11-jupyterhub_GCP.md",
            "title": "5分钟在谷歌云上创建jupyterhub",
            "description": "这个是常规操作,所以就不解释了",
            "date": "2020-04-11T00:00:00.000Z",
            "formattedDate": "April 11, 2020",
            "tags": [
              {
                "label": "git",
                "permalink": "/blog/tags/git"
              },
              {
                "label": "lfs",
                "permalink": "/blog/tags/lfs"
              },
              {
                "label": "github",
                "permalink": "/blog/tags/github"
              }
            ],
            "readingTime": 2.645,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "jupyterhub_GCP",
              "title": "5分钟在谷歌云上创建jupyterhub",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "git",
                "lfs",
                "github"
              ]
            },
            "prevItem": {
              "title": "5分钟在谷歌云上使用dataflow来运行job",
              "permalink": "/blog/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2"
            },
            "nextItem": {
              "title": "dataflow sql",
              "permalink": "/blog/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
            }
          },
          "content": "这个是常规操作,所以就不解释了\n![png](../img/jupyterhub/left-menu-button.png)\n![png](../img/jupyterhub/vm-instances-menu.png)\n![png](../img/jupyterhub/enable-billing.png)\n![png](../img/jupyterhub/create-vm-first.png)\n## 选择region,\n## cpu要超过1GB,\n## Boot disk启动磁盘选择ubuntu 18.04LTS\n<!--truncate-->\n![png](../img/jupyterhub/machine-type-basic.png)\n![png](../img/jupyterhub/boot-disk-button.png)\n![png](../img/jupyterhub/boot-disk-ubuntu.png)\n\n## 在identity and API access 选择No service account\n这样做可以防止你的jupyter hub users 进入其他云服务,提升安全\n![png](../img/jupyterhub/no-service-account.png)\n\n## 防火墙的选择,允许http和https\n![png](../img/jupyterhub/firewall.png)\n\n## copy 以下链接到startup script上来安装jupyterhub\n这里admin-user-name要替换成你的用户名,用来等下的登录,如\"flybird\"\n```python\n#!/bin/bash\ncurl https://raw.githubusercontent.com/jupyterhub/the-littlest-jupyterhub/master/bootstrap/bootstrap.py \\\n  | sudo python3 - \\\n    --admin <admin-user-name>\n```\n## 创建vm实例\n![png](../img/jupyterhub/create-vm-button.png)\n![png](../img/jupyterhub/vm-created.png)\n\n## 大概20分钟后,jupyterhub就创建成功,我们可以复制external ip到浏览器查看\n!! 注意, 没创建成功成, 浏览器会提示diaed tcp, conection refused, 所以不用着急\n![png](../img/jupyterhub/first-login.png)\n\n## 用之前startup script写的用户名登录\n![png](../img/jupyterhub/1.png)\n\n## 选择admin,可以创建用户\n![png](../img/jupyterhub/2.png)\n![png](../img/jupyterhub/3.png)\n![png](../img/jupyterhub/4.png)\n## 开启server,开启后,用户就可以登录了\n![png](../img/jupyterhub/5.png)\n\n## 进入终端,分别安装conda/pip安装包给所有用户\n管理员admin能够使用命令 sudo -E 对整个环境安装工具包\n```python\nsudo -E conda install -c conda-forge gdal\n```\n![png](../img/jupyterhub/6.png)\n![png](../img/jupyterhub/7.png)\n![png](../img/jupyterhub/8.png)\n![png](../img/jupyterhub/9.png)\n```python\nsudo -E pip install there\n```\n![png](../img/jupyterhub/10.png)\n\n## 测试创建文件\n![png](../img/jupyterhub/11.png)\n\n## 测试新用户登录\n![png](../img/jupyterhub/12.png)\n![png](../img/jupyterhub/13.png)\n\n# 成功啦!!!\n\n## 好啦,现在要你有了硬核了,但是要有软东西啦,package要安装,对不对啊\n比如安装 [conda, pip 或者apt package](http://tljh.jupyter.org/en/latest/install/google.html)嘛\n\n## 去到jupyter terminal,所有package都在jupyter terminal安装哦\n```\nsudo -E conda install -c conda-forge gdal\nsudo -E pip install there\npip install jupyter_contrib_nbextensions --user #用做jupyter自动提示\njupyter contrib nbextension install --user\npip install --user jupyter_nbextensions_configurator\njupyter nbextensions_configurator enable --user\n```\n[jupyter 代码提示自动补全参考链接](https://blog.csdn.net/mengfei2656/article/details/89287140)\n这里要敲黑板啦,想安装更多pip , apt packages和权限进入不再jupyter Hub用户环境的设置等,请看[官网文档](http://tljh.jupyter.org/en/latest/howto/env/user-environment.html#howto-env-user-environment)\n\n引用官网一段文字:\nAccessing user environment outside JupyterHub\nWe add /opt/tljh/user/bin to the $PATH environment variable for all JupyterHub users, so everything installed in the user environment is available to them automatically. If you are using ssh to access your server instead, you can get access to the same environment with:\n```\nexport PATH=/opt/tljh/user/bin:${PATH}\n```"
        },
        {
          "id": "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业",
          "metadata": {
            "permalink": "/blog/2020/4/11/使用Dataflow SQL界面运行Dataflow作业",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-11-使用Dataflow SQL界面运行Dataflow作业.md",
            "source": "@site/blog/2020-4-11-使用Dataflow SQL界面运行Dataflow作业.md",
            "title": "dataflow sql",
            "description": "总体流程",
            "date": "2020-04-11T00:00:00.000Z",
            "formattedDate": "April 11, 2020",
            "tags": [
              {
                "label": "dataflow",
                "permalink": "/blog/tags/dataflow"
              },
              {
                "label": "GCP",
                "permalink": "/blog/tags/gcp"
              },
              {
                "label": "apache beam",
                "permalink": "/blog/tags/apache-beam"
              },
              {
                "label": "sql",
                "permalink": "/blog/tags/sql"
              }
            ],
            "readingTime": 4.06,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dataflow3",
              "title": "dataflow sql",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "dataflow",
                "GCP",
                "apache beam",
                "sql"
              ]
            },
            "prevItem": {
              "title": "5分钟在谷歌云上创建jupyterhub",
              "permalink": "/blog/2020/4/11/jupyterhub_GCP"
            },
            "nextItem": {
              "title": "mac安装spark+jupyter+annocade+pycharm配置",
              "permalink": "/blog/2020/4/11/安装spark"
            }
          },
          "content": "[总体流程](https://www.youtube.com/watch?v=GBNBnobxsiI)\n# 选择项目 \n```\ngcloud config set project query-11\n```\n# 启用API\n启用 Cloud Dataflow, Compute Engine, Stackdriver Logging, Cloud Storage, Cloud Storage JSON, BigQuery, Cloud Pub/Sub, and Cloud Resource Manager API。\n\n具体流程可查看[Dataflow SQL 界面](https://cloud.google.com/dataflow/docs/guides/sql/dataflow-sql-ui-walkthrough)\n\n<!--truncate-->\n# 创建Pub/Sub主题和发布脚步\n\n(1)\n```\ngcloud pubsub topics create transactions\n```\n\n(2)\n创建python文件, transactions_injector.py,内容为一下\n```\n#!/usr/bin/env python\n\nimport datetime, json, os, random, time\n\n# Set the `project` variable to a Google Cloud project ID.\nproject = 'query-11'\n\nFIRST_NAMES = ['Monet', 'Julia', 'Angelique', 'Stephane', 'Allan', 'Ulrike', 'Vella', 'Melia',\n'Noel', 'Terrence', 'Leigh', 'Rubin', 'Tanja', 'Shirlene', 'Deidre', 'Dorthy', 'Leighann',\n'Mamie', 'Gabriella', 'Tanika', 'Kennith', 'Merilyn', 'Tonda', 'Adolfo', 'Von', 'Agnus',\n'Kieth', 'Lisette', 'Hui', 'Lilliana',]\nCITIES = ['Washington', 'Springfield', 'Franklin', 'Greenville', 'Bristol', 'Fairview', 'Salem',\n'Madison', 'Georgetown', 'Arlington', 'Ashland',]\nSTATES = ['MO','SC','IN','CA','IA','DE','ID','AK','NE','VA','PR','IL','ND','OK','VT','DC','CO','MS',\n'CT','ME','MN','NV','HI','MT','PA','SD','WA','NJ','NC','WV','AL','AR','FL','NM','KY','GA','MA',\n'KS','VI','MI','UT','AZ','WI','RI','NY','TN','OH','TX','AS','MD','OR','MP','LA','WY','GU','NH']\nPRODUCTS = ['Product 2', 'Product 2 XL', 'Product 3', 'Product 3 XL', 'Product 4', 'Product 4 XL', 'Product 5',\n'Product 5 XL',]\n\nwhile True:\n    first_name, last_name = random.sample(FIRST_NAMES, 2)\n    data = {\n    'tr_time_str': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    'first_name': first_name,\n    'last_name': last_name,\n    'city': random.choice(CITIES),\n    'state':random.choice(STATES),\n    'product': random.choice(PRODUCTS),\n    'amount': float(random.randrange(50000, 70000)) / 100,\n    }\n    message = json.dumps(data)\n    command = \"gcloud --project={} pubsub topics publish transactions --message='{}'\".format(project, message)\n    print(command)\n    os.system(command)\n    time.sleep(random.randrange(1, 5))\n```\n## python脚本下的PubSub的主要命令有:\n```\ngcloud pubsub topics create transactions 创建 topics\ngcloud --project=project_id pubsub topics publish transactions(topic名字) --message=(json格式的data,最后用json.dump(data))\ncommand = \"gcloud --project={} pubsub topics publish transactions --message='{}'\".format(project, message)\nos.system(command)\n```\n\n\n\n# 创建数据集和表\n```\nbq mk dataflow_sql_dataset\n```\n# 创建一个us_state_salesregions.csv文件,把以下数据复制到csv文件\n```\nstate_id,state_code,state_name,sales_region\n1,MO,Missouri,Region_1\n2,SC,South Carolina,Region_1\n3,IN,Indiana,Region_1\n6,DE,Delaware,Region_2\n15,VT,Vermont,Region_2\n16,DC,District of Columbia,Region_2\n19,CT,Connecticut,Region_2\n20,ME,Maine,Region_2\n35,PA,Pennsylvania,Region_2\n38,NJ,New Jersey,Region_2\n47,MA,Massachusetts,Region_2\n54,RI,Rhode Island,Region_2\n55,NY,New York,Region_2\n60,MD,Maryland,Region_2\n66,NH,New Hampshire,Region_2\n4,CA,California,Region_3\n8,AK,Alaska,Region_3\n37,WA,Washington,Region_3\n61,OR,Oregon,Region_3\n33,HI,Hawaii,Region_4\n59,AS,American Samoa,Region_4\n65,GU,Guam,Region_4\n5,IA,Iowa,Region_5\n32,NV,Nevada,Region_5\n11,PR,Puerto Rico,Region_6\n17,CO,Colorado,Region_6\n18,MS,Mississippi,Region_6\n41,AL,Alabama,Region_6\n42,AR,Arkansas,Region_6\n43,FL,Florida,Region_6\n44,NM,New Mexico,Region_6\n46,GA,Georgia,Region_6\n48,KS,Kansas,Region_6\n52,AZ,Arizona,Region_6\n56,TN,Tennessee,Region_6\n58,TX,Texas,Region_6\n63,LA,Louisiana,Region_6\n7,ID,Idaho,Region_7\n12,IL,Illinois,Region_7\n13,ND,North Dakota,Region_7\n31,MN,Minnesota,Region_7\n34,MT,Montana,Region_7\n36,SD,South Dakota,Region_7\n50,MI,Michigan,Region_7\n51,UT,Utah,Region_7\n64,WY,Wyoming,Region_7\n9,NE,Nebraska,Region_8\n10,VA,Virginia,Region_8\n14,OK,Oklahoma,Region_8\n39,NC,North Carolina,Region_8\n40,WV,West Virginia,Region_8\n45,KY,Kentucky,Region_8\n53,WI,Wisconsin,Region_8\n57,OH,Ohio,Region_8\n49,VI,United States Virgin Islands,Region_9\n62,MP,Commonwealth of the Northern Mariana Islands,Region_9\n```\n创建表\n```\nbq load --autodetect --source_format=CSV dataflow_sql_dataset.us_state_salesregions us_state_salesregions.csv\n```\n\n# 查找 Pub/Sub 来源\n首先把dataset设置成Gloud dataflow engine\n\n然后再点击添加数据下拉列表，然后选择 Cloud Dataflow 来源\n\n在右侧打开的添加 Cloud Dataflow 来源面板中，选择 Pub/Sub 主题。在搜索框中，搜索 transactions。 选择相应主题，然后点击添加。\n\n在Cloud dataflow sources的 cloud pub/sub topics下选择transactions >> 去到Schema >> edit schema,\n然后输入一下schema格式\n```\n[\n  {\n      \"description\": \"Pub/Sub event timestamp\",\n      \"name\": \"event_timestamp\",\n      \"mode\": \"REQUIRED\",\n      \"type\": \"TIMESTAMP\"\n  },\n  {\n      \"description\": \"Transaction time string\",\n      \"name\": \"tr_time_str\",\n      \"type\": \"STRING\"\n  },\n  {\n      \"description\": \"First name\",\n      \"name\": \"first_name\",\n      \"type\": \"STRING\"\n  },\n  {\n      \"description\": \"Last name\",\n      \"name\": \"last_name\",\n      \"type\": \"STRING\"\n  },\n  {\n      \"description\": \"City\",\n      \"name\": \"city\",\n      \"type\": \"STRING\"\n  },\n  {\n      \"description\": \"State\",\n      \"name\": \"state\",\n      \"type\": \"STRING\"\n  },\n  {\n      \"description\": \"Product\",\n      \"name\": \"product\",\n      \"type\": \"STRING\"\n  },\n  {\n      \"description\": \"Amount of transaction\",\n      \"name\": \"amount\",\n      \"type\": \"FLOAT64\"\n  }\n]\n```\n# 运行python脚本,这样我们就开始不停发送数据\n```\npython transactions_injector.py\n```\n\n# 创建SQL查询\n创建SQL查询来运行Dataflow jobs\n我们这里例子是添加一个处理PubSub发送的字段再添加一个字段\n```\nSELECT tr.*, sr.sales_region\nFROM pubsub.topic.`project-id`.transactions as tr\n  INNER JOIN bigquery.table.`project-id`.dataflow_sql_dataset.us_state_salesregions AS sr\n  ON tr.state = sr.state_code\n```\n\n# 我们设置好查询后就可以开始创建job了\n\n1 在Query editor下面,选择Create Dataflow job (这个按钮只有在设置好query后才能生效)\n\n2 点击进去后,我们填写job name, Primary ouput(选择Bigquery), Project, Dataset id和填写table name (自己命名)\n\n3 write disposition中 选择write if empty\n\n4 点击create\n\n# 这样job就开启了\n大概5分钟job应该差不多开启了,我们可以去dataflow查看job的运行情况,同时我们也可以去bigquery查看我们的table是否创立成功和数据有没有更新"
        },
        {
          "id": "/2020/4/11/安装spark",
          "metadata": {
            "permalink": "/blog/2020/4/11/安装spark",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-11-安装spark.md",
            "source": "@site/blog/2020-4-11-安装spark.md",
            "title": "mac安装spark+jupyter+annocade+pycharm配置",
            "description": "Spark的安装大多比较麻烦，而Mac安装Spark非常简单，本文分三部分内容。",
            "date": "2020-04-11T00:00:00.000Z",
            "formattedDate": "April 11, 2020",
            "tags": [
              {
                "label": "spark",
                "permalink": "/blog/tags/spark"
              },
              {
                "label": "jupyter",
                "permalink": "/blog/tags/jupyter"
              },
              {
                "label": "pycharm",
                "permalink": "/blog/tags/pycharm"
              },
              {
                "label": "java",
                "permalink": "/blog/tags/java"
              }
            ],
            "readingTime": 4.925,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "spark1",
              "title": "mac安装spark+jupyter+annocade+pycharm配置",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "spark",
                "jupyter",
                "pycharm",
                "java"
              ]
            },
            "prevItem": {
              "title": "dataflow sql",
              "permalink": "/blog/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
            },
            "nextItem": {
              "title": "tf_serving部署+遇到的问题",
              "permalink": "/blog/2020/4/05/tf_serving"
            }
          },
          "content": "Spark的安装大多比较麻烦，而Mac安装Spark非常简单，本文分三部分内容。\n1. 安装JDK\n2. 安装Spark\n3. 简单测试\n<!--truncate-->\n这里具体可以参考链接\n\n[安装JDK](https://blog.csdn.net/a595130080/article/details/53350076?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase)\n\n我的u盘有 JDK + annocada + spark,所以直接pass下载安装步骤啦(如图显示),双击点击JDK,Anaconda3安装就可以了,spark-3.0.0-preview2-bin-hadoop2.7就是spark,直接跳到测试阶段\n\n![png](../img/spark/1.png)\n\n打开终端,切换到spark的路径(spark-3.0.0-preview2-bin-hadoop2.7):\n然后输入\n```\n./sbin/start-master.sh\n```\n然后再(http://localhost:8080),就可以看到效果:\n![png](../img/spark/2.png)\n\n表示安装成功了\n\n然后在一个新的终端,进入同样的spark路径,然后输入\n```\n./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:PORT\n```\n这里的spark://IP:PORT修改成图片上的URL,如: ./bin/spark-class org.apache.spark.deploy.worker.Worker\nspark://chenbindeMacBook-Pro.local:7077\n\n这样我们就开启了一个新的worker\n\n然后我们在终端command+c就可以关掉worker\n\n最后是关掉主机,在终端输入\n```\n./sbin/stop-master.sh\n```\n这个是简单版的\n\n# 二 Anocada+jupyter+spark\n\n我们安装好spark,jdk后,我们还要安装Anocada,我的文件里面也有,直接双击安装就可以了\n\n1 我们切换到主目录\n\n2 #打开bash_profile\n\n3 设置anaconda和spark路径, 注意!!!这里spark_path路径是spark的具体路径\n\n4 使命令立刻生效\n\n```\ncd ~\nopen .bash_profile  \nexport PATH=\"/Applications/anaconda3/bin:$PATH\"\nexport SPARK_PATH=\"/Users/flybird/Desktop/spark/spark-3.0.0-preview2-bin-hadoop2.7\"\nexport PATH=$SPARK_PATH/bin:$PATH\nsource .bash_profile\n```\n\n## 安装pyspark,这一步很重要哦\n```\nsudo pip install pyspark -i https://pypi.douban.com/simple/\n```\n\n## 在Jupyter Notebook里运行PySpark, 配置PySpark driver\n详细教程可以看[这里url链接](https://blog.csdn.net/a1272899331/article/details/90081945?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase)\n配置PySpark driver，当运行pyspark命令就直接自动打开一个Jupyter Notebook，此时shell端不会打开,具体配置步骤:\n```\nsudo vim ~/.bashrc 在这个文件添加配置PySpark driver的环境变量\nexport PYSPARK_DRIVER_PYTHON=jupyter\nexport PYSPARK_DRIVER_PYTHON_OPTS='notebook'\nsource ~/.bashrc\n```\n然后重启terminal\n\n## 启动jupyter notebook3\n\n启动后,我们在jupyter notebook上创建python文件\n然后输入以下命令\n![png](../img/spark/3.png)\n```\nimport os\nimport sys\nspark_path = os.environ.get('SPARK_PATH', None)\nsys.path.insert(0, os.path.join(spark_path, 'python/lib/py4j-0.10.8.1-src.zip'))\nexec(open(os.path.join(spark_path, 'python/pyspark/shell.py')).read())\n```\n![png](../img/spark/4.png)\n\npy4j-0.10.8.1-src.zip需要根据实际名称修改,如果是用我的包,就不用改,如果是用spark官网下载的,就需要对应的zip文件名字\n\n输入命令后的效果如下, 然后输入sc,查看相应的输出:\n![png](../img/spark/7.png)\n\n可以用command来试试一下命令\n```\nimport pyspark\n \nimport random\nsc = pyspark.SparkContext(appName=\"Pi\")\nnum_samples = 100000000\ndef inside(p):     \n  x, y = random.random(), random.random()\n  return x*x + y*y < 1\ncount = sc.parallelize(range(0, num_samples)).filter(inside).count()\npi = 4 * count / num_samples\nprint(pi)\nsc.stop()\n```\n\n如果发现出现\n```\nUnsupported class file major version 57\n```\n就是jdk版本问题,去到终端\n```\njava -version #查看版本 java version \"13.0.2\" 2020-01-14\ncd /Library/Java/JavaVirtualMachines\nls\nsudo rm -rf jdk-13.0.2.jdk #删除java version\n```\n安装JDK（jdk-8u251），[下载jdk-8u251-macosx-x64.dmg](https://www.oracle.com/java/technologies/javase-jdk8-downloads.html)\n\n然后再回到jupyter notebook,重新输入命令,查看是否成功\n![png](../img/spark/8.png)\n\n\n# Pycharm设置啦\nPycharm这里具体可以[参考一下文章](https://www.jianshu.com/p/22426c490066)\n配置原因：在pyspark命令行 练习比较麻烦，不能自动补全，浪费时间。Jupyter notebook 是最理想的，但是还没配置成功。\n\n1.打开pycharm，新建一个工程\n\n2.点击 run --Edit Configuration..\n![png](../img/spark/9.png)\n\n3.配置\n\n3.1 新建 Python ，起个名\n\n3.2 配置script，指向你要引用 spark 的那个文件\n\n3.3 Enviroment variables：\n![png](../img/spark/10.png)\n可以直接在Enviroment variables上输入:\nPYTHONUNBUFFERED=1;SPARK_HOME=/Users/flybird/Desktop/spark/spark-3.0.0-preview2-bin-hadoop2.7;PYTHONPATH=/Users/flybird/Desktop/spark/spark-3.0.0-preview2-bin-hadoop2.7/python;PYSPARK_PYTHON=/Users/flybird/opt/anaconda3/bin/python3\n\nPYSPARK_PYTHON; 指向 你本机 的 python 路径, (可以去终端输入which python 来找到路径)\n\n\nSPARK_HOME ：指向 spark 安装目录(就是spark-3.0.0-preview2-bin-hadoop2.7的绝对路径)\n\nPYTHONPATH：指向 spark 安装目录的 Python 文件夹(就是spark-3.0.0-preview2-bin-hadoop2.7的python文件夹的绝对路径)\n\n4 安装 py4j\nsudo pip3 install py4j\n\n5.看到网上很多教程，一般都只执行到第四步即可，但是我仍然无法导入 pyspark 包，还需要下面的步骤：\n\n选择 File--->setting--->你的project--->project structure\n右上角Add content root添加：py4j-some-version.zip和pyspark.zip的路径（这两个文件都在Spark中的python文件夹下）\n![png](../img/spark/10.png)\n\n这里我们可以去到spark中的python文件夹(spark-3.0.0-preview2-bin-hadoop2.7/python),然后查找zip!,然后看到选择添加就可以了\n\n6 测试程序\n用之前的代码再次测试,一样可以了"
        },
        {
          "id": "/2020/4/05/tf_serving",
          "metadata": {
            "permalink": "/blog/2020/4/05/tf_serving",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-05-tf_serving.md",
            "source": "@site/blog/2020-4-05-tf_serving.md",
            "title": "tf_serving部署+遇到的问题",
            "description": "参考过的博客",
            "date": "2020-04-05T00:00:00.000Z",
            "formattedDate": "April 5, 2020",
            "tags": [
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              },
              {
                "label": "tf_serving",
                "permalink": "/blog/tags/tf-serving"
              },
              {
                "label": "model",
                "permalink": "/blog/tags/model"
              }
            ],
            "readingTime": 11.09,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "tf_serving",
              "title": "tf_serving部署+遇到的问题",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "docker",
                "tf_serving",
                "model"
              ]
            },
            "prevItem": {
              "title": "mac安装spark+jupyter+annocade+pycharm配置",
              "permalink": "/blog/2020/4/11/安装spark"
            },
            "nextItem": {
              "title": "使用webscrapper爬取信息",
              "permalink": "/blog/2020/4/04/webscrapper"
            }
          },
          "content": "[参考过的博客](https://blog.csdn.net/u011734144/article/details/82107610?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1)\n\n[Mac下docker保存路径](https://www.cnblogs.com/zhzhlong/p/9465571.html)\n\n<!--truncate-->\n# tf_serving 部署\n## Step1: 模型构建与训练\n我们通过模型构建与训练后,我们可以得到模型(cnn_model.h5)\n\n### 模型结构打印\n\n```python\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes=True, show_layer_names=True)\n```\n\n![png](../img/tf_serving/output_11_0.png)\n\n\n\n## Step2: 模型的导出与检查\n\n### 模型导出\n\n\n```python\nimport tensorflow as tf\nimport shutil \n\nmodel = tf.keras.models.load_model('./cnn_model.h5')\n\n#  指定路径\nif os.path.exists('./Models/CNN/1'):\n     shutil.rmtree('./Models/CNN/1')\nexport_path = './Models/CNN/1'\n\n# 导出tensorflow模型以便部署\ntf.saved_model.save(model,export_path)\n```\n\n![png](../img/tf_serving/5.png) \n\n\n\n### 模型的部署前检查与测试\ntensorflow官方提供了非常好的一些工具给大家，比如在实际部署服务之前，我想强调一下TensorFlow的SavedModel命令行工具，这对于快速检查我们模型的输入和输出规范很有用，我们用下面的命令检查一下我们的CNN模型：\n```shell\n$ saved_model_cli show --dir ./Models/CNN/1 --all\n```\n<!--truncate-->\n我们会看到以下输出信息:\n```shell\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['input_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 100)\n        name: serving_default_input_1:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['dense'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 5)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\n```\n\n我们还可以随机送一些符合输入维度要求的数据给模型，看看输出结果的形态。\n```shell\n$ saved_model_cli run --dir ./Models/CNN/1 --tag_set serve --signature_def serving_default --input_exp 'input_1=np.random.rand(1,100)'\n```\n<!--truncate-->\n大家将看到以下输出：\n```shell\nserve --signature_def serving_default --input_exp 'input_1=np.random.rand(1,100)'\n2019-06-13 16:33:46.095550: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nWARNING: Logging before flag parsing goes to stderr.\nW0613 16:33:46.095993 140736529130432 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py:339: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\nW0613 16:33:46.141968 140736529130432 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\nResult for output key dense:\n[[0.14112209 0.22561029 0.2605021  0.22022876 0.15253687]]\n```\n\n这表明我们随机输入长度为100的文本序列(词id)给模型，得到了长度为5的概率向量，分别表示5个类别的概率。到此为止，我们的模型一切正常。\n\n# 模型部署与服务构建介绍\n模型部署部分，强大的google生态下，已经有完整的部署方案，部署工具可以直接使用google的tensorflow-model-server。\n\n### tensorflow-model-server安装(最简单就是用docker来用tf serving)\n我是使用docker来安装,方便简单，\n\n如果是Ubuntu环境可以使用apt-get install安装，其他环境可以从源码编译，具体的内容大家可以参考：\n- [tensorflow官网安装指南](https://www.tensorflow.org/tfx/serving/setup)\n- [Ubuntu安装指南](https://blog.51cto.com/aaronsa/2284396)\n  \n### 关于tensorflow serving介绍\n大家习惯使用TensorFlow进行模型的训练、验证和预测，但模型完善之后的生产上线流程，就变得五花八门了。针对这种情况Google提供了TensorFlow Servering，可以将训练好的模型直接上线并提供服务。早在2017年的TensorFlow开发者Summit上就提出了TensorFlow Serving。\n 但那时候客户端和服务端的通信只支持gRPC。在实际的生产环境中比较广泛使用的C/S通信手段是基于RESTfull API的，幸运的是从TF1.8以后，TF Serving也正式支持RESTfull API通信方式了。\n\n#### 服务框架\n![img](../img/tf_serving/tf1.png)\n\nTF Serving服务框架\n\n基于TF Serving的持续集成框架还是挺简明的，基本分三个步骤：\n\n- 模型训练\n   这是大家最熟悉的部分，主要包括数据的收集和清洗、模型的训练、评测和优化；\n- 模型上线\n   前一个步骤训练好的模型在TF Server中上线；\n- 服务使用\n   客户端通过gRPC和RESTfull API两种方式同TF Servering端进行通信，并获取服务；\n\n#### TF Serving工作流程\n![img](../img/tf_serving/tf2.png)\n\nTF Serving工作流程\n\nTF Serving的工作流程主要分为以下几个步骤：\n\n- Source会针对需要进行加载的模型创建一个Loader，Loader中会包含要加载模型的全部信息；\n- Source通知Manager有新的模型需要进行加载；\n- Manager通过版本管理策略（Version Policy）来确定哪些模型需要被下架，哪些模型需要被加载；\n- Manger在确认需要加载的模型符合加载策略，便通知Loader来加载最新的模型；\n- 客户端像服务端请求模型结果时，可以指定模型的版本，也可以使用最新模型的结果；\n\n#### 简单示例\n\nTF Serving客户端和服务端的通信方式有两种（gRPC和RESTfull API）\n\n##### 示例（一）：RESTfull API形式\n\n- **1. 准备TF Serving的Docker环境**\n\n目前TF Serving有Docker、APT（二级制安装）和源码编译三种方式，但考虑实际的生产环境项目部署和简单性，推荐使用Docker方式。\n\n```shell\ndocker pull tensorflow/serving\n```\n\n- **2. 下载官方示例代码**\n\n示例代码中包含已训练好的模型和与服务端进行通信的客户端（RESTfull API形式不需要专门的客户端）\n\n```shell\nmkdir -p /tmp/tfserving\ncd /tmp/tfserving\ngit clone https://github.com/tensorflow/serving\n```\n\n- **3. 运行TF Serving**\n\n```shell\ndocker run -p 8501:8501 \\\n  --mount type=bind,\\\n   source=/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu,\\\ntarget=/models/half_plus_two \\\n-e MODEL_NAME=half_plus_two -t tensorflow/serving &\n```\n\n这里需要注意的是，较早的docker版本没有“--mount”选项，比如Ubuntu16.04默认安装的docker就没有（我的环境是Ubuntu 18.04）。\n\n- **4.客户端验证**\n\n```shell\ncurl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n  -X POST http://localhost:8501/v1/models/half_plus_two:predict \n```\n\n返回结果，\n\n```shell\n{ \"predictions\": [2.5, 3.0, 4.5] }\n```\n\n##### 示例（二）：gRPC形式\n\n- **1. 准备TF Serving的Docker环境**\n\n目前TF Serving有Docker、APT（二级制安装）和源码编译三种方式，但考虑实际的生产环境项目部署和简单性，推荐使用Docker方式。\n\n```shell\ndocker pull tensorflow/serving\n```\n\n- **2. 下载官方示例代码**\n\n```shell\nmkdir -p /tmp/tfserving\ncd /tmp/tfserving\ngit clone https://github.com/tensorflow/serving\n```\n\n- **3. 模型编译**\n\n```shell\npython tensorflow_serving/example/mnist_saved_model.py models/mnist\n```\n\n- **4. 运行TF Serving**\n\n```shell\ndocker run -p 8500:8500 \\\n--mount type=bind,source=$(pwd)/models/mnist,target=/models/mnist \\\n-e MODEL_NAME=mnist -t tensorflow/serving\n```\n\n这里需要注意的是，较早的docker版本没有“--mount”选项，比如Ubuntu16.04默认安装的docker就没有（这里的环境是Ubuntu 18.04）。\n\n- **5.客户端验证**\n\n```shell\npython tensorflow_serving/example/mnist_client.py --num_tests=1000 --server=127.0.0.1:8500\n```\n\n返回结果，\n\n```shell\nInference error rate: 11.13%\n```\n\n这里需要注意的是，直接运行mnist_client.py会出现找不到“tensorflow_serving”的问题，需要手动安装，\n\n```shell\npip install tensorflow-serving-api\n```\n\n#### 资料参考\nTF Serving官方文档：https://www.tensorflow.org/tfx/guide/serving\n个人的项目是使用RESTfull API形式,感觉简单很多,不需要gRpc形式那样创建客户端和模型编译\n\n## Step3: 具体实操, 安装Docker版的tensorflow/serving\n目前TF Serving有Docker、APT（二级制安装）和源码编译三种方式，我使用Docker方式。\n```python\ndocker pull tensorflow/serving\n```\n\n## Step4:模型部署\n### 获取当前绝对路径\n\n```python\nimport os\n# 获取当前绝对路径\nMODEL_DIR = os.getcwd()+\"/Models/RNN\" #指定绝对路径\nos.environ[\"MODEL_DIR\"] = MODEL_DIR #设置全局变量\nprint(MODEL_DIR)  打印绝对路径\n\n```\n\n\n    /Volumes/Untitled/NLP项目/docker部署/flask_news_classifier/Models/RNN\n\n\n## Step5: 用Docker启动tf serving\n\n### source这里一定要写绝对路径\n\n###  target这里要写/models/自定义名字(比如rnn_serving)\n\n### MODEL_NAME 和target的自定义名字一样\n\n\n```bash\n%%bash --bg ##后台终端运行\ndocker run -p 8501:8501 --mount type=bind,source=\"${MODEL_DIR}\",target=/models/rnn_serving -e MODEL_NAME=rnn_serving -t tensorflow/serving & >server.log 2>&1\n\n# 写日记\n! tail server.log\n\n# 安装 requests包,用来接收请求\n\n! pip install -q requests\n\n```\n\n## Step6: 检测模型是否部署到docker上,进入终端\n### 首先输入一下命令,查看运行的容器名字或者id\n```\ndocker ps\n```\n![png](../img/tf_serving/2.png)\n\n### 然后进入运行的容器,查看容器的models\n```\ndocker exec -it 容器名字或者容器id /bin/bash\ndocker exec -it wonderful_meitner /bin/bash\n```\n```\ncd models\nls\n```\n![png](../img/tf_serving/3.png)\n\n## 我们看到我们里面有文件夹rnn_serving,这就是我们之前step2tensorflow模型导出的文件\n\n![png](../img/tf_serving/4.png)\n\n\n## 测试数据预处理\n\n```python\nfrom tensorflow.keras.preprocessing import sequence\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom utils import *\nimport json\nimport numpy\nimport requests\nimport jieba\n\n# 路径等配置\ndata_dir = \"./processed_data\"\nvocab_file = \"./vocab/vocab.txt\"\nvocab_size = 40000\n\n# 神经网络配置\nmax_features = 40001\nmaxlen = 100\nbatch_size = 256\nembedding_dims = 50\nepochs = 8\n\nprint('数据预处理与加载数据...')\n# 如果不存在词汇表，重建\nif not os.path.exists(vocab_file):  \n    build_vocab(data_dir, vocab_file, vocab_size)\n# 获得 词汇/类别 与id映射字典\ncategories, cat_to_id = read_category()\nwords, word_to_id = read_vocab(vocab_file)\n```\n\n    数据预处理与加载数据...\n\n\n###  字典数据转换成json格式,使用request请求\n\n\n```python\ntext = \"杨幂好漂亮,发生原子弹\"\nprint(jieba.lcut(text))\ntext_seg = encode_sentences([jieba.lcut(text)], word_to_id)\ntext_input = sequence.pad_sequences(text_seg, maxlen=maxlen)\n\ndata = json.dumps({\"signature_name\": \"serving_default\",\n                   \"instances\": text_input.reshape(1,100).tolist()})\nheaders = {\"content-type\": \"application/json\"}\njson_response = requests.post('http://localhost:8501/v1/models/rnn_serving:predict',\n                              data=data, headers=headers)\n```\n\n    ['杨幂', '好', '漂亮', ',', '发生', '原子弹']\n\n\n### 呈现出测试数据的类别\n\n\n```python\nprint(json.loads(json_response.text))\n# print(json_response.text.split(':')[1].strip()[2:-9])\n# print(json_response.text.split(':')[1].strip()[2:-9].split(','))\nproba = json_response.text.split(':')[1].strip()[2:-9].split(',')\nproba\nproba = [float(i) for i in proba]\nprint(proba)\n\nimport numpy as np\n#\nnews_dict = {'0': 'car', '1':'entertainment', '2':'military', '3':'sports', '4':'technology'}\nprint('News Type:',news_dict[str(np.argmax(proba))])\n```\n\n    {'predictions': [[0.00735603366, 0.974295, 0.00240160106, 0.00155786274, 0.0143894823]]}\n    [0.00735603366, 0.974295, 0.00240160106, 0.00155786274, 0.0143894823]\n    News Type: entertainment\n\n# 成功部署自己的模型到tf_serving啦\n## 最重要是step5!!!step5!!!step5!!!"
        },
        {
          "id": "/2020/4/04/webscrapper",
          "metadata": {
            "permalink": "/blog/2020/4/04/webscrapper",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-04-webscrapper.md",
            "source": "@site/blog/2020-4-04-webscrapper.md",
            "title": "使用webscrapper爬取信息",
            "description": "step 1: 比如在一个网站搜索纸尿裤, 我们在google chrome的更多工具中点击开发者工具",
            "date": "2020-04-04T00:00:00.000Z",
            "formattedDate": "April 4, 2020",
            "tags": [
              {
                "label": "web",
                "permalink": "/blog/tags/web"
              },
              {
                "label": "scrapper",
                "permalink": "/blog/tags/scrapper"
              },
              {
                "label": "shopping",
                "permalink": "/blog/tags/shopping"
              }
            ],
            "readingTime": 1.3,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "webscrapper",
              "title": "使用webscrapper爬取信息",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "web",
                "scrapper",
                "shopping"
              ]
            },
            "prevItem": {
              "title": "tf_serving部署+遇到的问题",
              "permalink": "/blog/2020/4/05/tf_serving"
            },
            "nextItem": {
              "title": "用faker模拟数据+随机种子",
              "permalink": "/blog/2020/4/02/faker"
            }
          },
          "content": "<!--truncate-->\n## step 1: 比如在一个网站搜索纸尿裤, 我们在google chrome的更多工具中点击开发者工具\n\n![png](../img/scrapper/1.png)\n\n## step 2: \n### 1 .选择web scrapper\n### 2 .选择create new sitemap\n### 3 .选择select, 然后在商品页面选择能够覆盖商品所有信息的位置,然后点击done selecting, 选择mulitple\n\n## step3: \ndone selecting后,你发现selector第四个字段是link_54224078139(这个是特点商品的编号),我们去除它,使他泛化,变成div.search_prolist_info\n\n\n![png](../img/scrapper/2.png)\n\n## step4: 点击save selector,用element preview查看效果是否所有页面都包含\n![png](../img/scrapper/3.png)\n\n\n## step5: 点击item,然后创建新的selector\n流程基本一样,但是不选择multiple,然后评论字段的regex写[0-9]+\n\n## step6: 保存selector\n\n### 如果要怕所有页,可以去Sitemap下选择edit metadata来设置\n\n![png](../img/scrapper/4.png)\n\n## step7: 回到root,然后选择sitemap的下拉菜单的Scrape\n![png](../img/scrapper/5.png)\n\n## step8: 点击start scraping\n![png](../img/scrapper/6.png)\n\n## step9: Sitemap选择browse,可以查看所有data\n![png](../img/scrapper/7.png)\n\n## 选择Sitemap,导出csv文件"
        },
        {
          "id": "/2020/4/02/faker",
          "metadata": {
            "permalink": "/blog/2020/4/02/faker",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-02-faker.md",
            "source": "@site/blog/2020-4-02-faker.md",
            "title": "用faker模拟数据+随机种子",
            "description": "Pretty printing has been turned OFF",
            "date": "2020-04-02T00:00:00.000Z",
            "formattedDate": "April 2, 2020",
            "tags": [
              {
                "label": "faker",
                "permalink": "/blog/tags/faker"
              },
              {
                "label": "汇丰",
                "permalink": "/blog/tags/汇丰"
              },
              {
                "label": "random seed",
                "permalink": "/blog/tags/random-seed"
              }
            ],
            "readingTime": 0.895,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "faker",
              "title": "用faker模拟数据+随机种子",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "faker",
                "汇丰",
                "random seed"
              ]
            },
            "prevItem": {
              "title": "使用webscrapper爬取信息",
              "permalink": "/blog/2020/4/04/webscrapper"
            },
            "nextItem": {
              "title": "轻松上传超过100M的文件至GitHub",
              "permalink": "/blog/2020/4/02/git_big_file copy"
            }
          },
          "content": "<!--truncate-->\n\n```python\n%config ZMQInteractiveShell.ast_node_interactivity='all' \n%pprint\n```\n\n    Pretty printing has been turned OFF\n\n\n## 导入工具包\n\n\n```python\nimport pandas as pd\nfrom faker import Faker\n```\n\n## 创建实例,设置本地化\n\n```python\nfake = Faker('zh_CN')\n```\n\n## 创建模拟个人数据函数,设定随机种子\n\n\n```python\ndef generate_user(i):\n    fake.random.seed(i)\n    return dict(name=fake.name(),password=fake.password(length=10),company=fake.company(),job=fake.job(),birthday=fake.date_of_birth(minimum_age=0, maximum_age=120),\n                telephone=fake.phone_number(),\n                address=fake.address())\n```\n\n## 生成客户模拟数据列表, 用dataframe呈现\n\n```python\nusers = []\nfor i in range(5):\n    users.append(generate_user(i))\ndf = pd.DataFrame(users)\ndf\n```\n![png](../img/faker/1.png)\n\n### 验证随机种子设定是否成功\n```python\nusers = []\nfor i in range(8):\n    users.append(generate_user(i))\ndf = pd.DataFrame(users)\ndf\n```\n![png](../img/faker/2.png)\n\n## 导出数据,生成csv文件\n\n```python\ndf.to_csv('./fakedata.csv')\n```\n# 查看博客资料\n\n## 资料索引\n\n1.  [用faker模拟数据](https://blog.csdn.net/u011054333/article/details/84203878)\n\n2.  [faker官网](https://faker.readthedocs.io/en/master/locales/zh_CN.html)\n\n3.  [标准provides](https://faker.readthedocs.io/en/master/providers.html)\n\n4.  [第三方provides](https://faker.readthedocs.io/en/master/communityproviders.html)\n\n\n```python\n\n```"
        },
        {
          "id": "/2020/4/02/git_big_file copy",
          "metadata": {
            "permalink": "/blog/2020/4/02/git_big_file copy",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-02-git_big_file copy.md",
            "source": "@site/blog/2020-4-02-git_big_file copy.md",
            "title": "轻松上传超过100M的文件至GitHub",
            "description": "GitHub有一个限制，不能上传超过100M的文件。想要上传超过100M的文件，就需要借助Git LFS",
            "date": "2020-04-02T00:00:00.000Z",
            "formattedDate": "April 2, 2020",
            "tags": [
              {
                "label": "git",
                "permalink": "/blog/tags/git"
              },
              {
                "label": "lfs",
                "permalink": "/blog/tags/lfs"
              },
              {
                "label": "github",
                "permalink": "/blog/tags/github"
              }
            ],
            "readingTime": 1.195,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "git_big_file",
              "title": "轻松上传超过100M的文件至GitHub",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "git",
                "lfs",
                "github"
              ]
            },
            "prevItem": {
              "title": "用faker模拟数据+随机种子",
              "permalink": "/blog/2020/4/02/faker"
            },
            "nextItem": {
              "title": "快速按照工具包",
              "permalink": "/blog/2020/3/31/douban"
            }
          },
          "content": "GitHub有一个限制，不能上传超过100M的文件。想要上传超过100M的文件，就需要借助Git LFS\n<!--truncate-->\n\n## step1: 安装LFS,执行命令\n```python\nbrew install git-lfs\n```\n\n## step2: 进入仓库目录,执行命令\n\n```python\ngit lfs track \"file\"\ngit add .gitattributes\ngit commit -m \"submit file\"\ngit push -u origin master \n```\nfile是要上传的文件,一般执行完step2后,会生成\".gitattributes\"文件，文件内记录了我们要上传文件的信息。只有先把\".gitattributes\"传上去，才可以上传大文件。\n\n## step3: 上传大文件\n\n```python\ngit add file\ngit commit -m \"add file\"\ngit push -u origin master\n```\n## 实战demo如下:\n### 切换到仓库目录,是git status查看状态\n![png](../img/git/1.png)\n### 执行刚才的所有命令\n![png](../img/git/2.png)\n我们发现上传失败,是因为connection失败了\n\n### 我们尝试设置全球变量\n```python\ngit config --global user.name\"name\"\n```\n![png](../img/git/3.png)\n\n### 重新执行push命令,最后上传成功啦\n```python\ngit push -u origin master\n```"
        },
        {
          "id": "/2020/3/31/douban",
          "metadata": {
            "permalink": "/blog/2020/3/31/douban",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-31-douban.md",
            "source": "@site/blog/2020-3-31-douban.md",
            "title": "快速按照工具包",
            "description": "安装google-cloud-bigquery and google-cloud-bigquery-storage packages.",
            "date": "2020-03-31T00:00:00.000Z",
            "formattedDate": "March 31, 2020",
            "tags": [
              {
                "label": "python",
                "permalink": "/blog/tags/python"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 0.29,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "douban",
              "title": "快速按照工具包",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "python",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "轻松上传超过100M的文件至GitHub",
              "permalink": "/blog/2020/4/02/git_big_file copy"
            },
            "nextItem": {
              "title": "Install the BigQuery Python client library version 1.9.0 or higher and the BigQuery Storage API Python client library.",
              "permalink": "/blog/2020/3/30/BigQuery Storage API"
            }
          },
          "content": "安装google-cloud-bigquery and google-cloud-bigquery-storage packages.\n需要认证\n<!--truncate-->\n## step1 安装包很多时候很慢比如执行一下命令\n``` \npip install --upgrade google-cloud-bigquery[bqstorage,pandas] \n```\n## step2 在命令后面加上\n``` \npip install --upgrade google-cloud-bigquery[bqstorage,pandas] -i https://pypi.douban.com/simple\n```\n\n下载瞬间快很多"
        },
        {
          "id": "/2020/3/30/BigQuery Storage API",
          "metadata": {
            "permalink": "/blog/2020/3/30/BigQuery Storage API",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-30-BigQuery Storage API.md",
            "source": "@site/blog/2020-3-30-BigQuery Storage API.md",
            "title": "Install the BigQuery Python client library version 1.9.0 or higher and the BigQuery Storage API Python client library.",
            "description": "安装google-cloud-bigquery and google-cloud-bigquery-storage packages.",
            "date": "2020-03-30T00:00:00.000Z",
            "formattedDate": "March 30, 2020",
            "tags": [
              {
                "label": "python",
                "permalink": "/blog/tags/python"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 0.42,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "BigQueryStorageAPI",
              "title": "Install the BigQuery Python client library version 1.9.0 or higher and the BigQuery Storage API Python client library.",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "python",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "快速按照工具包",
              "permalink": "/blog/2020/3/31/douban"
            },
            "nextItem": {
              "title": "本地连接google cloud shell",
              "permalink": "/blog/2020/3/29/ssh_gcs"
            }
          },
          "content": "安装google-cloud-bigquery and google-cloud-bigquery-storage packages.\n需要认证\n<!--truncate-->\n## step1 一般方法是复制以下命令在mac os终端执行\n``` \npip install --upgrade google-cloud-bigquery[bqstorage,pandas]\n```\n## step2 我们反向step1操作,往往不成功会出现2\n![png](../img/BQ_API/2.png)\n\n## step3 我们在命令行后面加上--user\n``` \npip install --upgrade google-cloud-bigquery[bqstorage,pandas] --user\n```\n![png](../img/BQ_API/1.png)\n\n通过操作就成功了"
        },
        {
          "id": "/2020/3/29/ssh_gcs",
          "metadata": {
            "permalink": "/blog/2020/3/29/ssh_gcs",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-29-ssh_gcs.md",
            "source": "@site/blog/2020-3-29-ssh_gcs.md",
            "title": "本地连接google cloud shell",
            "description": "google cloud shell 经常断线,所以我们可以尝试用本地ssh连接google cloud shell",
            "date": "2020-03-29T00:00:00.000Z",
            "formattedDate": "March 29, 2020",
            "tags": [
              {
                "label": "python",
                "permalink": "/blog/tags/python"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 1.025,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "ssh_gcs",
              "title": "本地连接google cloud shell",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "python",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "Install the BigQuery Python client library version 1.9.0 or higher and the BigQuery Storage API Python client library.",
              "permalink": "/blog/2020/3/30/BigQuery Storage API"
            },
            "nextItem": {
              "title": "App Engine 使用快速入门",
              "permalink": "/blog/2020/3/28/App Engine"
            }
          },
          "content": "google cloud shell 经常断线,所以我们可以尝试用本地ssh连接google cloud shell\n## step1 \n``` \ngcloud alpha cloud-shell ssh\n```\n<!--truncate-->\n![png](../img/ssh/1.png)\n\n## step2 按照系统要求按照密钥\n![png](../img/ssh/2.png)\n\n## step3 连接成功\n![png](../img/ssh/3.png)\n\n## step4 如果连接失败,可能的原因是gcloud的设置\n1 我们可以重新安装SDK\n2 对gcloud 初始化, 重新设置project, account\n3 重新走step1\n\n具体:\n```\n./google-cloud-sdk/install.sh #重新安装SKD后,重新开启终端\n```\n\n```\ngcloud init 初始化 SDK\n```\n```\nTo continue, you must log in. Would you like to log in (Y/n)? Y\n\nPick cloud project to use:\n     [1] [my-project-1]\n     [2] [my-project-2]\n     ...\n     Please enter your numeric choice:\n\nWhich compute zone would you like to use as project default?\n     [1] [asia-east1-a]\n     [2] [asia-east1-b]\n     ...\n     [14] Do not use default zone\n     Please enter your numeric choice:\n\ngcloud has now been configured!\n    You can use [gcloud config] to change more gcloud settings.\n\n    Your active configuration is: [default]\n\ngcloud config set accessibility/screen_reader true\n```"
        },
        {
          "id": "/2020/3/28/App Engine",
          "metadata": {
            "permalink": "/blog/2020/3/28/App Engine",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-28-App Engine.md",
            "source": "@site/blog/2020-3-28-App Engine.md",
            "title": "App Engine 使用快速入门",
            "description": "step1: 创建自己的独立项目",
            "date": "2020-03-28T00:00:00.000Z",
            "formattedDate": "March 28, 2020",
            "tags": [
              {
                "label": "python",
                "permalink": "/blog/tags/python"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 0.74,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "App",
              "title": "App Engine 使用快速入门",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "python",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "本地连接google cloud shell",
              "permalink": "/blog/2020/3/29/ssh_gcs"
            },
            "nextItem": {
              "title": "mysql基本命令",
              "permalink": "/blog/2020/3/28/mysql"
            }
          },
          "content": "## step1: 创建自己的独立项目\n\n![png](../img/appengine/1.png)\n<!--truncate-->\n## step2: 使用App Engine\n![png](../img/appengine/2.png)\n\n## step3: 使用App Engine的两个重要命令\n```\ngcloud init 初始化SDK\ngcloud app deploy 部署到App Engine\n```\n![png](../img/appengine/3.png)\n\n## step4: 按照需要, 开启API \n![png](../img/appengine/4.png)\n\n## step5:本地终端操作登录账号,设置默认项目\n![png](../img/appengine/5.png)\n\n![png](../img/appengine/6.png)\n\n![png](../img/appengine/7.png)\n\n## step6: 下载项目/或者本地项目\n![png](../img/appengine/8.png)\n\n## step7: 使用step3的命令部署, 点击得到的链接\n```\ngcloud app deploy 部署\ngcloud app browse 查看效果\n```\n![png](../img/appengine/13.png)\n\n![png](../img/appengine/9.png)\n\n## 备注: 部署到app engine的几个重要文件\n1 app.yaml\n\n2 requirements.text\n\n3 main.py\n\n\n![png](../img/appengine/10.png)\n![png](../img/appengine/11.png)\n![png](../img/appengine/12.png)"
        },
        {
          "id": "/2020/3/28/mysql",
          "metadata": {
            "permalink": "/blog/2020/3/28/mysql",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-28-mysql.md",
            "source": "@site/blog/2020-3-28-mysql.md",
            "title": "mysql基本命令",
            "description": "数据库操作",
            "date": "2020-03-28T00:00:00.000Z",
            "formattedDate": "March 28, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 6.61,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "mysql",
              "title": "mysql基本命令",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "facebook",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "App Engine 使用快速入门",
              "permalink": "/blog/2020/3/28/App Engine"
            },
            "nextItem": {
              "title": "python脚本基础(1)-python中os的常用方法",
              "permalink": "/blog/2020/3/28/python"
            }
          },
          "content": "数据库操作\n进入数据库 \n\n方式1 mysql -u用户名 -p 密码 --------直接输入密码，缺点，会暴露自己的密码哦😝\n\n方式2 mysql -u用户名 -p 回车后输入密码\n\n![png](../img/mysql/1.png)　　\n<!--truncate-->\n　　　　\n主要内容：查询\n1、查询当前所有的数据库 show databases;\n\n![png](../img/mysql/2.png)\n\n2、使用数据库 use 数据库名字;\n\n3、查看当前数据库版本 select version();\n\n![png](../img/mysql/3.png)\n\n4、 创建数据库 create database 数据库名；\n　create database 数据库名 charset = utf8；\n![png](../img/mysql/4.png)\n\n5、查看当前使用数据库 select database();\n\n![png](../img/mysql/5.png)\n\n6 查看创建数据库 show create database 数据库名称;\n\n![png](../img/mysql/6.png)\n\n7、删除数据库 drop database 数据库名称;\n\n![png](../img/mysql/7.png)\n \n数据表的操作\n\n1、查看当前数据库所有表 show tables;\n2、创建表 creat table 表名（字段 类型 约束[字段 类型 约束]);\n3、查看表结构 desc 表名；\n4、查看创建表的语句 show create table 表名；\n5、查看表数据 select * from 表名；\n6、添加表的字段 alter table 表名 add 字段名 类型;\n7、修改表结构字段类型 alter table 表名 modify 字段 目标类型\n8、修改表的字段重命名版 alter table 表名 chang 字段原名 字段新名 目标类型 约束;\n9、删除表字段 alter table 表名 drop 字段;\n10、删除表 drop table 表名;\n\n \n \n表内数据操作\n\n增：\n\n1、添加数据 ---插入数据 insert into 表名 values(需要插入的内容)；\n可以分条插入，可以一次插入多条，每一条都是完整()\n2、添加数据 ----部分插入 insert into 表名(字段1，字段2...) values (值1，值2,...)\n可以分条插入，可以一次插入多条，每一条都是完整()\n改：\n1、改表里某字段里的值 update 表名 set 字段(列) = 值 (有时候报错要加\"\") 默认改所有人本字段的内容\nupdate 表名 set 字段 = 值 where 条件 改符合条件的字段内容\n2、update 和 alter 的区别 alter 是修改表结构(添加字段，删字段，修改字段名字) update 修改表里的数据\n \n查：\n\n1、查看表数据 select * from 表名；\n2、给定条件的查询 select * from 表名 where 条件;\n3、查询指定字段 select 字段名1,字段名2 from 表名；\n字段顺序影响显示顺序\nselect 字段1 as 别名1,字段2 as 别名2 from 表名;\nselect 表名.字段1,表名.字段2 from 表名;\nselcect 别名.字段1,别名.字段 from 表名 as 别名;\n4、可以使用as指定表or列名 select 字段1 as指定名1,字段2 as 指定名2 from 表名 as 指定表名;\n5、消除重复行 select distinct 字段名 from 表名;\n6、条件查询\n6.1 比较运算符 select * from 表名 where 条件语句\n　　<\n　　>\n　　=\n　　<=\n>=\n!= -----不等于\n6.2 逻辑运算符 select * from 表名 where 条件1 and 条件2;\nselect * from 表名 where 条件1 or 条件2；\nselect * from 表名 where not 条件；------ 条件可以是一个或多个\n6.3 模糊查询\n6.3.1like select * from 表名 where 字段 like “” -----一般查询字符串\n%替换任意多个字符\n_替换一个字符_\n6.3.2 rlike select * from 表名 where 字段 rlike \"^ .* $\" ^字 ---以该字开始,.* --- 多个字符 字$----以字结尾\n6.4 范围查询 select * from 表名 where 字段 in () -------非连续范围\nselect * from 表名 where 字段 not in () -------非连续范围\nselect * from 表名 where 字段 between 值1 and 值2； ------连续范围\nselect * from 表名 where 字段 not between 值1 and 值2;\n6.5 空判断 select * from 表名 where 字段 is null; -----null 可以是NULL也可以是NuLL\nselect * from 表名 where 字段 is not null;\n7、排序 select * from 表名 where 字段 条件(关系) order by 字段 asc (升序) desc (降序) 多个排序字段，只需\n要写一个order by 之间用, 隔开，优先按照写在前面的字段排序。\n8、聚合函数 select * from 表名 条件\n例：select count(*) as 男生人数 from students where gender = 1;\n在select 之后 from之前使用函数 ，用括号括住字段或者*，count---计算数量 max----计算最大值\nmin--- 计算最小值 sum ---求和 avg--- 求平均值 round(算数值，保留小数位数)\n例：select round(avg(height), 2) from students where gender =1;\n \n9、分组 group by\n \n6、表关联\n6.1 外链接 基本格式： select * from 表名1，表名2 where 表名1.字段1 = 表名2.字段2 ;\n6.2 内关联 基本格式： select * from 表名1 inner join 表名2 on 表名1.字段1 = 表名2.字段2\n \n删：\n\n物理删除 -------不推荐\n\n1、删除表 drop table 表名; -----表结构一起删除\n2、 delete from 表名; -----删除表内容，不删除表结构,记录主键的位置\ndelete from 表名 where ;例子：delect from students where name = '张三'\n3、删除表 truncate 表名 ------清空表，不删表结构，主键位置从新开始\n \n \n逻辑删除\n\n用一个字段来记录是否这条信息是否以及不再使用了\n添加一个字段 ,默认值为0 表示没有删除 使用1 表示已删除 alter table 表名 add is_delete bit default 0,\n改记录中字段is_delete 的值为1，表示逻辑删除 update 表名 set is_delete =1 where name = \"\""
        },
        {
          "id": "/2020/3/28/python",
          "metadata": {
            "permalink": "/blog/2020/3/28/python",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-28-python.md",
            "source": "@site/blog/2020-3-28-python.md",
            "title": "python脚本基础(1)-python中os的常用方法",
            "description": "1.os模块：os模块在python中包含普遍的操作系统功能，下面列出了一些在os模块中比较有用的部分。",
            "date": "2020-03-28T00:00:00.000Z",
            "formattedDate": "March 28, 2020",
            "tags": [
              {
                "label": "python",
                "permalink": "/blog/tags/python"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 3.88,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "python",
              "title": "python脚本基础(1)-python中os的常用方法",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "python",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "mysql基本命令",
              "permalink": "/blog/2020/3/28/mysql"
            },
            "nextItem": {
              "title": "从Dockerhub拉取镜像并且使用命令运行",
              "permalink": "/blog/2020/3/27/dockerhub_2"
            }
          },
          "content": "1.os模块：os模块在python中包含普遍的操作系统功能，下面列出了一些在os模块中比较有用的部分。\n\nos.sep可以取代操作系统特定的路径分隔符。windows下为 “\\\\”\n\nos.name字符串指示你正在使用的平台。比如对于Windows，它是'nt'，而对于Linux/Unix用户，它是'posix'。\n\nos.getcwd()函数得到当前工作目录，即当前Python脚本工作的目录路径。\n\n<!--truncate-->\nos.getenv()获取一个环境变量，如果没有返回none\n\nos.putenv(key, value)设置一个环境变量值\n\nos.listdir(path)返回指定目录下的所有文件和目录名。\n\nos.remove(path)函数用来删除一个文件。\n\nos.system(command)函数用来运行shell命令。\n\nos.linesep字符串给出当前平台使用的行终止符。例如，Windows使用'\\r\\n'，Linux使用'\\n'而Mac使用'\\r'。\n\nos.curdir:返回当前目录（'.')\nos.chdir(dirname):改变工作目录到dirname\n\n========================================================================================\n\nos.path常用方法：\n\nos.getcwd() 获取当前工作目录，即当前python脚本工作的目录路径\n\nos.chdir(\"dirname\")  改变当前脚本工作目录；相当于shell下cd\n\nos.curdir  返回当前目录: ('.')\n\nos.pardir  获取当前目录的父目录字符串名：('..')\n\nos.makedirs('dirname1/dirname2')    可生成多层递归目录\n\nos.removedirs('dirname1')    若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推\n\nos.mkdir('dirname')    生成单级目录；相当于shell中mkdir dirname\n\nos.rmdir('dirname')    删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirname\n\nos.listdir('dirname')    列出指定目录下的所有文件和子目录，包括隐藏文件，并以列表方式打印\n\nos.remove()  删除一个文件\n\nos.rename(\"oldname\",\"newname\")  重命名文件/目录\n\nos.stat('path/filename')  获取文件/目录信息\n\nos.sep    输出操作系统特定的路径分隔符，win下为\"\\\\\",Linux下为\"/\"\n\nos.linesep    输出当前平台使用的行终止符，win下为\"\\t\\n\",Linux下为\"\\n\"\n\nos.pathsep    输出用于分割文件路径的字符串 win下为;,Linux下为:\n\nos.name    输出字符串指示当前使用平台。win->'nt'; Linux->'posix'\n\nos.system(\"bash command\")  运行shell命令，直接显示\n\nos.environ  获取系统环境变量\n\nos.path.abspath(path)  返回path规范化的绝对路径\n\nos.path.split(path)  将path分割成目录和文件名二元组返回\n\nos.path.dirname(path)  返回path的目录。其实就是os.path.split(path)的第一个元素\n\nos.path.basename(path)  返回path最后的文件名。如何path以／或\\结尾，那么就会返回空值。即os.path.split(path)的第二个元素\n\nos.path.exists(path)  如果path存在，返回True；如果path不存在，返回False\n\nos.path.isabs(path)  如果path是绝对路径，返回True\n\nos.path.isfile(path)  如果path是一个存在的文件，返回True。否则返回False\n\nos.path.isdir(path)  如果path是一个存在的目录，则返回True。否则返回False\n\nos.path.join(path1[, path2[, ...]])  将多个路径组合后返回，第一个绝对路径之前的参数将被忽略\n\nos.path.getatime(path)  返回path所指向的文件或者目录的最后存取时间\n\nos.path.getmtime(path)  返回path所指向的文件或者目录的最后修改时间\n\nos.path.getsize(path) 返回path的大小\n\nos.path.normpath(os.path.join(os.path.abspath(__file__),'..','..','..'))表示返回当前文件的上上上层目录"
        },
        {
          "id": "/2020/3/27/dockerhub_2",
          "metadata": {
            "permalink": "/blog/2020/3/27/dockerhub_2",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-27-dockerhub_2.md",
            "source": "@site/blog/2020-3-27-dockerhub_2.md",
            "title": "从Dockerhub拉取镜像并且使用命令运行",
            "description": "step1: 登录自己的dockerhub,选择需要拉下的镜像",
            "date": "2020-03-27T00:00:00.000Z",
            "formattedDate": "March 27, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 0.59,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dockerhub_2",
              "title": "从Dockerhub拉取镜像并且使用命令运行",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "facebook",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "python脚本基础(1)-python中os的常用方法",
              "permalink": "/blog/2020/3/28/python"
            },
            "nextItem": {
              "title": "push镜像到自己的dockerhub",
              "permalink": "/blog/2020/3/26/dockerhub"
            }
          },
          "content": "<!--truncate-->\n## step1: 登录自己的dockerhub,选择需要拉下的镜像\n\n![png](../img/dockerhub/6.png)\n\n## step2: 点击镜像,点击public view获取拉取命令\n\n![png](../img/dockerhub/7.png)\n\n## step3: 复制获取拉取命令\n\n![png](../img/dockerhub/8.png)\n\n## step4: 在终端输入拉取命令\n1) 拉取镜像\n```python\ndocker pull 镜像名字\n```\n2)查看镜像\n```python\ndocker images\n```\n![png](../img/dockerhub/9.png)\n\n## step4: 运行镜像\n```python\ndocker run -p 127.0.0.1:80:5000/tcp flybirdgroup/classifier\n```\n![png](../img/dockerhub/10.png)\n\n## step5:浏览器运行\n```python\n在浏览器输入: 127.0.0.1\n```\n![png](../img/dockerhub/11.png)\n\n## step6:完美运行\n\n![png](../img/dockerhub/12.png)"
        },
        {
          "id": "/2020/3/26/dockerhub",
          "metadata": {
            "permalink": "/blog/2020/3/26/dockerhub",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-26-dockerhub.md",
            "source": "@site/blog/2020-3-26-dockerhub.md",
            "title": "push镜像到自己的dockerhub",
            "description": "step1: 登录自己的dockerhub,点击create Repository",
            "date": "2020-03-26T00:00:00.000Z",
            "formattedDate": "March 26, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "docker",
                "permalink": "/blog/tags/docker"
              }
            ],
            "readingTime": 0.515,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "dockerhub",
              "title": "push镜像到自己的dockerhub",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "facebook",
                "hello",
                "docusaurus",
                "google cloud",
                "docker"
              ]
            },
            "prevItem": {
              "title": "从Dockerhub拉取镜像并且使用命令运行",
              "permalink": "/blog/2020/3/27/dockerhub_2"
            },
            "nextItem": {
              "title": "Create Virtual Linux by Google 创建linux虚拟机",
              "permalink": "/blog/2020/3/21/createlinux"
            }
          },
          "content": "<!--truncate-->\n## step1: 登录自己的dockerhub,点击create Repository\n\n![png](../img/dockerhub/1.png)\n\n## step2: 创建仓库repository\n复制要保存的仓库名字和推送命令\n![png](../img/dockerhub/2.png)\n\n## step3: 查看本地仓库的镜像\n```python\ndocker images\n```\n![png](../img/dockerhub/3.png)\n\n## step4: 查看本地仓库的镜像\n```python\ndocker tag 本地镜像名字 dockerhub仓库镜像名字\ndocker tag quickstart-image flybirdgroup/helloworld:lastest\n```\n![png](../img/dockerhub/4.png)\n\n\n\n## step5: 推送镜像到远程仓库\n```python\ndocker push flybirdgroup/helloworld:lastest\n```\n![png](../img/dockerhub/5.png)"
        },
        {
          "id": "/2020/3/21/createlinux",
          "metadata": {
            "permalink": "/blog/2020/3/21/createlinux",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-21-createlinux.md",
            "source": "@site/blog/2020-3-21-createlinux.md",
            "title": "Create Virtual Linux by Google 创建linux虚拟机",
            "description": "Learn how to build linux system in Google Cloud Platform Docusaurus 2 alpha.",
            "date": "2020-03-21T00:00:00.000Z",
            "formattedDate": "March 21, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              },
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "linux",
                "permalink": "/blog/tags/linux"
              }
            ],
            "readingTime": 0.97,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "createlinux",
              "title": "Create Virtual Linux by Google 创建linux虚拟机",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "facebook",
                "hello",
                "docusaurus",
                "google cloud",
                "linux"
              ]
            },
            "prevItem": {
              "title": "push镜像到自己的dockerhub",
              "permalink": "/blog/2020/3/26/dockerhub"
            },
            "nextItem": {
              "title": "双向注意力LSTM神经网络",
              "permalink": "/blog/2020/3/19/attention"
            }
          },
          "content": "Learn how to build linux system in Google Cloud Platform [**Docusaurus 2 alpha**](https://v2.docusaurus.io/).\n\n<!--truncate-->\n## step1: 去compute Engine创建虚拟机实例\n\n![png](../img/google/linuxs_object.png)\n\n## step2: 创建实例\n\n![png](../img/google/create_object.png)\n\n## step3: 给虚拟机配置\n<img src=\"../img/google/create_object_1.png\" align=\"left\"/>\n\n## step4: \n名称,区域,机器配置,类型都是按需设置, 磁盘选择Linux 9,防火墙选择http流量\n\n![png](../img/google/create_object_2.png)\n\n\n\n## step 5: \n\n### 1) 我们可以看到有外部ip,我们要记住\n   \n### 2) 我们可以直接选择在浏览器窗口中打开,来进入虚拟机\n   \n<img src=\"../img/google/create_object_3.png\" align=\"left\"/>\n\n![png](../img/google/virtual_linux.png)\n\n\n# 使用终端连接到虚拟linux\n## step 6: 添加本地密钥到元数据中\n\n首先去元数据, 然后选择SSH密钥,点击修改按钮,添加本地的SSH密钥到框中\n![png](../img/google/ssh.png)\n\n## step 7: 使用本地终端连接\n![png](../img/google/linxus1.png)"
        },
        {
          "id": "/2020/3/19/attention",
          "metadata": {
            "permalink": "/blog/2020/3/19/attention",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-19-attention.md",
            "source": "@site/blog/2020-3-19-attention.md",
            "title": "双向注意力LSTM神经网络",
            "description": "原理讲解",
            "date": "2020-03-19T00:00:00.000Z",
            "formattedDate": "March 19, 2020",
            "tags": [
              {
                "label": "CNN",
                "permalink": "/blog/tags/cnn"
              },
              {
                "label": "classifier",
                "permalink": "/blog/tags/classifier"
              },
              {
                "label": "textCNN",
                "permalink": "/blog/tags/text-cnn"
              }
            ],
            "readingTime": 9.09,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "Attention",
              "title": "双向注意力LSTM神经网络",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "CNN",
                "classifier",
                "textCNN"
              ]
            },
            "prevItem": {
              "title": "Create Virtual Linux by Google 创建linux虚拟机",
              "permalink": "/blog/2020/3/21/createlinux"
            },
            "nextItem": {
              "title": "word2vec详解",
              "permalink": "/blog/2020/03/18/word2vec"
            }
          },
          "content": "## 原理讲解\n\nTextAttBiRNN是在双向LSTM文本分类模型的基础上改进的，主要是引入了注意力机制（Attention）。对于双向LSTM编码得到的表征向量，模型能够通过注意力机制，关注与决策最相关的信息。其中注意力机制最先在论文 [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) 中被提出，而此处对于注意力机制的实现参照了论文 [Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems](https://arxiv.org/pdf/1512.08756.pdf)。\n<!--truncate-->\n注意力机制参考\n- [深度学习中的注意力模型](https://zhuanlan.zhihu.com/p/37601161)\n- [深度学习注意力机制](https://zhuanlan.zhihu.com/p/53036028)\n\n请注意,这里的注意力机制与bert中transformer的注意力机制不同,transformer会更加复杂,大家可以参考我关于[transformer](https://github.com/weijiang2009/URun.ResearchPrototype/tree/dev/People/Xiaoxian/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/Transformer%E7%AC%94%E8%AE%B0)\n\nIn the paper [Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems](https://arxiv.org/pdf/1512.08756.pdf), the **feed forward attention** is simplified as follows,\n![png](../img/attention/FeedForwardAttention.png)\n\nFunction a, a learnable function, is recognized as a feed forward network. In this formulation, attention can be seen as producing a fixed-length embedding c of the input sequence by computing an adaptive weighted average of the state sequence h.\n\nc就是注意力,alpha就是权重,h就是隐含状态,alpha通过softmax计算,score就是通过h计算的,h就是当前状态输入的词语和上一隐含状态ht-1计算而来的\n\n\n![png](../img/attention/FeedForwardAttetion_fomular.png)\n\n## 细看结构\nTextAttBiRNN 的网络结构\n![png](../img/attention/text-attn-birnn.png)\n\n### 输入层\n输入层我们可以定义为句子输入长度，每个词经过一个embedding_dim=50的embedding矩阵，最终输出400×50的表示矩阵.假设一个句子有400个词语\n\n### Bi-LSTM\nBi-LSTM层作为一种特征编码层，这层可以提取每个词语的上下文特征，然后将双向的特征进行拼接，然后依旧将每个词语的特征进行输出，因此输出为400×256的特征矩阵\n\nAttention层\nAttention层对这个网络中对每个词语进行了加权求和，这个权重是通过训练不断训练出来的，这层我们的输入x为400×256，我们初始化权重矩阵W为256×1维，然后对x与W进行点乘、归一化（公式的前两个），这样就可以得到400×1的矩阵a，这个矩阵代表的是每个词对应的权重，权重大的词代表注意力大的，这个词的贡献程度更大，最后对每个词语进行加权平均，aT与x进行点乘，得到1×256，这是最终加权平均后输出的总特征向量。\n\n输出层\n与我之前textCNN做中文新闻分类实验差不多，使用全连接层，softmax作为激活函数进行输出。\n\ndemo项目: [情感分析](https://github.com/flybirdgroup/sentiment_analysis)\n# 导入工具包\n```python\nimport pandas as pd\nimport jieba_fast as jieba\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers,regularizers,constraints\nfrom tensorflow.keras import Input,Model,models\nfrom tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\nfrom tensorflow.keras import Input,Model\nfrom tensorflow.keras.layers import Embedding,Dropout,Dense,Bidirectional,LSTM\nfrom tensorflow.keras.models import load_model\nfrom elmoformanylangs import Embedder\nimport numpy as np\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom pandarallel import pandarallel\npandarallel.initialize()\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n```\n\n# 读取数据\n\n```python\ndf = pd.read_csv('./data/sentiment_analysis_data.csv',sep='\t')\ndf\n```\n![png](../img/attention/1.png)\n\n## 打乱样本\n```python\ndf = df.sample(frac=1).reset_index(drop=True)\n```\n\n# 建模思路\n\n## 技术路线\n分两种种情况,好评,负评,中评\n计算路线:\n1 使用TextCNN对每个句子类似n-gram处理\n\n2 使用RNN\n\n3 可以尝试使用attention机制做情感判断,对词进行word2vec,或者elmo embedding,可添加bi-lstm获取上下文信息\n\n[Attention原理请参考](https://www.xn--gmqr38alogxt2a.net/blog/Attention)\n\n\n## 查看是否有缺失值\n\n```python\ndf.info()\n```\n\n## 分析标签数据情况\n\n```python\nlabel_dict = {'-1':'负评','0':'中评','1':'正评'}\n\ndf['label']=df['label'].apply(lambda x: label_dict[str(x)] )\n```\n\n```python\ndf.tail()\n```\n![png](../img/attention/2.png)\n\n### 查看每个文本的长度\n\n```python\ndf['txt_num'] = df['txt'].agg(lambda x: len(x))\n```\n\n```python\ndf.agg({'txt_num':'mean'})\n```\n## 得到句子长度\n\n所以根据数据,得出我们会设置maxlen= 40左右\n\n## jieba分词\n\n\n```python\nfrom pandarallel import pandarallel\npandarallel.initialize()\n```\n\n## 获取停用词和设立分词函数\n\n\n```python\nstopwords = pd.read_csv('./data/stopwords.txt',sep='\\t',index_col=False,quoting=3,encoding='utf-8')\n```\n\n\n```python\ndef split_words(X):\n    result = [i for i in jieba.lcut(X) if i not in stopwords]\n    result = ' '.join(result)\n    return result\n```\n\n\n```python\ndf['txt']=df['txt'].parallel_apply(split_words)\n```\n\n## 建立模型\n\n### Attention网络\n```python\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n    \n    def compute_mask(self,input,input_mask=None):\n        #do not pass the mask to the next layers\n        return None\n    \n    def call(self,x,mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        \n        #K.reshape(x,(-1,features_dim))里面-1可以想象成一行,features_dim变成一行有features_dim维矩阵(1*dim维),K.reshape(self.W, (features_dim, 1)),变成矩阵(dim维*self.W)features_dim行和1维\n        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  \n        # 这里也可以用另外一种表示方式\n#         e = K.reshape(K.dot(K.reshape(x,(1,-1)),K.reshape(self.W,(-1,1))),(-1,1))\n        # 其实就是全连接的矩阵相乘 e = K.dot(x, self.W)\n        if self.bias:\n            e += self.b\n        e = K.tanh(e) # 激活函数\n        a = K.exp(e) # 去指数\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx()) # 转换成floatx类型\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) # softmax函数,得到权重矩阵\n        a = K.expand_dims(a) # 变成(dim,1),这样可以与x进行加权就和得到context\n        \n        c = K.sum(a*x,axis=1) #权重与hidden信息加权就和得到context,也就是我们的注意力\n        return c\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], self.features_dim  \n    \n    def get_config(self):\n        config = {\n                \"step_dim\":self.step_dim,\n                 \"W_regularizer\":self.W_regularizer, \"b_regularizer\":self.b_regularizer,\n                 \"W_constraint\":self.W_constraint, \"b_constraint\":self.b_constraint,\n                 \"bias\":self.bias}\n        base_config = super(Attention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```\n\n### 网络结构\n\n### elmo层\n哈工大开发的动态词向量[elmo](https://github.com/HIT-SCIR/ELMoForManyLangs)\n\nelmo原理可参考[链接](https://www.jianshu.com/p/2fff53696fac)\n\n```python\ne = Embedder('./zhs.model/')\n```\n\n### 创建padding函数\n超过句子长度就截取,不够就补空\n\n```python\ndef pad_sent(x, max_len):\n    if len(x)>max_len:\n        return x[:max_len]\n    else:\n        return x+['']*(max_len-len(x))\n```\n### 创建批量生成器\n\n```python\ndef batch_generator(x, y, batch_size=64):\n    n_batches_per_epoch = len(x)//batch_size\n    for i in range(n_batches_per_epoch):\n        x_batch = np.array(e.sents2elmo([pad_sent(sent,40) for sent in x[batch_size*i:batch_size*(i+1)]]))\n        y_batch = y[batch_size*i:batch_size*(i+1),:]\n        yield x_batch, np.array(y_batch)\n```\n\n```python\ndef predict_generator(x, batch_size=1): #预测\n    n_batches_per_epoch = len(x)//batch_size\n    for i in range(n_batches_per_epoch):\n        x_batch = np.array(e.sents2elmo([pad_sent(sent,40) for sent in x[batch_size*i:batch_size*(i+1)]]))\n        yield x_batch\n```\n\n### 构建ELMOTextBiRNN网络结构\n\n\n```python\nclass ELMOTextBiRNN(object):\n    def __init__(self,maxlen,max_features,embedding_dims,class_num=3,last_activation='softmax'):\n        self.maxlen = maxlen\n        self.max_features = max_features\n        self.embedding_dims = embedding_dims\n        self.class_num = class_num\n        self.last_activation = last_activation\n#     def get_model(self):\n#         embedding = Input((self.maxlen, self.embedding_dims,)) # 输入预训练的词向量\n#         convs = [] \n#         for kernel_size in [3,4,5]: #设定filter大小\n#             c = Conv1D(128,kernel_size,activation='relu')(embedding)\n#             c = GlobalMaxPooling1D()(c)\n#             convs.append(c)\n#         x = Concatenate()(convs)\n#         output = Dense(self.class_num,activation=self.last_activation)(x)\n#         model = Model(inputs=embedding,outputs=output)\n#         return model\n    \n    def get_model(self):\n        embedding = Input((self.maxlen,self.embedding_dims,))\n        x = Bidirectional(LSTM(128,return_sequences=True))(embedding)\n        x = Attention((self.maxlen))(x)\n        output = Dense(self.class_num,activation=self.last_activation)(x)\n        model = Model(embedding,output)\n        return model        \n```\n\n```python\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['txt'].values)\nvocab = tokenizer.word_index\nlen(vocab)+1\n```\n\n### 设置模型参数\n\n\n```python\nmaxlen = 40\nbatch_size = 32\nmax_features = len(vocab)+1\nembedding_dims = 1024\nepochs = 9\n```\n### 获取模型\n```python\nmodel = ELMOTextBiRNN(maxlen,max_features,embedding_dims).get_model()\n```\n\n```python\nplot_model(model,show_shapes=True)\n```\n![png](../img/attention/output_47_0.png)\n\n## 划分训练集,测试集\n\n\n```python\nx_train,x_test,y_train,y_test = train_test_split(df['txt'].values,df['label'])\n```\n\n## 建立词典,词语id化,标签独热编码\n\n```python\ndef encode_category_one_hot(y_train,y_test): \n    from tensorflow.keras.utils import to_categorical\n    set(y_train)\n    categories = set(y_train)\n    categories\n    cat_to_id = dict(zip(categories, range(len(categories))))\n    y_train_id = [cat_to_id[i] for i in y_train]\n    y_test_id = [cat_to_id[i] for i in y_test]\n    cat_to_id\n    y_train_one_hot = to_categorical(y_train_id)\n    y_test_one_hot = to_categorical(y_test_id)\n    return y_train_one_hot,y_test_one_hot,cat_to_id\n```\n\n\n```python\ny_train_one_hot,y_test_one_hot,cat_to_id = encode_category_one_hot(y_train,y_test)\n```\n\n\n```python\nx_train = sentences_list(x_train)\nx_test = sentences_list(x_test)\n```\n\n## 设立早停\n\n\n```python\nmy_callbacks = [ModelCheckpoint('.ELMO_birnn_model.h5'),\n                EarlyStopping(monitor='accuracy',patience=2,mode='max')]\n```\n\n\n```python\nmodel = ELMOTextBiRNN(40,max_features,1024).get_model()\nmodel.compile('adam','categorical_crossentropy',metrics=['accuracy'])\n```\n\n## 测试模型\n\n\n```python\ntext = '今天 天气 很 晴朗 处处 有 阳光 有 阳光'\nsentence = [['%s'%text]]\n```\n\n\n```python\ncat_to_id\n```\n  {'负评': 0, '正评': 1, '中评': 2}\n\n```python\nsentence\n```\n[['今天 天气 很 晴朗 处处 有 阳光 有 阳光']]\n\n\n```python\nmodel.predict_generator(predict_generator(sentence, batch_size=1),steps=1)\n```\narray([[0.21561107, 0.600974  , 0.18341494]], dtype=float32)"
        },
        {
          "id": "/2020/03/18/word2vec",
          "metadata": {
            "permalink": "/blog/2020/03/18/word2vec",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-03-18-word2vec.md",
            "source": "@site/blog/2020-03-18-word2vec.md",
            "title": "word2vec详解",
            "description": "什么是Word2Vec和Embeddings？",
            "date": "2020-03-18T00:00:00.000Z",
            "formattedDate": "March 18, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 31.345,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "elmo",
              "title": "word2vec详解",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "facebook",
                "hello",
                "docusaurus"
              ]
            },
            "prevItem": {
              "title": "双向注意力LSTM神经网络",
              "permalink": "/blog/2020/3/19/attention"
            },
            "nextItem": {
              "title": "使用Google Cloud SDK来配置Google App Engine",
              "permalink": "/blog/2020/3/18/google_cloud"
            }
          },
          "content": "## 什么是Word2Vec和Embeddings？\n我们可以简单认为Word2Vec就是把词语向量化，也就是每个我们对于一个语料库进行学习后，每个单词都有对应的坐标位置，这时候，通过每个单词的坐标位置，我们可以通过余弦定理推导出两个词语，短语或者句子的相似程度。这里可以参考我参与的云润大数据舆情系统的词云项目，[关键词匹配](https://github.com/weijiang2009/URun.ResearchPrototype/tree/dev/Projects/WordCloud/src/words_similarity_xiaoxian)\n![jpeg](../img/word2vec.jpeg)\n<!--truncate-->\n那么它是如何帮助我们做自然语言处理呢？Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。\nEmbedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。\n\n我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。\n\n## 模型\n\nWord2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。本篇文章仅讲解Skip-Gram模型。\n\n这里可以有几篇大神的文章可以参看:\n![一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b306c8b3b1.png?imageMogr2/format/jpg/quality/90)\n\nSkip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。\n\nWord2Vec模型实际上分为了两个部分，**第一部分为建立模型，第二部分是通过模型获取嵌入词向量**。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。\n\n> 上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。\n\n## The Fake Task\n\n我们在上面提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来看通过“Fake Task”我们如何间接地得到这些词向量。\n\n接下来我们来看看如何训练我们的神经网络。假如我们有一个句子**“The dog barked at the mailman”**。\n\n- 首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；\n- 有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是**['The', 'dog'，'barked', 'at']**。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 **(input word, output word)** 形式的训练数据，即 **('dog', 'barked')**，**('dog', 'the')**。\n- 神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 **('dog', 'barked')** 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。\n\n模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子**“The quick brown fox jumps over lazy dog”**，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。\n\n![一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b319eb5f1f.png?imageMogr2/format/jpg/quality/90)\n\n我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。\n\n## 模型细节\n\n我们如何来表示这些单词呢？首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行one-hot编码。\n\n假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，得到的每个单词都是一个10000维的向量，向量每个维度的值只有0或者1，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（ants=[0, 0, 1, 0, ..., 0]）。\n\n还是上面的例子，“The dog barked at the mailman”，那么我们基于这个句子，可以构建一个大小为5的词汇表（忽略大小写和标点符号）：(\"the\", \"dog\", \"barked\", \"at\", \"mailman\")，我们对这个词汇表的单词进行编号0-4。那么”dog“就可以被表示为一个5维向量[0, 1, 0, 0, 0]。\n\n模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。\n\n下图是我们神经网络的结构：\n\n![一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b31d0920ef.png?imageMogr2/format/jpg/quality/90)\n\n隐层没有使用任何激活函数，但是输出层使用了sotfmax。\n\n我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。\n\n## 隐层\n\n说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。\n\nGoogle在最新发布的基于Google news数据集训练的模型中使用的就是300个特征的词向量。词向量的维度是一个可以调节的超参数（在Python的gensim包中封装的Word2Vec接口默认的词向量大小为100， window_size为5）。\n\n看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。\n\n![一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b320f8ed60.png?imageMogr2/format/jpg/quality/90)\n\n所以我们最终的目标就是学习这个隐层的权重矩阵。\n\n我们现在回来接着通过模型的定义来训练我们的这个模型。\n\n上面我们提到，input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行（这句话很绕），看图就明白。\n\n![一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b322ae0c72.png?imageMogr2/format/jpg/quality/90)\n\n我们来看一下上图中的矩阵运算，左边分别是1 x 5和5 x 3的矩阵，结果应该是1 x 3的矩阵，按照矩阵乘法的规则，结果的第一行第一列元素为0 x 17 + 0 x 23 + 0 x 4 + 1 x 10 + 0 x 11 = 10，同理可得其余两个元素为12，19。如果10000个维度的矩阵采用这样的计算方式是十分低效的。\n\n为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1的索引，上面的例子中，左边向量中取值为1的对应维度为3（下标从0开始），那么计算结果就是矩阵的第3行（下标从0开始）—— [10, 12, 19]，这样模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“嵌入词向量”。\n\n## 输出层\n\n经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。\n\n下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图。\n\n![一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b3267c64f4.png?imageMogr2/format/jpg/quality/90)\n\n## 直觉上的理解\n\n下面我们将通过直觉来进行一些思考。\n\n如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。\n\n那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如”engine“和”transmission“这样相关的词语，可能也拥有着相似的上下文。\n\n实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。\n\n> 词干化（stemming）就是去除词缀得到词根的过程。\n\n\n\n在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。\n\n举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的**输入-隐层权重矩阵**和**隐层-输出层的权重矩阵**都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难（太凶残了）。\n\nWord2Vec 的作者在它的第二篇论文中强调了这些问题，下面是作者在第二篇论文中的三个创新：\n\n> 1.将常见的单词组合（word pairs）或者词组作为单个“words”来处理。\n>\n> 2.对高频次单词进行抽样来减少训练样本的个数。\n>\n> 3.对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。\n\n事实证明，对常用词抽样并且对优化目标采用“negative sampling”不仅降低了训练过程中的计算负担，还提高了训练的词向量的质量。\n\n## Word pairs and \"phases\"\n\n论文的作者指出，一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。\n\n在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。\n\n如果你对模型的词汇表感兴趣，可以点击：\n\n<http://t.cn/RoVde3h>\n\n你还可以直接浏览这个词汇表：\n\n<http://t.cn/RoVdsZr>\n\n如果想了解这个模型如何进行文档中的词组抽取，可以看论文中“Learning Phrases”这一章，对应的代码在 word2phrase.c ，相关链接如下。\n\n> 论文链接：\n>\n> <http://t.cn/RMct1c7>\n\n> 代码链接：\n>\n> [http://t.cn/R5auFLz](http://t.cn/RMct1c7)\n\n## 对高频词抽样\n\n在第一部分的讲解中，我们展示了训练样本是如何从原始文档中生成出来的，这里我再重复一次。我们的原始文本为“The quick brown fox jumps over the laze dog”，如果我使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。\n\n![一文详解 Word2vec 之 Skip-Gram 模型（训练篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b387f8455d.png?imageMogr2/format/jpg/quality/90)\n\n但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：\n\n> 1. 当我们得到成对的单词训练样本时，(\"fox\", \"the\") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现。\n> 2. 由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，...）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数。\n\nWord2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。\n如果我们设置窗口大小（即），并且从我们的文本中删除所有的“the”，那么会有下面的结果：\n\n- 由于我们删除了文本中所有的“the”，那么在我们的训练样本中，“the”这个词永远也不会出现在我们的上下文窗口中。\n\n- 当“the”作为input word时，我们的训练样本数至少会减少10个。\n\n> 这句话应该这么理解，假如我们的文本中仅出现了一个“the”，那么当这个“the”作为input word时，我们设置span=10，此时会得到10个训练样本 (\"the\", ...) ，如果删掉这个“the”，我们就会减少10个训练样本。实际中我们的文本中不止一个“the”，因此当“the”作为input word的时候，至少会减少10个训练样本。\n\n上面提到的这两个影响结果实际上就帮助我们解决了高频词带来的问题。\n\n## 抽样率\n\nword2vec的C语言代码实现了一个计算在词汇表中保留某个词概率的公式。\n\nωi 是一个单词，Z(ωi) 是 ωi 这个单词在所有语料中出现的频次。举个栗子，如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么 Z(peanut) = 1000/1000000000 = 1e - 6。\n\n在代码中还有一个参数叫“sample”，这个参数代表一个阈值，默认值为0.001**（在gensim包中的Word2Vec类说明中，这个参数默认为0.001，文档中对这个参数的解释为“ threshold for configuring which higher-frequency words are randomly downsampled”）**。这个值越小意味着这个单词被保留下来的概率越小（即有越大的概率被我们删除）。\n\nP(ωi) 代表着保留某个单词的概率：\n\n![一文详解 Word2vec 之 Skip-Gram 模型（训练篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b3a020bdea.png?imageMogr2/format/jpg/quality/90)\n\n![一文详解 Word2vec 之 Skip-Gram 模型（训练篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b3a191fdcd.png?imageMogr2/format/jpg/quality/90)\n\n图中x轴代表着 Z(ωi) ，即单词 ωi 在语料中出现频率，y轴代表某个单词被保留的概率。对于一个庞大的语料来说，单个单词的出现频率不会很大，即使是常用词，也不可能特别大。\n\n从这个图中，我们可以看到，随着单词出现频率的增高，它被采样保留的概率越来越小，我们还可以看到一些有趣的结论：\n\n- 当 Z(ωi) <= 0.0026 时，P(ωi) = 1.0 。当单词在语料中出现的频率小于 0.0026 时，它是 100% 被保留的，这意味着只有那些在语料中出现频率超过 0.26% 的单词才会被采样。\n\n- 当时 Z(ωi) = 0.00746 时，P(ωi) = 0.5，意味着这一部分的单词有 50% 的概率被保留。\n\n- 当 Z(ωi) = 1.0 时，P(ωi) = 0.033，意味着这部分单词以 3.3% 的概率被保留。\n\n> 如果你去看那篇论文的话，你会发现作者在论文中对函数公式的定义和在C语言代码的实现上有一些差别，但我认为C语言代码的公式实现是更权威的一个版本。\n\n## 负采样（negative sampling）\n\n训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。\n正如我们上面所讨论的，vocabulary的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过我们数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。\n**负采样（negative sampling）**解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。\n当我们用训练样本 ( input word: \"fox\"，output word: \"quick\") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。\n当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。\n\n> 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。\n\n回忆一下我们的隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新 300 x 6 = 1800 个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。\n\n## 如何选择negative words\n\n我们使用“一元模型分布（unigram distribution）”来选择“negative words”。\n要注意的一点是，一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。\n在word2vec的C语言实现中，你可以看到对于这个概率的实现公式。每个单词被选为“negative words”的概率计算公式与其出现的频次有关。\n代码中的公式实现如下：\n\n![一文详解 Word2vec 之 Skip-Gram 模型（训练篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b3b5516125.png?imageMogr2/format/jpg/quality/90)\n\n每个单词被赋予一个权重，即 f(ωi)， 它代表着单词出现的频次。\n\n公式中开3/4的根号完全是基于经验的，论文中提到这个公式的效果要比其它公式更加出色。你可以在google的搜索栏中输入“plot y = x^(3/4) and y = x”，然后看到这两幅图（如下图），仔细观察x在[0,1]区间内时y的取值，x^(3/4) 有一小段弧形，取值在 y = x 函数之上。\n\n![一文详解 Word2vec 之 Skip-Gram 模型（训练篇）](https://static.leiphone.com/uploads/new/article/740_740/201706/594b3bb3458fc.png?imageMogr2/format/jpg/quality/90)\n\n负采样的C语言实现非常的有趣。unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式，也就是说计算出的**负采样概率\\*1亿=单词在表中出现的次数**。\n\n有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。\n\n到目前为止，Word2Vec中的Skip-Gram模型就讲完了，对于里面具体的数学公式推导细节这里并没有深入。这篇文章只是对于实现细节上的一些思想进行了阐述。\n\n\n\n\n\n```"
        },
        {
          "id": "/2020/3/18/google_cloud",
          "metadata": {
            "permalink": "/blog/2020/3/18/google_cloud",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-18-google_cloud.md",
            "source": "@site/blog/2020-3-18-google_cloud.md",
            "title": "使用Google Cloud SDK来配置Google App Engine",
            "description": "Learn how to build deep learning in Google Cloud Platform Docusaurus 2 alpha.",
            "date": "2020-03-18T00:00:00.000Z",
            "formattedDate": "March 18, 2020",
            "tags": [
              {
                "label": "google cloud",
                "permalink": "/blog/tags/google-cloud"
              },
              {
                "label": "SDK",
                "permalink": "/blog/tags/sdk"
              },
              {
                "label": "socket",
                "permalink": "/blog/tags/socket"
              },
              {
                "label": "google cloud init",
                "permalink": "/blog/tags/google-cloud-init"
              }
            ],
            "readingTime": 1.145,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI engine @ Facebook",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "google cloud",
              "title": "使用Google Cloud SDK来配置Google App Engine",
              "author": "招晓贤",
              "author_title": "AI engine @ Facebook",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "google cloud",
                "SDK",
                "socket",
                "google cloud init"
              ]
            },
            "prevItem": {
              "title": "word2vec详解",
              "permalink": "/blog/2020/03/18/word2vec"
            },
            "nextItem": {
              "title": "textCNN卷积神经网络分类",
              "permalink": "/blog/2020/3/18/textCNN"
            }
          },
          "content": "Learn how to build deep learning in Google Cloud Platform [**Docusaurus 2 alpha**](https://v2.docusaurus.io/).\n\n<!--truncate-->\n\n使用Google Cloud SDK来配置Google App Engine\n\nGoogle App Engine 是一个脱离了基础架构束缚的全面托管型平台，功能十分强大，当今最成功的一些公司都纷纷在 App Engine 上运行他们的应用。\n\n　　之前我曾经介绍过使用Google App Engine SDK来更新Google App Engine的工程，目前Google App Engine有了一个新的SDK：Google Cloud SDK，使用这个SDK能更快更高效地进行维护和更新。下面我就介绍一下Google Cloud SDK的简单使用方法。\n\n　　先从这个地址来下载安装SDK环境，包括下载并安装 Python 2.7， 下载并安装 Google Cloud SDK。\n\n　　使用 gcloud init --skip-diagnostics 来初始化并登陆Google账户，选择一个工程。支持socks5代理，用户可以在初始化的时候把代理设置上。\n\n![png](../img/SDK/1.png)\n\n![png](../img/SDK/2.png)"
        },
        {
          "id": "/2020/3/18/textCNN",
          "metadata": {
            "permalink": "/blog/2020/3/18/textCNN",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-3-18-textCNN.md",
            "source": "@site/blog/2020-3-18-textCNN.md",
            "title": "textCNN卷积神经网络分类",
            "description": "云润人工智能部门《NLP零基础快速上手教程》课程资料 by 算法工程师:招晓贤",
            "date": "2020-03-18T00:00:00.000Z",
            "formattedDate": "March 18, 2020",
            "tags": [
              {
                "label": "facebook",
                "permalink": "/blog/tags/facebook"
              },
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 4.69,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "招晓贤",
                "title": "AI Engineer",
                "url": "https://github.com/flybirdgroup",
                "imageURL": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"
              }
            ],
            "frontMatter": {
              "id": "welcome",
              "title": "textCNN卷积神经网络分类",
              "author": "招晓贤",
              "author_title": "AI Engineer",
              "author_url": "https://github.com/flybirdgroup",
              "author_image_url": "https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",
              "tags": [
                "facebook",
                "hello",
                "docusaurus"
              ]
            },
            "prevItem": {
              "title": "使用Google Cloud SDK来配置Google App Engine",
              "permalink": "/blog/2020/3/18/google_cloud"
            },
            "nextItem": {
              "title": "Long Blog Post",
              "permalink": "/blog/long-blog-post"
            }
          },
          "content": "**[云润人工智能部门](https://my-website-six.now.sh/)《NLP零基础快速上手教程》课程资料 by [算法工程师:招晓贤](https://github.com/flybirdgroup)**\n<!--truncate-->\n# 菜鸟也能玩转NLP-卷积神经网络文本分类\n**[云润人工智能部门](https://my-website-six.now.sh/)《NLP零基础快速上手教程》课程资料 by [算法工程师:招晓贤](https://github.com/flybirdgroup)**\n\n## 原理讲解\nTextCNN出处：论文[Convolutional Neural Networks for Sentence Classification](http://www.aclweb.org/anthology/D14-1181)\n\n### 论文核心点\n\n![png](../img/CNN/TextCNN.png)\n\n1. Represent sentence with **static and non-static channels**.\n2. **Convolve** with multiple filter widths and feature maps.\n3. Use **max-over-time pooling**.\n4. Use **fully connected layer** with **dropout** and **softmax** ouput.\n\n### TextCNN基础知识\n#### 词向量\n1. 随机初始化\n2. 预训练词向量进行初始化,在训练过程中固定(CNN-static),注意与图像CNN的不同\n3. 预训练词向量进行初始化,在训练过程中微调(CNN-non-static)\n4. 多通道(CNN-multichannel):将固定的预训练词向量和微调的词向量分别作为一个通道(channel),卷积操作同时在两个通道上进行,可以类比图像RGB三通道\n\n#### 详细说明\n1. 上图的图片为例,句子长度为n=9,词向量维度k为6,filter有两种窗口大小,每种有2个,所以filter有4个.\n2. 红色框的为h=2,卷积后的向量维度为n-h+1=9-2+1=8\n3. 黄色框h=3,卷积后的向量维度是n-h+1=9-3+1=7 (论文原图少画了一个维度)\n\n### 项目实现\n\nTextCNN 的网络结构：\n\n\n![png](../img/CNN/TextCNN_network_structure.png)\n\n\n\n## 模型构建与训练\n\n### 1.定义网络结构\n\n\n```python\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n\n#重要参数解释\n#maxlen:句子最大长度\n#max_features:词典最大数量\n# embeding_dims:词向量维度数\n#class_num: 分类数\n#last_activation:激活函数\n\nclass TextCNN(object):\n    def __init__(self, maxlen, max_features, embedding_dims,\n                 class_num=5,\n                 last_activation='softmax'):\n        self.maxlen = maxlen\n        self.max_features = max_features\n        self.embedding_dims = embedding_dims\n        self.class_num = class_num\n        self.last_activation = last_activation\n\n    def get_model(self):\n        input = Input((self.maxlen,))\n        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n        convs = []\n        for kernel_size in [3, 4, 5]:\n            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n            c = GlobalMaxPooling1D()(c)\n            convs.append(c)\n        x = Concatenate()(convs)\n\n        output = Dense(self.class_num, activation=self.last_activation)(x)\n        model = Model(inputs=input, outputs=output)\n        return model\n```\n\n### 2.数据处理与训练\n\n\n```python\nfrom tensorflow.keras.preprocessing import sequence\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom utils import *\n\n# 路径等配置\ndata_dir = \"./processed_data\"\nvocab_file = \"./vocab/vocab.txt\"\nvocab_size = 40000\n\n# 神经网络配置\nmax_features = 40001\nmaxlen = 100\nbatch_size = 64\nembedding_dims = 50\nepochs = 8\n\nprint('数据预处理与加载数据...')\n# 如果不存在词汇表，重建\nif not os.path.exists(vocab_file):  \n    build_vocab(data_dir, vocab_file, vocab_size)\n# 获得 词汇/类别 与id映射字典\ncategories, cat_to_id = read_category()\nwords, word_to_id = read_vocab(vocab_file)\n\n# 全部数据\nx, y = read_files(data_dir)\ndata = list(zip(x,y))\ndel x,y\n# 乱序\nrandom.shuffle(data)\n# 切分训练集和测试集\ntrain_data, test_data = train_test_split(data)\n# 对文本的词id和类别id进行编码\nx_train = encode_sentences([content[0] for content in train_data], word_to_id)\ny_train = to_categorical(encode_cate([content[1] for content in train_data], cat_to_id))\nx_test = encode_sentences([content[0] for content in test_data], word_to_id)\ny_test = to_categorical(encode_cate([content[1] for content in test_data], cat_to_id))\n\nprint('对序列做padding，保证是 samples*timestep 的维度')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('构建模型...')\nmodel = TextCNN(maxlen, max_features, embedding_dims).get_model()\nmodel.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n\nprint('训练...')\n# 设定callbacks回调函数\nmy_callbacks = [\n    ModelCheckpoint('./cnn_model.h5', verbose=1),\n    EarlyStopping(monitor='val_accuracy', patience=2, mode='max')\n]\n\n# fit拟合数据\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          callbacks=my_callbacks,\n          validation_data=(x_test, y_test))\n\n#print('对测试集预测...')\n#result = model.predict(x_test)\n```\n\n\n### 3.训练中间信息输出\n\n\n```python\nimport matplotlib.pyplot as plt\nplt.switch_backend('agg')\n%matplotlib inline\n\nfig1 = plt.figure()\nplt.plot(history.history['loss'],'r',linewidth=3.0)\nplt.plot(history.history['val_loss'],'b',linewidth=3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves :CNN',fontsize=16)\nfig1.savefig('loss_cnn.png')\nplt.show()\n```\n![png](../img/CNN/output_8_0.png)\n\n```python\nfig2=plt.figure()\nplt.plot(history.history['acc'],'r',linewidth=3.0)\nplt.plot(history.history['val_acc'],'b',linewidth=3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves : CNN',fontsize=16)\nfig2.savefig('accuracy_cnn.png')\nplt.show()\n```\n\n![png](../img/CNN/output_9_0.png)\n\n\n### 4.模型结构打印\n\n\n```python\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes=True, show_layer_names=True)\n```\n![png](../img/CNN/CNN_structure.png)\n\n\n\n### 5.模型导出\n```python\nimport tensorflow as tf\nimport shutil \nmodel = tf.keras.models.load_model('./cnn_model.h5')\n```\n\n### 6.预测模型\n```python\nimport jieba\ntext = \"杨幂好漂亮,发生原子弹\"\nprint(jieba.lcut(text))\ntext_seg = encode_sentences([jieba.lcut(text)], word_to_id)\ntext_input = sequence.pad_sequences(text_seg, maxlen=maxlen)\nprint(model.predict(text_input))\n```\n\n###   7.前后端结合+docker部署\n项目demo：docker部署链接:[中文新闻多分类demo](https://hub.docker.com/repository/docker/flybirdgroup/classifier)\n\n`docker启动命令`\n```python\ndocker run -p 127.0.0.1:80:5000/tcp flybirdgroup/classifier\n```\n![png](../img/CNN/docker1.png)\n\n![png](../img/CNN/docker2.png)"
        },
        {
          "id": "long-blog-post",
          "metadata": {
            "permalink": "/blog/long-blog-post",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-05-29-long-blog-post.md",
            "source": "@site/blog/2019-05-29-long-blog-post.md",
            "title": "Long Blog Post",
            "description": "This is the summary of a very long blog post,",
            "date": "2019-05-29T00:00:00.000Z",
            "formattedDate": "May 29, 2019",
            "tags": [
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 2.05,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Endilie Yacop Sucipto",
                "title": "Maintainer of Docusaurus",
                "url": "https://github.com/endiliey",
                "imageURL": "https://github.com/endiliey.png",
                "key": "endi"
              }
            ],
            "frontMatter": {
              "slug": "long-blog-post",
              "title": "Long Blog Post",
              "authors": "endi",
              "tags": [
                "hello",
                "docusaurus"
              ]
            },
            "prevItem": {
              "title": "textCNN卷积神经网络分类",
              "permalink": "/blog/2020/3/18/textCNN"
            },
            "nextItem": {
              "title": "Long Blog Post",
              "permalink": "/blog/long-blog-post"
            }
          },
          "content": "This is the summary of a very long blog post,\n\nUse a `<!--` `truncate` `-->` comment to limit blog post size in the list view.\n\n<!--truncate-->\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"
        },
        {
          "id": "long-blog-post",
          "metadata": {
            "permalink": "/blog/long-blog-post",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-05-29-long-blog-post的副本.md",
            "source": "@site/blog/2019-05-29-long-blog-post的副本.md",
            "title": "Long Blog Post",
            "description": "This is the summary of a very long blog post,",
            "date": "2019-05-29T00:00:00.000Z",
            "formattedDate": "May 29, 2019",
            "tags": [
              {
                "label": "hello",
                "permalink": "/blog/tags/hello"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 2.05,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Endilie Yacop Sucipto",
                "title": "Maintainer of Docusaurus",
                "url": "https://github.com/endiliey",
                "imageURL": "https://github.com/endiliey.png",
                "key": "endi"
              }
            ],
            "frontMatter": {
              "slug": "long-blog-post",
              "title": "Long Blog Post",
              "authors": "endi",
              "tags": [
                "hello",
                "docusaurus"
              ]
            },
            "prevItem": {
              "title": "Long Blog Post",
              "permalink": "/blog/long-blog-post"
            },
            "nextItem": {
              "title": "First Blog Post",
              "permalink": "/blog/first-blog-post"
            }
          },
          "content": "This is the summary of a very long blog post,\n\nUse a `<!--` `truncate` `-->` comment to limit blog post size in the list view.\n\n<!--truncate-->\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"
        },
        {
          "id": "first-blog-post",
          "metadata": {
            "permalink": "/blog/first-blog-post",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-05-28-first-blog-post.md",
            "source": "@site/blog/2019-05-28-first-blog-post.md",
            "title": "First Blog Post",
            "description": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet",
            "date": "2019-05-28T00:00:00.000Z",
            "formattedDate": "May 28, 2019",
            "tags": [
              {
                "label": "hola",
                "permalink": "/blog/tags/hola"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 0.12,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "Gao Wei",
                "title": "Docusaurus Core Team",
                "url": "https://github.com/wgao19",
                "image_url": "https://github.com/wgao19.png",
                "imageURL": "https://github.com/wgao19.png"
              }
            ],
            "frontMatter": {
              "slug": "first-blog-post",
              "title": "First Blog Post",
              "authors": {
                "name": "Gao Wei",
                "title": "Docusaurus Core Team",
                "url": "https://github.com/wgao19",
                "image_url": "https://github.com/wgao19.png",
                "imageURL": "https://github.com/wgao19.png"
              },
              "tags": [
                "hola",
                "docusaurus"
              ]
            },
            "prevItem": {
              "title": "Long Blog Post",
              "permalink": "/blog/long-blog-post"
            },
            "nextItem": {
              "title": "First Blog Post",
              "permalink": "/blog/first-blog-post"
            }
          },
          "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"
        },
        {
          "id": "first-blog-post",
          "metadata": {
            "permalink": "/blog/first-blog-post",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-05-28-first-blog-post的副本.md",
            "source": "@site/blog/2019-05-28-first-blog-post的副本.md",
            "title": "First Blog Post",
            "description": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet",
            "date": "2019-05-28T00:00:00.000Z",
            "formattedDate": "May 28, 2019",
            "tags": [
              {
                "label": "hola",
                "permalink": "/blog/tags/hola"
              },
              {
                "label": "docusaurus",
                "permalink": "/blog/tags/docusaurus"
              }
            ],
            "readingTime": 0.12,
            "hasTruncateMarker": false,
            "authors": [
              {
                "name": "Gao Wei",
                "title": "Docusaurus Core Team",
                "url": "https://github.com/wgao19",
                "image_url": "https://github.com/wgao19.png",
                "imageURL": "https://github.com/wgao19.png"
              }
            ],
            "frontMatter": {
              "slug": "first-blog-post",
              "title": "First Blog Post",
              "authors": {
                "name": "Gao Wei",
                "title": "Docusaurus Core Team",
                "url": "https://github.com/wgao19",
                "image_url": "https://github.com/wgao19.png",
                "imageURL": "https://github.com/wgao19.png"
              },
              "tags": [
                "hola",
                "docusaurus"
              ]
            },
            "prevItem": {
              "title": "First Blog Post",
              "permalink": "/blog/first-blog-post"
            }
          },
          "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"
        }
      ],
      "blogListPaginated": [
        {
          "items": [
            "welcome",
            "mdx-blog-post",
            "/2020/6/12/Airflow_Dataproc",
            "/2020/6/12/快速搭建wordpress网站",
            "/2020/5/1/Kubernetes",
            "/2020/4/30/linux命令",
            "/2020/4/30/python读取Biggquery",
            "/2020/4/30/创建jenkins在GKE",
            "/2020/4/29/bq_常用命令",
            "/2020/4/29/_GCS读写by python"
          ],
          "metadata": {
            "permalink": "/blog",
            "page": 1,
            "postsPerPage": 10,
            "totalPages": 6,
            "totalCount": 53,
            "nextPage": "/blog/page/2",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "/2020/4/29/_gsutil常用命令",
            "/2020/4/27/_terraform",
            "/2020/4/26/关于nginx和port知识总结",
            "/2020/4/21/python读取avro的schema",
            "/2020/4/21/把avro文件放到bigquery",
            "/2020/4/18/dataproc+spark+yaml",
            "/2020/4/18/yaml语法学习",
            "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
            "/2020/4/17/dataproc参数化跑spark和读写avro",
            "/2020/4/17/dataproc自动伸缩和运行sparkjob"
          ],
          "metadata": {
            "permalink": "/blog/page/2",
            "page": 2,
            "postsPerPage": 10,
            "totalPages": 6,
            "totalCount": 53,
            "previousPage": "/blog",
            "nextPage": "/blog/page/3",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "/2020/4/17/dataproc跑pyspark",
            "/2020/4/17/dataproc运行Apache Spark集群",
            "/2020/4/16/bq_脚本制作presentation",
            "/2020/4/15/bq_sql常用短语",
            "/2020/4/15/gcp_常用命令",
            "/2020/4/11/dataflow-apache beam基本概念",
            "/2020/4/11/dataflow创建作业",
            "/2020/4/11/dataflow流数据输入到bigquery",
            "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2",
            "/2020/4/11/jupyterhub_GCP"
          ],
          "metadata": {
            "permalink": "/blog/page/3",
            "page": 3,
            "postsPerPage": 10,
            "totalPages": 6,
            "totalCount": 53,
            "previousPage": "/blog/page/2",
            "nextPage": "/blog/page/4",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业",
            "/2020/4/11/安装spark",
            "/2020/4/05/tf_serving",
            "/2020/4/04/webscrapper",
            "/2020/4/02/faker",
            "/2020/4/02/git_big_file copy",
            "/2020/3/31/douban",
            "/2020/3/30/BigQuery Storage API",
            "/2020/3/29/ssh_gcs",
            "/2020/3/28/App Engine"
          ],
          "metadata": {
            "permalink": "/blog/page/4",
            "page": 4,
            "postsPerPage": 10,
            "totalPages": 6,
            "totalCount": 53,
            "previousPage": "/blog/page/3",
            "nextPage": "/blog/page/5",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "/2020/3/28/mysql",
            "/2020/3/28/python",
            "/2020/3/27/dockerhub_2",
            "/2020/3/26/dockerhub",
            "/2020/3/21/createlinux",
            "/2020/3/19/attention",
            "/2020/03/18/word2vec",
            "/2020/3/18/google_cloud",
            "/2020/3/18/textCNN",
            "long-blog-post"
          ],
          "metadata": {
            "permalink": "/blog/page/5",
            "page": 5,
            "postsPerPage": 10,
            "totalPages": 6,
            "totalCount": 53,
            "previousPage": "/blog/page/4",
            "nextPage": "/blog/page/6",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "long-blog-post",
            "first-blog-post",
            "first-blog-post"
          ],
          "metadata": {
            "permalink": "/blog/page/6",
            "page": 6,
            "postsPerPage": 10,
            "totalPages": 6,
            "totalCount": 53,
            "previousPage": "/blog/page/5",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        }
      ],
      "blogTags": {
        "/blog/tags/facebook": {
          "label": "facebook",
          "items": [
            "welcome",
            "/2020/6/12/Airflow_Dataproc",
            "/2020/6/12/快速搭建wordpress网站",
            "/2020/5/1/Kubernetes",
            "/2020/3/28/mysql",
            "/2020/3/27/dockerhub_2",
            "/2020/3/26/dockerhub",
            "/2020/3/21/createlinux",
            "/2020/03/18/word2vec",
            "/2020/3/18/textCNN"
          ],
          "permalink": "/blog/tags/facebook",
          "pages": [
            {
              "items": [
                "welcome",
                "/2020/6/12/Airflow_Dataproc",
                "/2020/6/12/快速搭建wordpress网站",
                "/2020/5/1/Kubernetes",
                "/2020/3/28/mysql",
                "/2020/3/27/dockerhub_2",
                "/2020/3/26/dockerhub",
                "/2020/3/21/createlinux",
                "/2020/03/18/word2vec",
                "/2020/3/18/textCNN"
              ],
              "metadata": {
                "permalink": "/blog/tags/facebook",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 10,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/hello": {
          "label": "hello",
          "items": [
            "welcome",
            "/2020/6/12/Airflow_Dataproc",
            "/2020/6/12/快速搭建wordpress网站",
            "/2020/5/1/Kubernetes",
            "/2020/3/31/douban",
            "/2020/3/30/BigQuery Storage API",
            "/2020/3/29/ssh_gcs",
            "/2020/3/28/App Engine",
            "/2020/3/28/mysql",
            "/2020/3/28/python",
            "/2020/3/27/dockerhub_2",
            "/2020/3/26/dockerhub",
            "/2020/3/21/createlinux",
            "/2020/03/18/word2vec",
            "/2020/3/18/textCNN",
            "long-blog-post",
            "long-blog-post"
          ],
          "permalink": "/blog/tags/hello",
          "pages": [
            {
              "items": [
                "welcome",
                "/2020/6/12/Airflow_Dataproc",
                "/2020/6/12/快速搭建wordpress网站",
                "/2020/5/1/Kubernetes",
                "/2020/3/31/douban",
                "/2020/3/30/BigQuery Storage API",
                "/2020/3/29/ssh_gcs",
                "/2020/3/28/App Engine",
                "/2020/3/28/mysql",
                "/2020/3/28/python"
              ],
              "metadata": {
                "permalink": "/blog/tags/hello",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 17,
                "nextPage": "/blog/tags/hello/page/2",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            },
            {
              "items": [
                "/2020/3/27/dockerhub_2",
                "/2020/3/26/dockerhub",
                "/2020/3/21/createlinux",
                "/2020/03/18/word2vec",
                "/2020/3/18/textCNN",
                "long-blog-post",
                "long-blog-post"
              ],
              "metadata": {
                "permalink": "/blog/tags/hello/page/2",
                "page": 2,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 17,
                "previousPage": "/blog/tags/hello",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/docusaurus": {
          "label": "docusaurus",
          "items": [
            "welcome",
            "mdx-blog-post",
            "/2020/6/12/Airflow_Dataproc",
            "/2020/6/12/快速搭建wordpress网站",
            "/2020/5/1/Kubernetes",
            "/2020/3/31/douban",
            "/2020/3/30/BigQuery Storage API",
            "/2020/3/29/ssh_gcs",
            "/2020/3/28/App Engine",
            "/2020/3/28/mysql",
            "/2020/3/28/python",
            "/2020/3/27/dockerhub_2",
            "/2020/3/26/dockerhub",
            "/2020/3/21/createlinux",
            "/2020/03/18/word2vec",
            "/2020/3/18/textCNN",
            "long-blog-post",
            "long-blog-post",
            "first-blog-post",
            "first-blog-post"
          ],
          "permalink": "/blog/tags/docusaurus",
          "pages": [
            {
              "items": [
                "welcome",
                "mdx-blog-post",
                "/2020/6/12/Airflow_Dataproc",
                "/2020/6/12/快速搭建wordpress网站",
                "/2020/5/1/Kubernetes",
                "/2020/3/31/douban",
                "/2020/3/30/BigQuery Storage API",
                "/2020/3/29/ssh_gcs",
                "/2020/3/28/App Engine",
                "/2020/3/28/mysql"
              ],
              "metadata": {
                "permalink": "/blog/tags/docusaurus",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 20,
                "nextPage": "/blog/tags/docusaurus/page/2",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            },
            {
              "items": [
                "/2020/3/28/python",
                "/2020/3/27/dockerhub_2",
                "/2020/3/26/dockerhub",
                "/2020/3/21/createlinux",
                "/2020/03/18/word2vec",
                "/2020/3/18/textCNN",
                "long-blog-post",
                "long-blog-post",
                "first-blog-post",
                "first-blog-post"
              ],
              "metadata": {
                "permalink": "/blog/tags/docusaurus/page/2",
                "page": 2,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 20,
                "previousPage": "/blog/tags/docusaurus",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/google-cloud": {
          "label": "google cloud",
          "items": [
            "/2020/6/12/Airflow_Dataproc",
            "/2020/6/12/快速搭建wordpress网站",
            "/2020/5/1/Kubernetes",
            "/2020/3/31/douban",
            "/2020/3/30/BigQuery Storage API",
            "/2020/3/29/ssh_gcs",
            "/2020/3/28/App Engine",
            "/2020/3/28/mysql",
            "/2020/3/28/python",
            "/2020/3/27/dockerhub_2",
            "/2020/3/26/dockerhub",
            "/2020/3/21/createlinux",
            "/2020/3/18/google_cloud"
          ],
          "permalink": "/blog/tags/google-cloud",
          "pages": [
            {
              "items": [
                "/2020/6/12/Airflow_Dataproc",
                "/2020/6/12/快速搭建wordpress网站",
                "/2020/5/1/Kubernetes",
                "/2020/3/31/douban",
                "/2020/3/30/BigQuery Storage API",
                "/2020/3/29/ssh_gcs",
                "/2020/3/28/App Engine",
                "/2020/3/28/mysql",
                "/2020/3/28/python",
                "/2020/3/27/dockerhub_2"
              ],
              "metadata": {
                "permalink": "/blog/tags/google-cloud",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 13,
                "nextPage": "/blog/tags/google-cloud/page/2",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            },
            {
              "items": [
                "/2020/3/26/dockerhub",
                "/2020/3/21/createlinux",
                "/2020/3/18/google_cloud"
              ],
              "metadata": {
                "permalink": "/blog/tags/google-cloud/page/2",
                "page": 2,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 13,
                "previousPage": "/blog/tags/google-cloud",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/linux": {
          "label": "linux",
          "items": [
            "/2020/6/12/Airflow_Dataproc",
            "/2020/6/12/快速搭建wordpress网站",
            "/2020/5/1/Kubernetes",
            "/2020/4/30/linux命令",
            "/2020/4/30/python读取Biggquery",
            "/2020/3/21/createlinux"
          ],
          "permalink": "/blog/tags/linux",
          "pages": [
            {
              "items": [
                "/2020/6/12/Airflow_Dataproc",
                "/2020/6/12/快速搭建wordpress网站",
                "/2020/5/1/Kubernetes",
                "/2020/4/30/linux命令",
                "/2020/4/30/python读取Biggquery",
                "/2020/3/21/createlinux"
              ],
              "metadata": {
                "permalink": "/blog/tags/linux",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 6,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/command-line": {
          "label": "command line",
          "items": [
            "/2020/4/30/linux命令",
            "/2020/4/30/python读取Biggquery",
            "/2020/4/29/bq_常用命令",
            "/2020/4/15/bq_sql常用短语",
            "/2020/4/15/gcp_常用命令"
          ],
          "permalink": "/blog/tags/command-line",
          "pages": [
            {
              "items": [
                "/2020/4/30/linux命令",
                "/2020/4/30/python读取Biggquery",
                "/2020/4/29/bq_常用命令",
                "/2020/4/15/bq_sql常用短语",
                "/2020/4/15/gcp_常用命令"
              ],
              "metadata": {
                "permalink": "/blog/tags/command-line",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 5,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/cat": {
          "label": "cat",
          "items": [
            "/2020/4/30/linux命令",
            "/2020/4/30/python读取Biggquery"
          ],
          "permalink": "/blog/tags/cat",
          "pages": [
            {
              "items": [
                "/2020/4/30/linux命令",
                "/2020/4/30/python读取Biggquery"
              ],
              "metadata": {
                "permalink": "/blog/tags/cat",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/eof": {
          "label": "<<EOF",
          "items": [
            "/2020/4/30/linux命令",
            "/2020/4/30/python读取Biggquery"
          ],
          "permalink": "/blog/tags/eof",
          "pages": [
            {
              "items": [
                "/2020/4/30/linux命令",
                "/2020/4/30/python读取Biggquery"
              ],
              "metadata": {
                "permalink": "/blog/tags/eof",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/loop": {
          "label": "loop",
          "items": [
            "/2020/4/30/linux命令",
            "/2020/4/30/python读取Biggquery"
          ],
          "permalink": "/blog/tags/loop",
          "pages": [
            {
              "items": [
                "/2020/4/30/linux命令",
                "/2020/4/30/python读取Biggquery"
              ],
              "metadata": {
                "permalink": "/blog/tags/loop",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/jenkins": {
          "label": "Jenkins",
          "items": [
            "/2020/4/30/创建jenkins在GKE"
          ],
          "permalink": "/blog/tags/jenkins",
          "pages": [
            {
              "items": [
                "/2020/4/30/创建jenkins在GKE"
              ],
              "metadata": {
                "permalink": "/blog/tags/jenkins",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/gke": {
          "label": "GKE",
          "items": [
            "/2020/4/30/创建jenkins在GKE"
          ],
          "permalink": "/blog/tags/gke",
          "pages": [
            {
              "items": [
                "/2020/4/30/创建jenkins在GKE"
              ],
              "metadata": {
                "permalink": "/blog/tags/gke",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/gcp": {
          "label": "gcp",
          "items": [
            "/2020/4/29/bq_常用命令",
            "/2020/4/29/_GCS读写by python",
            "/2020/4/29/_gsutil常用命令",
            "/2020/4/27/_terraform",
            "/2020/4/21/python读取avro的schema",
            "/2020/4/21/把avro文件放到bigquery",
            "/2020/4/18/dataproc+spark+yaml",
            "/2020/4/18/yaml语法学习",
            "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
            "/2020/4/17/dataproc参数化跑spark和读写avro",
            "/2020/4/17/dataproc自动伸缩和运行sparkjob",
            "/2020/4/17/dataproc跑pyspark",
            "/2020/4/17/dataproc运行Apache Spark集群",
            "/2020/4/15/bq_sql常用短语",
            "/2020/4/15/gcp_常用命令",
            "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2",
            "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
          ],
          "permalink": "/blog/tags/gcp",
          "pages": [
            {
              "items": [
                "/2020/4/29/bq_常用命令",
                "/2020/4/29/_GCS读写by python",
                "/2020/4/29/_gsutil常用命令",
                "/2020/4/27/_terraform",
                "/2020/4/21/python读取avro的schema",
                "/2020/4/21/把avro文件放到bigquery",
                "/2020/4/18/dataproc+spark+yaml",
                "/2020/4/18/yaml语法学习",
                "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
                "/2020/4/17/dataproc参数化跑spark和读写avro"
              ],
              "metadata": {
                "permalink": "/blog/tags/gcp",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 17,
                "nextPage": "/blog/tags/gcp/page/2",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            },
            {
              "items": [
                "/2020/4/17/dataproc自动伸缩和运行sparkjob",
                "/2020/4/17/dataproc跑pyspark",
                "/2020/4/17/dataproc运行Apache Spark集群",
                "/2020/4/15/bq_sql常用短语",
                "/2020/4/15/gcp_常用命令",
                "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2",
                "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
              ],
              "metadata": {
                "permalink": "/blog/tags/gcp/page/2",
                "page": 2,
                "postsPerPage": 10,
                "totalPages": 2,
                "totalCount": 17,
                "previousPage": "/blog/tags/gcp",
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/github": {
          "label": "github",
          "items": [
            "/2020/4/29/bq_常用命令",
            "/2020/4/15/bq_sql常用短语",
            "/2020/4/15/gcp_常用命令",
            "/2020/4/11/jupyterhub_GCP",
            "/2020/4/02/git_big_file copy"
          ],
          "permalink": "/blog/tags/github",
          "pages": [
            {
              "items": [
                "/2020/4/29/bq_常用命令",
                "/2020/4/15/bq_sql常用短语",
                "/2020/4/15/gcp_常用命令",
                "/2020/4/11/jupyterhub_GCP",
                "/2020/4/02/git_big_file copy"
              ],
              "metadata": {
                "permalink": "/blog/tags/github",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 5,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/terraform": {
          "label": "terraform",
          "items": [
            "/2020/4/29/_GCS读写by python",
            "/2020/4/29/_gsutil常用命令",
            "/2020/4/27/_terraform"
          ],
          "permalink": "/blog/tags/terraform",
          "pages": [
            {
              "items": [
                "/2020/4/29/_GCS读写by python",
                "/2020/4/29/_gsutil常用命令",
                "/2020/4/27/_terraform"
              ],
              "metadata": {
                "permalink": "/blog/tags/terraform",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 3,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/compute-engine": {
          "label": "compute engine",
          "items": [
            "/2020/4/29/_GCS读写by python",
            "/2020/4/29/_gsutil常用命令",
            "/2020/4/27/_terraform"
          ],
          "permalink": "/blog/tags/compute-engine",
          "pages": [
            {
              "items": [
                "/2020/4/29/_GCS读写by python",
                "/2020/4/29/_gsutil常用命令",
                "/2020/4/27/_terraform"
              ],
              "metadata": {
                "permalink": "/blog/tags/compute-engine",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 3,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/vm": {
          "label": "vm",
          "items": [
            "/2020/4/29/_GCS读写by python",
            "/2020/4/29/_gsutil常用命令",
            "/2020/4/27/_terraform"
          ],
          "permalink": "/blog/tags/vm",
          "pages": [
            {
              "items": [
                "/2020/4/29/_GCS读写by python",
                "/2020/4/29/_gsutil常用命令",
                "/2020/4/27/_terraform"
              ],
              "metadata": {
                "permalink": "/blog/tags/vm",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 3,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/nginx": {
          "label": "nginx",
          "items": [
            "/2020/4/26/关于nginx和port知识总结"
          ],
          "permalink": "/blog/tags/nginx",
          "pages": [
            {
              "items": [
                "/2020/4/26/关于nginx和port知识总结"
              ],
              "metadata": {
                "permalink": "/blog/tags/nginx",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/port": {
          "label": "port",
          "items": [
            "/2020/4/26/关于nginx和port知识总结"
          ],
          "permalink": "/blog/tags/port",
          "pages": [
            {
              "items": [
                "/2020/4/26/关于nginx和port知识总结"
              ],
              "metadata": {
                "permalink": "/blog/tags/port",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/target": {
          "label": "target",
          "items": [
            "/2020/4/26/关于nginx和port知识总结"
          ],
          "permalink": "/blog/tags/target",
          "pages": [
            {
              "items": [
                "/2020/4/26/关于nginx和port知识总结"
              ],
              "metadata": {
                "permalink": "/blog/tags/target",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/load-balance": {
          "label": "load-balance",
          "items": [
            "/2020/4/26/关于nginx和port知识总结"
          ],
          "permalink": "/blog/tags/load-balance",
          "pages": [
            {
              "items": [
                "/2020/4/26/关于nginx和port知识总结"
              ],
              "metadata": {
                "permalink": "/blog/tags/load-balance",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/dataproc": {
          "label": "dataproc",
          "items": [
            "/2020/4/21/python读取avro的schema",
            "/2020/4/21/把avro文件放到bigquery",
            "/2020/4/18/dataproc+spark+yaml",
            "/2020/4/18/yaml语法学习",
            "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
            "/2020/4/17/dataproc参数化跑spark和读写avro",
            "/2020/4/17/dataproc自动伸缩和运行sparkjob",
            "/2020/4/17/dataproc跑pyspark",
            "/2020/4/17/dataproc运行Apache Spark集群",
            "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2"
          ],
          "permalink": "/blog/tags/dataproc",
          "pages": [
            {
              "items": [
                "/2020/4/21/python读取avro的schema",
                "/2020/4/21/把avro文件放到bigquery",
                "/2020/4/18/dataproc+spark+yaml",
                "/2020/4/18/yaml语法学习",
                "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
                "/2020/4/17/dataproc参数化跑spark和读写avro",
                "/2020/4/17/dataproc自动伸缩和运行sparkjob",
                "/2020/4/17/dataproc跑pyspark",
                "/2020/4/17/dataproc运行Apache Spark集群",
                "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2"
              ],
              "metadata": {
                "permalink": "/blog/tags/dataproc",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 10,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/avro": {
          "label": "avro",
          "items": [
            "/2020/4/21/python读取avro的schema",
            "/2020/4/21/把avro文件放到bigquery"
          ],
          "permalink": "/blog/tags/avro",
          "pages": [
            {
              "items": [
                "/2020/4/21/python读取avro的schema",
                "/2020/4/21/把avro文件放到bigquery"
              ],
              "metadata": {
                "permalink": "/blog/tags/avro",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/bigquery": {
          "label": "bigquery",
          "items": [
            "/2020/4/21/python读取avro的schema",
            "/2020/4/21/把avro文件放到bigquery",
            "/2020/4/11/dataflow-apache beam基本概念",
            "/2020/4/11/dataflow创建作业",
            "/2020/4/11/dataflow流数据输入到bigquery"
          ],
          "permalink": "/blog/tags/bigquery",
          "pages": [
            {
              "items": [
                "/2020/4/21/python读取avro的schema",
                "/2020/4/21/把avro文件放到bigquery",
                "/2020/4/11/dataflow-apache beam基本概念",
                "/2020/4/11/dataflow创建作业",
                "/2020/4/11/dataflow流数据输入到bigquery"
              ],
              "metadata": {
                "permalink": "/blog/tags/bigquery",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 5,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/spark": {
          "label": "Spark",
          "items": [
            "/2020/4/18/dataproc+spark+yaml",
            "/2020/4/18/yaml语法学习",
            "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
            "/2020/4/17/dataproc参数化跑spark和读写avro",
            "/2020/4/17/dataproc自动伸缩和运行sparkjob",
            "/2020/4/17/dataproc跑pyspark",
            "/2020/4/17/dataproc运行Apache Spark集群",
            "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2",
            "/2020/4/11/安装spark"
          ],
          "permalink": "/blog/tags/spark",
          "pages": [
            {
              "items": [
                "/2020/4/18/dataproc+spark+yaml",
                "/2020/4/18/yaml语法学习",
                "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
                "/2020/4/17/dataproc参数化跑spark和读写avro",
                "/2020/4/17/dataproc自动伸缩和运行sparkjob",
                "/2020/4/17/dataproc跑pyspark",
                "/2020/4/17/dataproc运行Apache Spark集群",
                "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2",
                "/2020/4/11/安装spark"
              ],
              "metadata": {
                "permalink": "/blog/tags/spark",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 9,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/hadoop": {
          "label": "Hadoop",
          "items": [
            "/2020/4/18/dataproc+spark+yaml",
            "/2020/4/18/yaml语法学习",
            "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
            "/2020/4/17/dataproc参数化跑spark和读写avro",
            "/2020/4/17/dataproc自动伸缩和运行sparkjob",
            "/2020/4/17/dataproc跑pyspark",
            "/2020/4/17/dataproc运行Apache Spark集群",
            "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2"
          ],
          "permalink": "/blog/tags/hadoop",
          "pages": [
            {
              "items": [
                "/2020/4/18/dataproc+spark+yaml",
                "/2020/4/18/yaml语法学习",
                "/2020/4/17/dataproc+GCS+Bigquery+pyspark",
                "/2020/4/17/dataproc参数化跑spark和读写avro",
                "/2020/4/17/dataproc自动伸缩和运行sparkjob",
                "/2020/4/17/dataproc跑pyspark",
                "/2020/4/17/dataproc运行Apache Spark集群",
                "/2020/4/11/dataflow用Apache Beam python运行 Apache Beam copy 2"
              ],
              "metadata": {
                "permalink": "/blog/tags/hadoop",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 8,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/script": {
          "label": "script",
          "items": [
            "/2020/4/16/bq_脚本制作presentation"
          ],
          "permalink": "/blog/tags/script",
          "pages": [
            {
              "items": [
                "/2020/4/16/bq_脚本制作presentation"
              ],
              "metadata": {
                "permalink": "/blog/tags/script",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/excel": {
          "label": "excel",
          "items": [
            "/2020/4/16/bq_脚本制作presentation"
          ],
          "permalink": "/blog/tags/excel",
          "pages": [
            {
              "items": [
                "/2020/4/16/bq_脚本制作presentation"
              ],
              "metadata": {
                "permalink": "/blog/tags/excel",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/google": {
          "label": "google",
          "items": [
            "/2020/4/16/bq_脚本制作presentation"
          ],
          "permalink": "/blog/tags/google",
          "pages": [
            {
              "items": [
                "/2020/4/16/bq_脚本制作presentation"
              ],
              "metadata": {
                "permalink": "/blog/tags/google",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/dataflow": {
          "label": "dataflow",
          "items": [
            "/2020/4/11/dataflow-apache beam基本概念",
            "/2020/4/11/dataflow创建作业",
            "/2020/4/11/dataflow流数据输入到bigquery",
            "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
          ],
          "permalink": "/blog/tags/dataflow",
          "pages": [
            {
              "items": [
                "/2020/4/11/dataflow-apache beam基本概念",
                "/2020/4/11/dataflow创建作业",
                "/2020/4/11/dataflow流数据输入到bigquery",
                "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
              ],
              "metadata": {
                "permalink": "/blog/tags/dataflow",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 4,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/subpub": {
          "label": "subpub",
          "items": [
            "/2020/4/11/dataflow-apache beam基本概念",
            "/2020/4/11/dataflow创建作业",
            "/2020/4/11/dataflow流数据输入到bigquery"
          ],
          "permalink": "/blog/tags/subpub",
          "pages": [
            {
              "items": [
                "/2020/4/11/dataflow-apache beam基本概念",
                "/2020/4/11/dataflow创建作业",
                "/2020/4/11/dataflow流数据输入到bigquery"
              ],
              "metadata": {
                "permalink": "/blog/tags/subpub",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 3,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/steaming": {
          "label": "steaming",
          "items": [
            "/2020/4/11/dataflow-apache beam基本概念",
            "/2020/4/11/dataflow创建作业",
            "/2020/4/11/dataflow流数据输入到bigquery"
          ],
          "permalink": "/blog/tags/steaming",
          "pages": [
            {
              "items": [
                "/2020/4/11/dataflow-apache beam基本概念",
                "/2020/4/11/dataflow创建作业",
                "/2020/4/11/dataflow流数据输入到bigquery"
              ],
              "metadata": {
                "permalink": "/blog/tags/steaming",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 3,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/git": {
          "label": "git",
          "items": [
            "/2020/4/11/jupyterhub_GCP",
            "/2020/4/02/git_big_file copy"
          ],
          "permalink": "/blog/tags/git",
          "pages": [
            {
              "items": [
                "/2020/4/11/jupyterhub_GCP",
                "/2020/4/02/git_big_file copy"
              ],
              "metadata": {
                "permalink": "/blog/tags/git",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/lfs": {
          "label": "lfs",
          "items": [
            "/2020/4/11/jupyterhub_GCP",
            "/2020/4/02/git_big_file copy"
          ],
          "permalink": "/blog/tags/lfs",
          "pages": [
            {
              "items": [
                "/2020/4/11/jupyterhub_GCP",
                "/2020/4/02/git_big_file copy"
              ],
              "metadata": {
                "permalink": "/blog/tags/lfs",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/apache-beam": {
          "label": "apache beam",
          "items": [
            "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
          ],
          "permalink": "/blog/tags/apache-beam",
          "pages": [
            {
              "items": [
                "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
              ],
              "metadata": {
                "permalink": "/blog/tags/apache-beam",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/sql": {
          "label": "sql",
          "items": [
            "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
          ],
          "permalink": "/blog/tags/sql",
          "pages": [
            {
              "items": [
                "/2020/4/11/使用Dataflow SQL界面运行Dataflow作业"
              ],
              "metadata": {
                "permalink": "/blog/tags/sql",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/jupyter": {
          "label": "jupyter",
          "items": [
            "/2020/4/11/安装spark"
          ],
          "permalink": "/blog/tags/jupyter",
          "pages": [
            {
              "items": [
                "/2020/4/11/安装spark"
              ],
              "metadata": {
                "permalink": "/blog/tags/jupyter",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/pycharm": {
          "label": "pycharm",
          "items": [
            "/2020/4/11/安装spark"
          ],
          "permalink": "/blog/tags/pycharm",
          "pages": [
            {
              "items": [
                "/2020/4/11/安装spark"
              ],
              "metadata": {
                "permalink": "/blog/tags/pycharm",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/java": {
          "label": "java",
          "items": [
            "/2020/4/11/安装spark"
          ],
          "permalink": "/blog/tags/java",
          "pages": [
            {
              "items": [
                "/2020/4/11/安装spark"
              ],
              "metadata": {
                "permalink": "/blog/tags/java",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/docker": {
          "label": "docker",
          "items": [
            "/2020/4/05/tf_serving",
            "/2020/3/31/douban",
            "/2020/3/30/BigQuery Storage API",
            "/2020/3/29/ssh_gcs",
            "/2020/3/28/App Engine",
            "/2020/3/28/mysql",
            "/2020/3/28/python",
            "/2020/3/27/dockerhub_2",
            "/2020/3/26/dockerhub"
          ],
          "permalink": "/blog/tags/docker",
          "pages": [
            {
              "items": [
                "/2020/4/05/tf_serving",
                "/2020/3/31/douban",
                "/2020/3/30/BigQuery Storage API",
                "/2020/3/29/ssh_gcs",
                "/2020/3/28/App Engine",
                "/2020/3/28/mysql",
                "/2020/3/28/python",
                "/2020/3/27/dockerhub_2",
                "/2020/3/26/dockerhub"
              ],
              "metadata": {
                "permalink": "/blog/tags/docker",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 9,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/tf-serving": {
          "label": "tf_serving",
          "items": [
            "/2020/4/05/tf_serving"
          ],
          "permalink": "/blog/tags/tf-serving",
          "pages": [
            {
              "items": [
                "/2020/4/05/tf_serving"
              ],
              "metadata": {
                "permalink": "/blog/tags/tf-serving",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/model": {
          "label": "model",
          "items": [
            "/2020/4/05/tf_serving"
          ],
          "permalink": "/blog/tags/model",
          "pages": [
            {
              "items": [
                "/2020/4/05/tf_serving"
              ],
              "metadata": {
                "permalink": "/blog/tags/model",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/web": {
          "label": "web",
          "items": [
            "/2020/4/04/webscrapper"
          ],
          "permalink": "/blog/tags/web",
          "pages": [
            {
              "items": [
                "/2020/4/04/webscrapper"
              ],
              "metadata": {
                "permalink": "/blog/tags/web",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/scrapper": {
          "label": "scrapper",
          "items": [
            "/2020/4/04/webscrapper"
          ],
          "permalink": "/blog/tags/scrapper",
          "pages": [
            {
              "items": [
                "/2020/4/04/webscrapper"
              ],
              "metadata": {
                "permalink": "/blog/tags/scrapper",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/shopping": {
          "label": "shopping",
          "items": [
            "/2020/4/04/webscrapper"
          ],
          "permalink": "/blog/tags/shopping",
          "pages": [
            {
              "items": [
                "/2020/4/04/webscrapper"
              ],
              "metadata": {
                "permalink": "/blog/tags/shopping",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/faker": {
          "label": "faker",
          "items": [
            "/2020/4/02/faker"
          ],
          "permalink": "/blog/tags/faker",
          "pages": [
            {
              "items": [
                "/2020/4/02/faker"
              ],
              "metadata": {
                "permalink": "/blog/tags/faker",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/汇丰": {
          "label": "汇丰",
          "items": [
            "/2020/4/02/faker"
          ],
          "permalink": "/blog/tags/汇丰",
          "pages": [
            {
              "items": [
                "/2020/4/02/faker"
              ],
              "metadata": {
                "permalink": "/blog/tags/汇丰",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/random-seed": {
          "label": "random seed",
          "items": [
            "/2020/4/02/faker"
          ],
          "permalink": "/blog/tags/random-seed",
          "pages": [
            {
              "items": [
                "/2020/4/02/faker"
              ],
              "metadata": {
                "permalink": "/blog/tags/random-seed",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/python": {
          "label": "python",
          "items": [
            "/2020/3/31/douban",
            "/2020/3/30/BigQuery Storage API",
            "/2020/3/29/ssh_gcs",
            "/2020/3/28/App Engine",
            "/2020/3/28/python"
          ],
          "permalink": "/blog/tags/python",
          "pages": [
            {
              "items": [
                "/2020/3/31/douban",
                "/2020/3/30/BigQuery Storage API",
                "/2020/3/29/ssh_gcs",
                "/2020/3/28/App Engine",
                "/2020/3/28/python"
              ],
              "metadata": {
                "permalink": "/blog/tags/python",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 5,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/cnn": {
          "label": "CNN",
          "items": [
            "/2020/3/19/attention"
          ],
          "permalink": "/blog/tags/cnn",
          "pages": [
            {
              "items": [
                "/2020/3/19/attention"
              ],
              "metadata": {
                "permalink": "/blog/tags/cnn",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/classifier": {
          "label": "classifier",
          "items": [
            "/2020/3/19/attention"
          ],
          "permalink": "/blog/tags/classifier",
          "pages": [
            {
              "items": [
                "/2020/3/19/attention"
              ],
              "metadata": {
                "permalink": "/blog/tags/classifier",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/text-cnn": {
          "label": "textCNN",
          "items": [
            "/2020/3/19/attention"
          ],
          "permalink": "/blog/tags/text-cnn",
          "pages": [
            {
              "items": [
                "/2020/3/19/attention"
              ],
              "metadata": {
                "permalink": "/blog/tags/text-cnn",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/sdk": {
          "label": "SDK",
          "items": [
            "/2020/3/18/google_cloud"
          ],
          "permalink": "/blog/tags/sdk",
          "pages": [
            {
              "items": [
                "/2020/3/18/google_cloud"
              ],
              "metadata": {
                "permalink": "/blog/tags/sdk",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/socket": {
          "label": "socket",
          "items": [
            "/2020/3/18/google_cloud"
          ],
          "permalink": "/blog/tags/socket",
          "pages": [
            {
              "items": [
                "/2020/3/18/google_cloud"
              ],
              "metadata": {
                "permalink": "/blog/tags/socket",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/google-cloud-init": {
          "label": "google cloud init",
          "items": [
            "/2020/3/18/google_cloud"
          ],
          "permalink": "/blog/tags/google-cloud-init",
          "pages": [
            {
              "items": [
                "/2020/3/18/google_cloud"
              ],
              "metadata": {
                "permalink": "/blog/tags/google-cloud-init",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/blog/tags/hola": {
          "label": "hola",
          "items": [
            "first-blog-post",
            "first-blog-post"
          ],
          "permalink": "/blog/tags/hola",
          "pages": [
            {
              "items": [
                "first-blog-post",
                "first-blog-post"
              ],
              "metadata": {
                "permalink": "/blog/tags/hola",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "Blog",
                "blogTitle": "Blog"
              }
            }
          ]
        }
      },
      "blogTagsListPath": "/blog/tags"
    }
  },
  "docusaurus-plugin-content-pages": {
    "default": [
      {
        "type": "jsx",
        "permalink": "/",
        "source": "@site/src/pages/index.js"
      },
      {
        "type": "mdx",
        "permalink": "/markdown-page",
        "source": "@site/src/pages/markdown-page.md",
        "title": "Markdown page example",
        "description": "You don't need React to write simple standalone pages.",
        "frontMatter": {
          "title": "Markdown page example"
        }
      }
    ]
  },
  "docusaurus-plugin-debug": {},
  "docusaurus-theme-classic": {},
  "docusaurus-theme-search-algolia": {},
  "docusaurus-bootstrap-plugin": {},
  "docusaurus-mdx-fallback-plugin": {}
}