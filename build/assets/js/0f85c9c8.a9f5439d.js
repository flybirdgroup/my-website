"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4957],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>f});var a=r(67294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var s=a.createContext({}),c=function(e){var t=a.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},p=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(r),f=n,m=d["".concat(s,".").concat(f)]||d[f]||u[f]||o;return r?a.createElement(m,i(i({ref:t},p),{},{components:r})):a.createElement(m,i({ref:t},p))}));function f(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:n,i[1]=l;for(var c=2;c<o;c++)i[c]=r[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}d.displayName="MDXCreateElement"},19348:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var a=r(87462),n=(r(67294),r(3905));const o={id:"dataproc6",title:"\u7528yaml\u914d\u7f6e\u6587\u4ef6\u4f20\u53c2\u6570\u7ed9pyspark,\u7136\u540e\u518ddataproc\u8fd0\u884c",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},i=void 0,l={unversionedId:"Big_Data/dataproc6",id:"Big_Data/dataproc6",title:"\u7528yaml\u914d\u7f6e\u6587\u4ef6\u4f20\u53c2\u6570\u7ed9pyspark,\u7136\u540e\u518ddataproc\u8fd0\u884c",description:"\u9996\u5148\u6211\u4eec\u8981\u5b66Yaml\u8bed\u6cd5:",source:"@site/docs/Big_Data/2020-4-18-dataproc+spark+yaml.md",sourceDirName:"Big_Data",slug:"/Big_Data/dataproc6",permalink:"/docs/Big_Data/dataproc6",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Big_Data/2020-4-18-dataproc+spark+yaml.md",tags:[{label:"dataproc",permalink:"/docs/tags/dataproc"},{label:"GCP",permalink:"/docs/tags/gcp"},{label:"Spark",permalink:"/docs/tags/spark"},{label:"Hadoop",permalink:"/docs/tags/hadoop"}],version:"current",frontMatter:{id:"dataproc6",title:"\u7528yaml\u914d\u7f6e\u6587\u4ef6\u4f20\u53c2\u6570\u7ed9pyspark,\u7136\u540e\u518ddataproc\u8fd0\u884c",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},sidebar:"tutorialSidebar",previous:{title:"5\u5206\u949f\u5728\u8c37\u6b4c\u4e91\u4e0a\u4f7f\u7528Dataproc\u8fd0\u884cApache Spark\u96c6\u7fa4",permalink:"/docs/Big_Data/dataproc1"},next:{title:"\u901a\u8fc7Composer\u4f7f\u7528Airflow\u8fd0\u884c\u4e00\u4e2aworkflow",permalink:"/docs/Big_Data/Airflow1"}},s={},c=[{value:"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark",id:"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark",level:2},{value:"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobs,jobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005table,fillna,union,transoform\u7b49etl\u4f5c\u4e1a",id:"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobsjobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005tablefillnauniontransoform\u7b49etl\u4f5c\u4e1a",level:2},{value:"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801",id:"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801",level:2},{value:"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc clusters",id:"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc-clusters",level:2}],p={toc:c};function u(e){let{components:t,...r}=e;return(0,n.kt)("wrapper",(0,a.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"\u9996\u5148\u6211\u4eec\u8981\u5b66Yaml\u8bed\u6cd5:\n\u5177\u4f53yaml\u8bed\u6cd5\u53ef\u4ee5\u53c2\u8003",(0,n.kt)("a",{parentName:"p",href:"yaml1"},"yaml\u8bed\u6cd5\u8be6\u60c5")),(0,n.kt)("p",null,"\u6574\u4f53\u601d\u8def"),(0,n.kt)("h2",{id:"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark"},"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\u5bfc\u5165\u5de5\u5177\u5305")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"#!/usr/bin/env python\nfrom pyspark.sql import SparkSession\nimport sys,yaml,datetime\nimport os\nimport pathlib\nimport google.cloud.storage as gcs\n")),(0,n.kt)("ol",{start:2},(0,n.kt)("li",{parentName:"ol"},"\u6211\u4eec\u5199\u4e86\u4e00\u4e2aYaml\u6587\u4ef6\u4f5c\u4e3aconfig\u6587\u4ef6"),(0,n.kt)("li",{parentName:"ol"},'\u6211\u4eec\u5728\u6211\u4eec\u7684pyspark\u6587\u4ef6\u8bfb\u53d6yaml\u6587\u4ef6,\u8fd9\u91cc\u8981\u6ce8\u610f\u7684\u662f,\u56e0\u4e3a\u672c\u5730\u548cGCS\u4f1a\u6709\u4e0d\u540c,\u672c\u5730\u662f\u53ef\u4ee5\u76f4\u63a5\u8bfb\u53d6\u7684,\u4f46\u662f\u5982\u679cyaml\u6587\u4ef6\u5728GCS,yaml\u6587\u4ef6\u5c31\u662fobject,\u662f\u4e0d\u53ef\u6539\u5199\u7684,\u6240\u4ee5\u6211\u4eec\u4e0d\u80fd\u76f4\u63a5open(yaml\u6587\u4ef6,"r")'),(0,n.kt)("li",{parentName:"ol"},"\u6211\u4eec\u9700\u8981\u5728pyspark\u6587\u4ef6\u4e0a\u521b\u5efagcs\u5ba2\u6237\u7aef,\u7136\u540e\u521b\u5efa\u8bbe\u7f6e\u4e00\u4e2a\u672c\u5730\u6587\u4ef6\u8def\u5f84,\u7136\u540e\u901a\u8fc7\u5ba2\u6237\u7aef\u8bfb\u53d6yaml\u6587\u4ef6\u5185\u5bb9\u5e76\u4e14\u4e0b\u8f7d\u5230\u672c\u5730,\u7136\u540e\u518d\u901a\u8fc7\u672c\u5730\u4f7f\u7528with open\u65b9\u6cd5\u8bfb\u53d6yaml\u6587\u4ef6\u5185\u5bb9")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'client = gcs.Client()\n\n#set target file to write to\ntarget = pathlib.Path("local_file.yaml")\n\nconfig_file = sys.argv[1] +"config.yaml"\n\n#set file to download\nFULL_FILE_PATH = config_file\n\n#open filestream with write permissions\nwith target.open(mode="wb") as downloaded_file:\n\n        #download and write file locally\n        client.download_blob_to_file(FULL_FILE_PATH, downloaded_file)\n\nconfig_file="local_file.yaml"\n')),(0,n.kt)("p",null,"\u8bfb\u53d6\u540e,\u6211\u4eec\u5c31\u53ef\u4ee5\u64cd\u4f5c\u4e00\u4e0b\u4ee3\u7801"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'for job in config["jobs"]:\n    print("Creating source views...")\n    for source in job["sources"]:\n        print(source)\n        if source.get("table") is not None:\n            print("Creating view %s from table %s..." % (source["view"], source["table"]))\n            df = spark.table(source["table"])\n            df.show(5)\n            print(\'table now\')\n        else:\n            print("Creating view %s from object %s..." % (source["view"], source["object"]))\n            df = spark.read.format(source[\'object\'][source[\'object\'].rfind(\'.\')+1:]).option("header","true").load(source[\'object\'])\n            df.show(5)\n        if source.get("columns") is not None:\n            # columns listed, select given columns\n            df = df.select(source["columns"])\n            df.show(5)\n        if source.get(\'Fillna\') is not None:\n            print(source[\'Fillna\']["fields"])\n            print(\'hah\',type(source[\'Fillna\']))\n            df = df.fillna({source[\'Fillna\']["fields"]:source[\'Fillna\']["num"]})\n            df.show(5)\n        if source.get("filters") is not None:\n            df.filter(source["filters"])\n        if source.get("union") is not None:\n            df_union = spark.sql("select * from %s"%(source[\'union\']))\n            df.union(df_union)\n            df.show(1)\n        if source.get("join") is not None:\n            cur = df.select(source[\'Key\'])\n            pre = spark.sql("select * from %s"%(source[\'right\']))\n            df = cur.join(pre,[source[\'Key\']],source[\'how\'])\n            df.show(5)\n        df.createOrReplaceTempView(source["view"])\n    print("Performing SQL Transformations...")\n    if job.get("transforms") is not None:\n        for transform in job["transforms"]:\n            spark.sql(transform["sql"])\n            print(df.count())\n    if job.get("targets") is not None:\n        print("Writing out final object to %s..." % (job["targets"]["target_location"]))\n        start = datetime.datetime.now()\n        final_df = spark.table(job["targets"]["final_object"])\n        final_df.write.mode(job["targets"]["mode"]).format(job["targets"]["format"]).save(job["targets"]["target_location"])\n        finish = datetime.datetime.now()\n        print("Finished writing out target object...")\n')),(0,n.kt)("h2",{id:"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobsjobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005tablefillnauniontransoform\u7b49etl\u4f5c\u4e1a"},"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobs,jobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005table,fillna,union,transoform\u7b49etl\u4f5c\u4e1a"),(0,n.kt)("h2",{id:"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801"},"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"CLUSTER_NAME=newnew\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n    --region=global \\\n    --zone=us-central1-b \\\n    --worker-machine-type n1-standard-1 \\\n    --num-workers 2 \\\n    --image-version 1.4-debian \\\n    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n    --metadata 'PIP_PACKAGES=google-cloud-storage PyYAML pathlib avro-python3 dask[dataframe] gcsfs fastavro' \\\n    --enable-component-gateway \\\n    --worker-boot-disk-size=40 \\\n    --optional-components=ANACONDA \\\n    --enable-component-gateway\nBUCKET_NAME=zz_michael\ngcloud config set dataproc/region global\ngcloud dataproc jobs submit pyspark dyyaml.py --cluster newnew \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--jars=https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/spark-avro_2.11-2.4.4.jar \\\n-- gs://${BUCKET_NAME}/yaml/ \n")),(0,n.kt)("h2",{id:"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc-clusters"},"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc clusters"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"CLUSTER_NAME=newnew\ngcloud dataproc clusters delete $CLUSTER_NAME\n")))}u.isMDXComponent=!0}}]);