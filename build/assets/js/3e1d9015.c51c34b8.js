"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4685],{3905:(e,r,a)=>{a.d(r,{Zo:()=>l,kt:()=>m});var t=a(67294);function n(e,r,a){return r in e?Object.defineProperty(e,r,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[r]=a,e}function o(e,r){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),a.push.apply(a,t)}return a}function p(e){for(var r=1;r<arguments.length;r++){var a=null!=arguments[r]?arguments[r]:{};r%2?o(Object(a),!0).forEach((function(r){n(e,r,a[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(a,r))}))}return e}function s(e,r){if(null==e)return{};var a,t,n=function(e,r){if(null==e)return{};var a,t,n={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],r.indexOf(a)>=0||(n[a]=e[a]);return n}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],r.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var i=t.createContext({}),c=function(e){var r=t.useContext(i),a=r;return e&&(a="function"==typeof e?e(r):p(p({},r),e)),a},l=function(e){var r=c(e.components);return t.createElement(i.Provider,{value:r},e.children)},u={inlineCode:"code",wrapper:function(e){var r=e.children;return t.createElement(t.Fragment,{},r)}},d=t.forwardRef((function(e,r){var a=e.components,n=e.mdxType,o=e.originalType,i=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),d=c(a),m=n,g=d["".concat(i,".").concat(m)]||d[m]||u[m]||o;return a?t.createElement(g,p(p({ref:r},l),{},{components:a})):t.createElement(g,p({ref:r},l))}));function m(e,r){var a=arguments,n=r&&r.mdxType;if("string"==typeof e||n){var o=a.length,p=new Array(o);p[0]=d;var s={};for(var i in r)hasOwnProperty.call(r,i)&&(s[i]=r[i]);s.originalType=e,s.mdxType="string"==typeof e?e:n,p[1]=s;for(var c=2;c<o;c++)p[c]=a[c];return t.createElement.apply(null,p)}return t.createElement.apply(null,a)}d.displayName="MDXCreateElement"},70880:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>i,contentTitle:()=>p,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var t=a(87462),n=(a(67294),a(3905));const o={id:"dataproc4",title:"dataproc--dataproc+GCS+Bigquery+Pyspark",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},p="\u4eca\u5929\u66f4\u6709\u610f\u601d\u5566 !! big query + pyspark + Dataproc",s={permalink:"/blog/2020/4/17/dataproc+GCS+Bigquery+pyspark",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-17-dataproc+GCS+Bigquery+pyspark.md",source:"@site/blog/2020-4-17-dataproc+GCS+Bigquery+pyspark.md",title:"dataproc--dataproc+GCS+Bigquery+Pyspark",description:"\u6d41\u7a0b\u5f88\u7b80\u5355",date:"2020-04-17T00:00:00.000Z",formattedDate:"April 17, 2020",tags:[{label:"dataproc",permalink:"/blog/tags/dataproc"},{label:"GCP",permalink:"/blog/tags/gcp"},{label:"Spark",permalink:"/blog/tags/spark"},{label:"Hadoop",permalink:"/blog/tags/hadoop"}],readingTime:1.585,hasTruncateMarker:!1,authors:[{name:"\u62db\u6653\u8d24",title:"AI Engineer",url:"https://github.com/flybirdgroup",imageURL:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"}],frontMatter:{id:"dataproc4",title:"dataproc--dataproc+GCS+Bigquery+Pyspark",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},prevItem:{title:"yaml\u8bed\u6cd5\u5b66\u4e60",permalink:"/blog/2020/4/18/yaml\u8bed\u6cd5\u5b66\u4e60"},nextItem:{title:"dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro\u6587\u4ef6",permalink:"/blog/2020/4/17/dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro"}},i={authorsImageUrls:[void 0]},c=[{value:"\u6d41\u7a0b\u5f88\u7b80\u5355",id:"\u6d41\u7a0b\u5f88\u7b80\u5355",level:2},{value:"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199python.py",id:"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199pythonpy",level:2},{value:"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4,\u63d0\u4ea4spark job",id:"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4\u63d0\u4ea4spark-job",level:3}],l={toc:c};function u(e){let{components:r,...a}=e;return(0,n.kt)("wrapper",(0,t.Z)({},l,a,{components:r,mdxType:"MDXLayout"}),(0,n.kt)("h2",{id:"\u6d41\u7a0b\u5f88\u7b80\u5355"},"\u6d41\u7a0b\u5f88\u7b80\u5355"),(0,n.kt)("p",null,"\u9996\u5148\u6211\u4eec\u4eceGCS\u90a3\u91cc\u8bfb\u53d6avro\u6570\u636e,\u7136\u540e\u6211\u4eec\u8bfb\u53d6avro\u6570\u636e\u53d8\u6210dask.Dataframe,\u7136\u540e\u5bf9dask.Dataframe\u64cd\u4f5c,\u518d\u8f6c\u6210pandas Dataframe,\u7136\u540e\u53d8\u6210Spark Dataframe,\u6700\u540e\u901a\u8fc7Spark \u4e0e bigquery \u7684connector\u5bf9\u63a5\u8d77\u6765,\u5199\u5165big query"),(0,n.kt)("h2",{id:"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199pythonpy"},"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199python.py"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"import dask.bag as db # \u5bfc\u5165\u5de5\u5177\u5305\ndef run():\n    b = db.read_avro('gs://zz_mm_bucket/account_id_schema_new.avro') #\u4eceGCS\u8bfb\u53d6avro\u6587\u4ef6\n    df = b.to_dataframe() # \u8f6c\u6210Dataframe\n    df_values = df.compute().values.tolist() #\u8f6c\u6210pandas\u7684dataframe\n    df_columns = list(df.columns)\n\n    import pandas as pd\n    from pyspark.sql import SparkSession #spark\u521d\u59cb\u5316\n    spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n    bucket = \"haha_mm_bucket\" #\u8bbe\u7f6ebucket\n    spark.conf.set('temporaryGcsBucket', bucket) #\u7ed9spark\u521d\u59cb\u5316\u8bbe\u7f6ebucket\u96f6\u65f6\u5b58\u653e\u6570\u636e\u7684gcs\n\n    spark_df = spark.createDataFrame(df_values, df_columns) \u628adataframe\u8f6c\u6210spark\u7684dataframe\n    spark_df.show(10) #\u5bf9spark\u7684dataframe\u8fdb\u884c\u64cd\u4f5c\n    spark_df.write.format('bigquery').option('table','query-11:newdata.newdata').save() # \u5199\u5165bigquery\n\nif __name__ == '__main__':\n    run()\n")),(0,n.kt)("h3",{id:"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4\u63d0\u4ea4spark-job"},"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4,\u63d0\u4ea4spark job"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'gcloud dataproc jobs submit pyspark wordcount.py \\\n    --cluster cluster-name \\\n    --region cluster-region (example: "us-central1") \\\n    --jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n')),(0,n.kt)("p",null,"\u4e3b\u8981\u683c\u5f0f: gcloud dataproc jobs submit pyspark python.py(python\u6587\u4ef6) \\\n--cluster cluster-name \\\n--region cluster-region(\u6bd4\u5982:us-central1,\u4e00\u5b9a\u8981\u5bf9\u5e94dataproc\u96c6\u7fa4\u7684region)\n--jars \u4e0ebiguqery\u8fde\u63a5\u7684\u5305\n\u6ce8\u610f\u8fd9\u91cc\u7684jars:\nIf you are using Dataproc image 1.5, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\nIf you are using Dataproc image 1.4 or below, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"gcloud config set dataproc/region us-central1\nBUCKET_NAME=haha_mm_bucket\ninput=new.avro\ngcloud dataproc jobs submit pyspark wordcount3.py \\\n--cluster cluster-662b \\\n-- gs://${BUCKET_NAME}/${input} \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--packages com.databricks:spark-avro_2.11:4.0.0\n")))}u.isMDXComponent=!0}}]);