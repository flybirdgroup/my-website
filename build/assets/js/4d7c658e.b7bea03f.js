"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1536],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>d});var n=r(67294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function l(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var s=n.createContext({}),p=function(e){var t=n.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):l(l({},t),e)),r},c=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),m=p(r),d=o,f=m["".concat(s,".").concat(d)]||m[d]||u[d]||a;return r?n.createElement(f,l(l({ref:t},c),{},{components:r})):n.createElement(f,l({ref:t},c))}));function d(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=r.length,l=new Array(a);l[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:o,l[1]=i;for(var p=2;p<a;p++)l[p]=r[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,r)}m.displayName="MDXCreateElement"},29882:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>p});var n=r(87462),o=(r(67294),r(3905));const a={id:"dataproc6",title:"\u7528yaml\u914d\u7f6e\u6587\u4ef6\u4f20\u53c2\u6570\u7ed9pyspark,\u7136\u540e\u518ddataproc\u8fd0\u884c",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},l=void 0,i={permalink:"/blog/2020/4/18/dataproc+spark+yaml",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-4-18-dataproc+spark+yaml.md",source:"@site/blog/2020-4-18-dataproc+spark+yaml.md",title:"\u7528yaml\u914d\u7f6e\u6587\u4ef6\u4f20\u53c2\u6570\u7ed9pyspark,\u7136\u540e\u518ddataproc\u8fd0\u884c",description:"\u9996\u5148\u6211\u4eec\u8981\u5b66Yaml\u8bed\u6cd5:",date:"2020-04-18T00:00:00.000Z",formattedDate:"April 18, 2020",tags:[{label:"dataproc",permalink:"/blog/tags/dataproc"},{label:"GCP",permalink:"/blog/tags/gcp"},{label:"Spark",permalink:"/blog/tags/spark"},{label:"Hadoop",permalink:"/blog/tags/hadoop"}],readingTime:2.73,hasTruncateMarker:!1,authors:[{name:"\u62db\u6653\u8d24",title:"AI Engineer",url:"https://github.com/flybirdgroup",imageURL:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg"}],frontMatter:{id:"dataproc6",title:"\u7528yaml\u914d\u7f6e\u6587\u4ef6\u4f20\u53c2\u6570\u7ed9pyspark,\u7136\u540e\u518ddataproc\u8fd0\u884c",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},prevItem:{title:"\u628aavro\u6587\u4ef6\u653e\u5230bigquery",permalink:"/blog/2020/4/21/\u628aavro\u6587\u4ef6\u653e\u5230bigquery"},nextItem:{title:"yaml\u8bed\u6cd5\u5b66\u4e60",permalink:"/blog/2020/4/18/yaml\u8bed\u6cd5\u5b66\u4e60"}},s={authorsImageUrls:[void 0]},p=[{value:"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark",id:"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark",level:2},{value:"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobs,jobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005table,fillna,union,transoform\u7b49etl\u4f5c\u4e1a",id:"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobsjobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005tablefillnauniontransoform\u7b49etl\u4f5c\u4e1a",level:2},{value:"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801",id:"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801",level:2},{value:"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc clusters",id:"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc-clusters",level:2}],c={toc:p};function u(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,n.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"\u9996\u5148\u6211\u4eec\u8981\u5b66Yaml\u8bed\u6cd5:\n\u5177\u4f53yaml\u8bed\u6cd5\u53ef\u4ee5\u53c2\u8003",(0,o.kt)("a",{parentName:"p",href:"yaml1"},"yaml\u8bed\u6cd5\u8be6\u60c5")),(0,o.kt)("p",null,"\u6574\u4f53\u601d\u8def"),(0,o.kt)("h2",{id:"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark"},"\u51c6\u5907yaml\u6587\u4ef6\u548cpyspark"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"\u5bfc\u5165\u5de5\u5177\u5305")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"#!/usr/bin/env python\nfrom pyspark.sql import SparkSession\nimport sys,yaml,datetime\nimport os\nimport pathlib\nimport google.cloud.storage as gcs\n")),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},"\u6211\u4eec\u5199\u4e86\u4e00\u4e2aYaml\u6587\u4ef6\u4f5c\u4e3aconfig\u6587\u4ef6"),(0,o.kt)("li",{parentName:"ol"},'\u6211\u4eec\u5728\u6211\u4eec\u7684pyspark\u6587\u4ef6\u8bfb\u53d6yaml\u6587\u4ef6,\u8fd9\u91cc\u8981\u6ce8\u610f\u7684\u662f,\u56e0\u4e3a\u672c\u5730\u548cGCS\u4f1a\u6709\u4e0d\u540c,\u672c\u5730\u662f\u53ef\u4ee5\u76f4\u63a5\u8bfb\u53d6\u7684,\u4f46\u662f\u5982\u679cyaml\u6587\u4ef6\u5728GCS,yaml\u6587\u4ef6\u5c31\u662fobject,\u662f\u4e0d\u53ef\u6539\u5199\u7684,\u6240\u4ee5\u6211\u4eec\u4e0d\u80fd\u76f4\u63a5open(yaml\u6587\u4ef6,"r")'),(0,o.kt)("li",{parentName:"ol"},"\u6211\u4eec\u9700\u8981\u5728pyspark\u6587\u4ef6\u4e0a\u521b\u5efagcs\u5ba2\u6237\u7aef,\u7136\u540e\u521b\u5efa\u8bbe\u7f6e\u4e00\u4e2a\u672c\u5730\u6587\u4ef6\u8def\u5f84,\u7136\u540e\u901a\u8fc7\u5ba2\u6237\u7aef\u8bfb\u53d6yaml\u6587\u4ef6\u5185\u5bb9\u5e76\u4e14\u4e0b\u8f7d\u5230\u672c\u5730,\u7136\u540e\u518d\u901a\u8fc7\u672c\u5730\u4f7f\u7528with open\u65b9\u6cd5\u8bfb\u53d6yaml\u6587\u4ef6\u5185\u5bb9")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'client = gcs.Client()\n\n#set target file to write to\ntarget = pathlib.Path("local_file.yaml")\n\nconfig_file = sys.argv[1] +"config.yaml"\n\n#set file to download\nFULL_FILE_PATH = config_file\n\n#open filestream with write permissions\nwith target.open(mode="wb") as downloaded_file:\n\n        #download and write file locally\n        client.download_blob_to_file(FULL_FILE_PATH, downloaded_file)\n\nconfig_file="local_file.yaml"\n')),(0,o.kt)("p",null,"\u8bfb\u53d6\u540e,\u6211\u4eec\u5c31\u53ef\u4ee5\u64cd\u4f5c\u4e00\u4e0b\u4ee3\u7801"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'for job in config["jobs"]:\n    print("Creating source views...")\n    for source in job["sources"]:\n        print(source)\n        if source.get("table") is not None:\n            print("Creating view %s from table %s..." % (source["view"], source["table"]))\n            df = spark.table(source["table"])\n            df.show(5)\n            print(\'table now\')\n        else:\n            print("Creating view %s from object %s..." % (source["view"], source["object"]))\n            df = spark.read.format(source[\'object\'][source[\'object\'].rfind(\'.\')+1:]).option("header","true").load(source[\'object\'])\n            df.show(5)\n        if source.get("columns") is not None:\n            # columns listed, select given columns\n            df = df.select(source["columns"])\n            df.show(5)\n        if source.get(\'Fillna\') is not None:\n            print(source[\'Fillna\']["fields"])\n            print(\'hah\',type(source[\'Fillna\']))\n            df = df.fillna({source[\'Fillna\']["fields"]:source[\'Fillna\']["num"]})\n            df.show(5)\n        if source.get("filters") is not None:\n            df.filter(source["filters"])\n        if source.get("union") is not None:\n            df_union = spark.sql("select * from %s"%(source[\'union\']))\n            df.union(df_union)\n            df.show(1)\n        if source.get("join") is not None:\n            cur = df.select(source[\'Key\'])\n            pre = spark.sql("select * from %s"%(source[\'right\']))\n            df = cur.join(pre,[source[\'Key\']],source[\'how\'])\n            df.show(5)\n        df.createOrReplaceTempView(source["view"])\n    print("Performing SQL Transformations...")\n    if job.get("transforms") is not None:\n        for transform in job["transforms"]:\n            spark.sql(transform["sql"])\n            print(df.count())\n    if job.get("targets") is not None:\n        print("Writing out final object to %s..." % (job["targets"]["target_location"]))\n        start = datetime.datetime.now()\n        final_df = spark.table(job["targets"]["final_object"])\n        final_df.write.mode(job["targets"]["mode"]).format(job["targets"]["format"]).save(job["targets"]["target_location"])\n        finish = datetime.datetime.now()\n        print("Finished writing out target object...")\n')),(0,o.kt)("h2",{id:"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobsjobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005tablefillnauniontransoform\u7b49etl\u4f5c\u4e1a"},"\u8fd9\u7aef\u4ee3\u7801\u7684\u903b\u8f91\u5c31\u662f\u5faa\u73afconfig\u91cc\u9762\u7684jobs,jobs\u91cc\u9762\u5305\u62ec\u8bfb\u53d6\u6587\u4ef6\u6216\u8005table,fillna,union,transoform\u7b49etl\u4f5c\u4e1a"),(0,o.kt)("h2",{id:"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801"},"\u51c6\u5907\u542f\u52a8dataproc\u4ee3\u7801"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"CLUSTER_NAME=newnew\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n    --region=global \\\n    --zone=us-central1-b \\\n    --worker-machine-type n1-standard-1 \\\n    --num-workers 2 \\\n    --image-version 1.4-debian \\\n    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n    --metadata 'PIP_PACKAGES=google-cloud-storage PyYAML pathlib avro-python3 dask[dataframe] gcsfs fastavro' \\\n    --enable-component-gateway \\\n    --worker-boot-disk-size=40 \\\n    --optional-components=ANACONDA \\\n    --enable-component-gateway\nBUCKET_NAME=zz_michael\ngcloud config set dataproc/region global\ngcloud dataproc jobs submit pyspark dyyaml.py --cluster newnew \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--jars=https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/spark-avro_2.11-2.4.4.jar \\\n-- gs://${BUCKET_NAME}/yaml/ \n")),(0,o.kt)("h2",{id:"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc-clusters"},"job\u5b8c\u6210\u540e\u9700\u8981\u5220\u9664dataproc clusters"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"CLUSTER_NAME=newnew\ngcloud dataproc clusters delete $CLUSTER_NAME\n")))}u.isMDXComponent=!0}}]);