"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2791],{3905:(a,e,r)=>{r.d(e,{Zo:()=>d,kt:()=>m});var t=r(67294);function n(a,e,r){return e in a?Object.defineProperty(a,e,{value:r,enumerable:!0,configurable:!0,writable:!0}):a[e]=r,a}function o(a,e){var r=Object.keys(a);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(a);e&&(t=t.filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable}))),r.push.apply(r,t)}return r}function s(a){for(var e=1;e<arguments.length;e++){var r=null!=arguments[e]?arguments[e]:{};e%2?o(Object(r),!0).forEach((function(e){n(a,e,r[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(a,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(e){Object.defineProperty(a,e,Object.getOwnPropertyDescriptor(r,e))}))}return a}function p(a,e){if(null==a)return{};var r,t,n=function(a,e){if(null==a)return{};var r,t,n={},o=Object.keys(a);for(t=0;t<o.length;t++)r=o[t],e.indexOf(r)>=0||(n[r]=a[r]);return n}(a,e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(a);for(t=0;t<o.length;t++)r=o[t],e.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(a,r)&&(n[r]=a[r])}return n}var l=t.createContext({}),i=function(a){var e=t.useContext(l),r=e;return a&&(r="function"==typeof a?a(e):s(s({},e),a)),r},d=function(a){var e=i(a.components);return t.createElement(l.Provider,{value:e},a.children)},c={inlineCode:"code",wrapper:function(a){var e=a.children;return t.createElement(t.Fragment,{},e)}},u=t.forwardRef((function(a,e){var r=a.components,n=a.mdxType,o=a.originalType,l=a.parentName,d=p(a,["components","mdxType","originalType","parentName"]),u=i(r),m=n,k=u["".concat(l,".").concat(m)]||u[m]||c[m]||o;return r?t.createElement(k,s(s({ref:e},d),{},{components:r})):t.createElement(k,s({ref:e},d))}));function m(a,e){var r=arguments,n=e&&e.mdxType;if("string"==typeof a||n){var o=r.length,s=new Array(o);s[0]=u;var p={};for(var l in e)hasOwnProperty.call(e,l)&&(p[l]=e[l]);p.originalType=a,p.mdxType="string"==typeof a?a:n,s[1]=p;for(var i=2;i<o;i++)s[i]=r[i];return t.createElement.apply(null,s)}return t.createElement.apply(null,r)}u.displayName="MDXCreateElement"},87174:(a,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>p,toc:()=>i});var t=r(87462),n=(r(67294),r(3905));const o={id:"dataproc5",title:"dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro\u6587\u4ef6",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},s="\u4eca\u5929\u6709\u610f\u601d\u5566 !! spark \u8bfb\u53d6 GCS avro\u6587\u4ef6 + ETL\u64cd\u4f5c + \u5199avro\u6587\u4ef6",p={unversionedId:"Big_Data/dataproc5",id:"Big_Data/dataproc5",title:"dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro\u6587\u4ef6",description:"1. spark \u521d\u59cb\u5316,\u56e0\u4e3a\u8981\u8bfb\u53d6\u6210dataframe\u6216\u8005sql\u5f62\u5f0f,\u5bfc\u5165SparkSession",source:"@site/docs/Big_Data/2020-4-17-dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro.md",sourceDirName:"Big_Data",slug:"/Big_Data/dataproc5",permalink:"/docs/Big_Data/dataproc5",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Big_Data/2020-4-17-dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro.md",tags:[{label:"dataproc",permalink:"/docs/tags/dataproc"},{label:"GCP",permalink:"/docs/tags/gcp"},{label:"Spark",permalink:"/docs/tags/spark"},{label:"Hadoop",permalink:"/docs/tags/hadoop"}],version:"current",frontMatter:{id:"dataproc5",title:"dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro\u6587\u4ef6",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},sidebar:"tutorialSidebar",previous:{title:"dataproc--dataproc+GCS+Bigquery+Pyspark",permalink:"/docs/Big_Data/dataproc4"},next:{title:"dataproc\u81ea\u52a8\u4f38\u7f29\u548c\u8fd0\u884cspark job",permalink:"/docs/Big_Data/dataproc2"}},l={},i=[{value:"1. spark \u521d\u59cb\u5316,\u56e0\u4e3a\u8981\u8bfb\u53d6\u6210dataframe\u6216\u8005sql\u5f62\u5f0f,\u5bfc\u5165SparkSession",id:"1-spark-\u521d\u59cb\u5316\u56e0\u4e3a\u8981\u8bfb\u53d6\u6210dataframe\u6216\u8005sql\u5f62\u5f0f\u5bfc\u5165sparksession",level:3},{value:"2. \u521b\u5efaspark\u5bf9\u8c61",id:"2-\u521b\u5efaspark\u5bf9\u8c61",level:3},{value:"\u53c2\u6570\u5224\u65ad\u548c\u53c2\u6570\u8bbe\u7f6e",id:"\u53c2\u6570\u5224\u65ad\u548c\u53c2\u6570\u8bbe\u7f6e",level:3},{value:"4 \u8bfb\u53d6avro\u6587\u4ef6",id:"4-\u8bfb\u53d6avro\u6587\u4ef6",level:3},{value:"5 \u6ce8\u518c\u89c6\u56fe,\u5b9e\u884c\u67e5\u8be2\u8bed\u53e5",id:"5-\u6ce8\u518c\u89c6\u56fe\u5b9e\u884c\u67e5\u8be2\u8bed\u53e5",level:3},{value:"6 \u5904\u7406\u597d\u7684dataframe\u5bf9\u8c61\u5199\u6210avro\u6587\u4ef6 (\u6ce8\u610f,\u7528sql\u5904\u7406\u8fc7\u540e\u7684\u8fd8\u662fdataframe\u5bf9\u8c61)",id:"6-\u5904\u7406\u597d\u7684dataframe\u5bf9\u8c61\u5199\u6210avro\u6587\u4ef6-\u6ce8\u610f\u7528sql\u5904\u7406\u8fc7\u540e\u7684\u8fd8\u662fdataframe\u5bf9\u8c61",level:3},{value:"7 \u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4,\u521b\u5efadataproc\u96c6\u7fa4,\u7136\u540e\u63d0\u4ea4spark job",id:"7-\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4\u521b\u5efadataproc\u96c6\u7fa4\u7136\u540e\u63d0\u4ea4spark-job",level:3},{value:"\u8fd9\u91cc\u6ce8\u610f\u7684\u662fgcloud dataproc jobs sumbit\u7684\u53c2\u6570\u683c\u5f0f\u662f pyspark.py\u6587\u4ef6, files",id:"\u8fd9\u91cc\u6ce8\u610f\u7684\u662fgcloud-dataproc-jobs-sumbit\u7684\u53c2\u6570\u683c\u5f0f\u662f-pysparkpy\u6587\u4ef6-files",level:2},{value:"\u5173\u4e8e\u751f\u6210\u6587\u4ef6,\u56e0\u4e3aspark\u662f\u57fa\u4e8ehadoop\u7684,\u6240\u4ee5\u6587\u4ef6\u4e5f\u4f1a\u5206\u5e03\u5f0f\u5b58\u50a8,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u770b\u5230",id:"\u5173\u4e8e\u751f\u6210\u6587\u4ef6\u56e0\u4e3aspark\u662f\u57fa\u4e8ehadoop\u7684\u6240\u4ee5\u6587\u4ef6\u4e5f\u4f1a\u5206\u5e03\u5f0f\u5b58\u50a8\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u770b\u5230",level:2},{value:"\u4e00\u822c\u662f\u5206\u533a\u662f\u4f1a\u6839\u636e\u4f60\u7684\u7535\u8111\u7684cpu\u6838\u6570\u81ea\u52a8\u5206\u914d,\u6211\u7684\u7535\u8111\u662fcore i5,\u4e5f\u5c31\u662f\u56db\u6838\u7684,\u6240\u4ee5\u9ed8\u8ba4\u662f4",id:"\u4e00\u822c\u662f\u5206\u533a\u662f\u4f1a\u6839\u636e\u4f60\u7684\u7535\u8111\u7684cpu\u6838\u6570\u81ea\u52a8\u5206\u914d\u6211\u7684\u7535\u8111\u662fcore-i5\u4e5f\u5c31\u662f\u56db\u6838\u7684\u6240\u4ee5\u9ed8\u8ba4\u662f4",level:2}],d={toc:i};function c(a){let{components:e,...r}=a;return(0,n.kt)("wrapper",(0,t.Z)({},d,r,{components:e,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"\u4eca\u5929\u6709\u610f\u601d\u5566--spark-\u8bfb\u53d6-gcs-avro\u6587\u4ef6--etl\u64cd\u4f5c--\u5199avro\u6587\u4ef6"},"\u4eca\u5929\u6709\u610f\u601d\u5566 !! spark \u8bfb\u53d6 GCS avro\u6587\u4ef6 + ETL\u64cd\u4f5c + \u5199avro\u6587\u4ef6"),(0,n.kt)("h3",{id:"1-spark-\u521d\u59cb\u5316\u56e0\u4e3a\u8981\u8bfb\u53d6\u6210dataframe\u6216\u8005sql\u5f62\u5f0f\u5bfc\u5165sparksession"},"1. spark \u521d\u59cb\u5316,\u56e0\u4e3a\u8981\u8bfb\u53d6\u6210dataframe\u6216\u8005sql\u5f62\u5f0f,\u5bfc\u5165SparkSession"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"from pyspark.sql import SparkSession\nimport sys\n")),(0,n.kt)("h3",{id:"2-\u521b\u5efaspark\u5bf9\u8c61"},"2. \u521b\u5efaspark\u5bf9\u8c61"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"spark = SparkSession \\\n  .builder \\\n  .master('yarn') \\\n  .appName('gcs-sparkdataframe-sql-avro') \\\n  .getOrCreate()\n")),(0,n.kt)("h3",{id:"\u53c2\u6570\u5224\u65ad\u548c\u53c2\u6570\u8bbe\u7f6e"},"\u53c2\u6570\u5224\u65ad\u548c\u53c2\u6570\u8bbe\u7f6e"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'if len(sys.argv) != 4:\n  raise Exception("Exactly 3 arguments are required: <inputUri> <table1><table2>")\n\ninputUri=sys.argv[1]\ntable1=sys.argv[2]\ntable2=sys.argv[3]\n')),(0,n.kt)("h3",{id:"4-\u8bfb\u53d6avro\u6587\u4ef6"},"4 \u8bfb\u53d6avro\u6587\u4ef6"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"df = spark.read.format('avro').load(inputUri)\n")),(0,n.kt)("h3",{id:"5-\u6ce8\u518c\u89c6\u56fe\u5b9e\u884c\u67e5\u8be2\u8bed\u53e5"},"5 \u6ce8\u518c\u89c6\u56fe,\u5b9e\u884c\u67e5\u8be2\u8bed\u53e5"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'df1 = spark.sql("select ACNO,%s from bigtable" % (",".join(df.columns[1:round(len(df.columns) / 2)])))\ndf2 = spark.sql("select ACNO,%s from bigtable" % (",".join(df.columns[round(len(df.columns) / 2):])))\ndf1.show(10)\ndf2.show(10)\n')),(0,n.kt)("h3",{id:"6-\u5904\u7406\u597d\u7684dataframe\u5bf9\u8c61\u5199\u6210avro\u6587\u4ef6-\u6ce8\u610f\u7528sql\u5904\u7406\u8fc7\u540e\u7684\u8fd8\u662fdataframe\u5bf9\u8c61"},"6 \u5904\u7406\u597d\u7684dataframe\u5bf9\u8c61\u5199\u6210avro\u6587\u4ef6 (\u6ce8\u610f,\u7528sql\u5904\u7406\u8fc7\u540e\u7684\u8fd8\u662fdataframe\u5bf9\u8c61)"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"df1.write.format('avro').save(table1,'avro')\n\ndf2.write.format('avro').save(table2,'avro')\n")),(0,n.kt)("h3",{id:"7-\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4\u521b\u5efadataproc\u96c6\u7fa4\u7136\u540e\u63d0\u4ea4spark-job"},"7 \u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4,\u521b\u5efadataproc\u96c6\u7fa4,\u7136\u540e\u63d0\u4ea4spark job"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"CLUSTER_NAME=newnew\ngcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n    --region=global \\\n    --zone=us-central1-b \\\n    --worker-machine-type n1-standard-1 \\\n    --num-workers 2 \\\n    --image-version 1.4-debian \\\n    --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh \\\n    --metadata 'PIP_PACKAGES=google-cloud-storage avro-python3 dask[dataframe] gcsfs fastavro' \\\n    --enable-component-gateway \\\n    --worker-boot-disk-size=40 \\\n    --optional-components=ANACONDA \\\n    --enable-component-gateway\nBUCKET_NAME=zz_mm_bucket\ngcloud config set dataproc/region global\ngcloud dataproc jobs submit pyspark avrosqlargs.py --cluster newnew \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--jars=https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/spark-avro_2.11-2.4.4.jar \\\n-- gs://${BUCKET_NAME}/input/gs://zz_mm_bucket/input/ gs://${BUCKET_NAME}/output/table1 gs://${BUCKET_NAME}/output/table2\n")),(0,n.kt)("h2",{id:"\u8fd9\u91cc\u6ce8\u610f\u7684\u662fgcloud-dataproc-jobs-sumbit\u7684\u53c2\u6570\u683c\u5f0f\u662f-pysparkpy\u6587\u4ef6-files"},"\u8fd9\u91cc\u6ce8\u610f\u7684\u662fgcloud dataproc jobs sumbit\u7684\u53c2\u6570\u683c\u5f0f\u662f pyspark.py\u6587\u4ef6, files"),(0,n.kt)("p",null,"\u6240\u4ee5\u4f8b\u5b50\u4e2d\u6211\u4eec\u7684\u53c2\u6570\u603b\u5171\u67094\u4e2a\n1 avrosqlargs.py"),(0,n.kt)("p",null,"2 gs://${BUCKET_NAME}/input/gs://zz_mm_bucket/input/"),(0,n.kt)("p",null,"3 gs://${BUCKET_NAME}/output/table1 "),(0,n.kt)("p",null,"4 gs://${BUCKET_NAME}/output/table2"),(0,n.kt)("p",null,"jars\u548ccluster\u90fd\u4e0d\u7b97\u4e3a\u53c2\u6570"),(0,n.kt)("p",null,"\u8fd8\u6709\u5c31\u662ffiles\u7684\u662f\u6587\u4ef6\u5939\u5f62\u5f0f\u800c\u4e0d\u80fd\u662f\u6587\u4ef6\u5f62\u5f0f,\u6240\u4ee5\u8bfb\u5165\u6587\u4ef6\u5939\u540e,\u53ef\u4ee5\u6839\u636e\u9700\u8981\u8bfb\u53d6\u4f60\u9700\u8981\u7684\u6587\u4ef6,\u6bd4\u5982sys.argv+'\u6587\u4ef6\u540d'"),(0,n.kt)("p",null,"\u6240\u4ee5\u6574\u4f53\u53ef\u4ee5\u6539\u6210:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'#!/usr/bin/python\n"""BigQuery I/O PySpark example."""\nfrom pyspark.sql import SparkSession\nimport sys\n\n\n\nspark = SparkSession \\\n  .builder \\\n  .master(\'yarn\') \\\n  .appName(\'gcs-sparkdataframe-sql-avro\') \\\n  .getOrCreate()\n\n# get spark datafrom from avro file in GCS\n\nif len(sys.argv) != 4:\n  raise Exception("Exactly 3 arguments are required: <inputUri> <table1><table2>")\n\ninputUri=sys.argv[1]\ntable1=sys.argv[2]\ntable2=sys.argv[3]\n\nfile = inputUri+\'account_id_schema_new.avro\'\ndf = spark.read.format(\'avro\').load(file)\n\n\n#create temp table\ndf.createOrReplaceTempView(\'bigtable\')\n\n# split temp table into 2 spark dataframes\ndf1 = spark.sql("select ACNO,%s from bigtable" % (",".join(df.columns[1:round(len(df.columns) / 2)])))\ndf2 = spark.sql("select ACNO,%s from bigtable" % (",".join(df.columns[round(len(df.columns) / 2):])))\ndf1.show(10)\ndf2.show(10)\n\n# Saving the dataframes into avro files and dump avro files into GCS\n\ndf1.write.mode("overwrite").format(\'avro\').save(table1,\'avro\')\n\ndf2.write.mode("overwrite").format(\'avro\').save(table2,\'avro\')\n')),(0,n.kt)("h2",{id:"\u5173\u4e8e\u751f\u6210\u6587\u4ef6\u56e0\u4e3aspark\u662f\u57fa\u4e8ehadoop\u7684\u6240\u4ee5\u6587\u4ef6\u4e5f\u4f1a\u5206\u5e03\u5f0f\u5b58\u50a8\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u770b\u5230"},"\u5173\u4e8e\u751f\u6210\u6587\u4ef6,\u56e0\u4e3aspark\u662f\u57fa\u4e8ehadoop\u7684,\u6240\u4ee5\u6587\u4ef6\u4e5f\u4f1a\u5206\u5e03\u5f0f\u5b58\u50a8,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u770b\u5230"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"df = spark.read.format('avro').load(sys.argv[3])\n")),(0,n.kt)("h2",{id:"\u4e00\u822c\u662f\u5206\u533a\u662f\u4f1a\u6839\u636e\u4f60\u7684\u7535\u8111\u7684cpu\u6838\u6570\u81ea\u52a8\u5206\u914d\u6211\u7684\u7535\u8111\u662fcore-i5\u4e5f\u5c31\u662f\u56db\u6838\u7684\u6240\u4ee5\u9ed8\u8ba4\u662f4"},"\u4e00\u822c\u662f\u5206\u533a\u662f\u4f1a\u6839\u636e\u4f60\u7684\u7535\u8111\u7684cpu\u6838\u6570\u81ea\u52a8\u5206\u914d,\u6211\u7684\u7535\u8111\u662fcore i5,\u4e5f\u5c31\u662f\u56db\u6838\u7684,\u6240\u4ee5\u9ed8\u8ba4\u662f4"),(0,n.kt)("p",null,"\u6211\u4eec\u53ef\u4ee5\u91cd\u5206\u533a:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"df.repartition(10) # \u5c31\u662f\u520610\u533a\ndf.rdd.getNumPartitions() #\u67e5\u770b\u5206\u533a\u6570\ndf.coalesce(1)\n")))}c.isMDXComponent=!0}}]);