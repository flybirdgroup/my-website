"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8082],{3905:(a,r,e)=>{e.d(r,{Zo:()=>u,kt:()=>m});var t=e(67294);function n(a,r,e){return r in a?Object.defineProperty(a,r,{value:e,enumerable:!0,configurable:!0,writable:!0}):a[r]=e,a}function o(a,r){var e=Object.keys(a);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(a);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(a,r).enumerable}))),e.push.apply(e,t)}return e}function p(a){for(var r=1;r<arguments.length;r++){var e=null!=arguments[r]?arguments[r]:{};r%2?o(Object(e),!0).forEach((function(r){n(a,r,e[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(a,Object.getOwnPropertyDescriptors(e)):o(Object(e)).forEach((function(r){Object.defineProperty(a,r,Object.getOwnPropertyDescriptor(e,r))}))}return a}function s(a,r){if(null==a)return{};var e,t,n=function(a,r){if(null==a)return{};var e,t,n={},o=Object.keys(a);for(t=0;t<o.length;t++)e=o[t],r.indexOf(e)>=0||(n[e]=a[e]);return n}(a,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(a);for(t=0;t<o.length;t++)e=o[t],r.indexOf(e)>=0||Object.prototype.propertyIsEnumerable.call(a,e)&&(n[e]=a[e])}return n}var c=t.createContext({}),i=function(a){var r=t.useContext(c),e=r;return a&&(e="function"==typeof a?a(r):p(p({},r),a)),e},u=function(a){var r=i(a.components);return t.createElement(c.Provider,{value:r},a.children)},l={inlineCode:"code",wrapper:function(a){var r=a.children;return t.createElement(t.Fragment,{},r)}},d=t.forwardRef((function(a,r){var e=a.components,n=a.mdxType,o=a.originalType,c=a.parentName,u=s(a,["components","mdxType","originalType","parentName"]),d=i(e),m=n,g=d["".concat(c,".").concat(m)]||d[m]||l[m]||o;return e?t.createElement(g,p(p({ref:r},u),{},{components:e})):t.createElement(g,p({ref:r},u))}));function m(a,r){var e=arguments,n=r&&r.mdxType;if("string"==typeof a||n){var o=e.length,p=new Array(o);p[0]=d;var s={};for(var c in r)hasOwnProperty.call(r,c)&&(s[c]=r[c]);s.originalType=a,s.mdxType="string"==typeof a?a:n,p[1]=s;for(var i=2;i<o;i++)p[i]=e[i];return t.createElement.apply(null,p)}return t.createElement.apply(null,e)}d.displayName="MDXCreateElement"},8498:(a,r,e)=>{e.r(r),e.d(r,{assets:()=>c,contentTitle:()=>p,default:()=>l,frontMatter:()=>o,metadata:()=>s,toc:()=>i});var t=e(87462),n=(e(67294),e(3905));const o={id:"dataproc4",title:"dataproc--dataproc+GCS+Bigquery+Pyspark",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},p="\u4eca\u5929\u66f4\u6709\u610f\u601d\u5566 !! big query + pyspark + Dataproc",s={unversionedId:"Big_Data/dataproc4",id:"Big_Data/dataproc4",title:"dataproc--dataproc+GCS+Bigquery+Pyspark",description:"\u6d41\u7a0b\u5f88\u7b80\u5355",source:"@site/docs/Big_Data/2020-4-17-dataproc+GCS+Bigquery+pyspark.md",sourceDirName:"Big_Data",slug:"/Big_Data/dataproc4",permalink:"/docs/Big_Data/dataproc4",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Big_Data/2020-4-17-dataproc+GCS+Bigquery+pyspark.md",tags:[{label:"dataproc",permalink:"/docs/tags/dataproc"},{label:"GCP",permalink:"/docs/tags/gcp"},{label:"Spark",permalink:"/docs/tags/spark"},{label:"Hadoop",permalink:"/docs/tags/hadoop"}],version:"current",frontMatter:{id:"dataproc4",title:"dataproc--dataproc+GCS+Bigquery+Pyspark",author:"\u62db\u6653\u8d24",author_title:"AI Engineer",author_url:"https://github.com/flybirdgroup",author_image_url:"https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1615738601,1434436036&fm=26&gp=0.jpg",tags:["dataproc","GCP","Spark","Hadoop"]},sidebar:"tutorialSidebar",previous:{title:"mac\u5b89\u88c5spark+jupyter+annocade+pycharm\u914d\u7f6e",permalink:"/docs/Big_Data/spark1"},next:{title:"dataproc\u53c2\u6570\u5316\u8dd1spark\u548c\u8bfb\u5199avro\u6587\u4ef6",permalink:"/docs/Big_Data/dataproc5"}},c={},i=[{value:"\u6d41\u7a0b\u5f88\u7b80\u5355",id:"\u6d41\u7a0b\u5f88\u7b80\u5355",level:2},{value:"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199python.py",id:"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199pythonpy",level:2},{value:"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4,\u63d0\u4ea4spark job",id:"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4\u63d0\u4ea4spark-job",level:3}],u={toc:i};function l(a){let{components:r,...e}=a;return(0,n.kt)("wrapper",(0,t.Z)({},u,e,{components:r,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"\u4eca\u5929\u66f4\u6709\u610f\u601d\u5566--big-query--pyspark--dataproc"},"\u4eca\u5929\u66f4\u6709\u610f\u601d\u5566 !! big query + pyspark + Dataproc"),(0,n.kt)("h2",{id:"\u6d41\u7a0b\u5f88\u7b80\u5355"},"\u6d41\u7a0b\u5f88\u7b80\u5355"),(0,n.kt)("p",null,"\u9996\u5148\u6211\u4eec\u4eceGCS\u90a3\u91cc\u8bfb\u53d6avro\u6570\u636e,\u7136\u540e\u6211\u4eec\u8bfb\u53d6avro\u6570\u636e\u53d8\u6210dask.Dataframe,\u7136\u540e\u5bf9dask.Dataframe\u64cd\u4f5c,\u518d\u8f6c\u6210pandas Dataframe,\u7136\u540e\u53d8\u6210Spark Dataframe,\u6700\u540e\u901a\u8fc7Spark \u4e0e bigquery \u7684connector\u5bf9\u63a5\u8d77\u6765,\u5199\u5165big query"),(0,n.kt)("h2",{id:"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199pythonpy"},"\u5b89\u88c5\u521a\u624d\u7684\u601d\u8def\u5199python.py"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"import dask.bag as db # \u5bfc\u5165\u5de5\u5177\u5305\ndef run():\n    b = db.read_avro('gs://zz_mm_bucket/account_id_schema_new.avro') #\u4eceGCS\u8bfb\u53d6avro\u6587\u4ef6\n    df = b.to_dataframe() # \u8f6c\u6210Dataframe\n    df_values = df.compute().values.tolist() #\u8f6c\u6210pandas\u7684dataframe\n    df_columns = list(df.columns)\n\n    import pandas as pd\n    from pyspark.sql import SparkSession #spark\u521d\u59cb\u5316\n    spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n    bucket = \"haha_mm_bucket\" #\u8bbe\u7f6ebucket\n    spark.conf.set('temporaryGcsBucket', bucket) #\u7ed9spark\u521d\u59cb\u5316\u8bbe\u7f6ebucket\u96f6\u65f6\u5b58\u653e\u6570\u636e\u7684gcs\n\n    spark_df = spark.createDataFrame(df_values, df_columns) \u628adataframe\u8f6c\u6210spark\u7684dataframe\n    spark_df.show(10) #\u5bf9spark\u7684dataframe\u8fdb\u884c\u64cd\u4f5c\n    spark_df.write.format('bigquery').option('table','query-11:newdata.newdata').save() # \u5199\u5165bigquery\n\nif __name__ == '__main__':\n    run()\n")),(0,n.kt)("h3",{id:"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4\u63d0\u4ea4spark-job"},"\u53bb\u5230\u7ec8\u7aef\u8f93\u5165\u547d\u4ee4,\u63d0\u4ea4spark job"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'gcloud dataproc jobs submit pyspark wordcount.py \\\n    --cluster cluster-name \\\n    --region cluster-region (example: "us-central1") \\\n    --jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n')),(0,n.kt)("p",null,"\u4e3b\u8981\u683c\u5f0f: gcloud dataproc jobs submit pyspark python.py(python\u6587\u4ef6) \\\n--cluster cluster-name \\\n--region cluster-region(\u6bd4\u5982:us-central1,\u4e00\u5b9a\u8981\u5bf9\u5e94dataproc\u96c6\u7fa4\u7684region)\n--jars \u4e0ebiguqery\u8fde\u63a5\u7684\u5305\n\u6ce8\u610f\u8fd9\u91cc\u7684jars:\nIf you are using Dataproc image 1.5, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\nIf you are using Dataproc image 1.4 or below, add the following parameter:\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"gcloud config set dataproc/region us-central1\nBUCKET_NAME=haha_mm_bucket\ninput=new.avro\ngcloud dataproc jobs submit pyspark wordcount3.py \\\n--cluster cluster-662b \\\n-- gs://${BUCKET_NAME}/${input} \\\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n--packages com.databricks:spark-avro_2.11:4.0.0\n")))}l.isMDXComponent=!0}}]);